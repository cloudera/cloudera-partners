= CDP Private Cloud (PvC) :: Deployment Guide
v0.1, 2025-04-10: Draft
:description: *Installation instructions for Cloudera OnPrem Cluster*
:toc: left
:toc-title: Table of Contents
:toclevels: 2
:sectnums:
:source-highlighter: highlightjs
:icons: font
:imagesdir: ./extracted-media/media/
:keywords: Cloudera, Automation
:hide-uri-scheme:
:homepage: https://github.com/kuldeepsahu1105/cdp-onprem-automation

**Installation & Setup on VMWare Infrastructure**

**Published: April 2025**

image:image117.png[image117,width=350,height=79]

In partnership with:

image:image144.png[A close up of a logo Description automatically generated,width=244,height=49]

*Author: Kuldeep Sahu*, Partner Solutions Engineer, Cloudera Inc.

---

**Audience**

The intended audience for this document includes, but is not limited to, sales engineers, field consultants, professional services, IT managers, IT engineers, partners, and customers who are interested in learning about and deploying the Cloudera Data Platform Private Cloud (CDP Private Cloud) on the VMware Platform for digital transformation through cloud-native modern data analytics and AI/ML.

**Purpose of this Document**

This document describes the architecture, installation, configuration, and validated use cases for the VMware Platform using Cloudera Data Platform Private Cloud Base on VMware servers. A reference architecture is provided to configure the Cloudera Data Platform on VMware.

---

== Table of Contents

link:#table-of-contents[[.underline]#Table of Contents#]

link:#author-history[[.underline]#Author History#]

link:#cdp-pvc-installation-and-configuration-on-vmware-infrastructure-platform[[.underline]#CDP PvC Installation and Configuration on VMware Infrastructure Platform:#]

link:#important-urls[[.underline]#Important URLs:#]

link:#introduction[[.underline]#Introduction:#]

link:#cdp-pvc-setup-consists-of-the-following-three-parts[[.underline]#CDP PVC setup consists of the following three parts.#]

link:#solution-summary[[.underline]#Solution Summary#]

link:#reference-architecture[[.underline]#Prerequisites:#]

link:#hardware-requirements[[.underline]#Hardware Requirements#]

link:#freeipakerberos-private-dns-server-in-case-we-are-not-going-with-freeipa-external-kerberoskdc-reqd[[.underline]#FreeIPA/Kerberos & Private DNS Server: (In case we are not going with FreeIPA: External Kerberos/KDC Reqd.)#]

link:#pvc-base-cluster-we-will-be-installing-a-6-node-cluster-on-vms[[.underline]#PvC Base Cluster we will be installing a 6-node cluster on VMs:#]

link:#pvc-data-service-ecs-cluster-with-cdw-we-will-be-installing-a-6-node-cluster-on-vms[[.underline]#PvC Data Service (ECS) Cluster with CDW: we will be installing a 6-node cluster on VMs:#]

link:#pvc-data-service-ecs-cluster-with-cml-we-will-be-installing-a-4-node-cluster-on-vms[[.underline]#PvC Data Service (ECS) Cluster with CML: we will be installing a 4-node cluster on VMs:#]

link:#pvc-data-service-ecs-cluster-with-cde-we-will-be-installing-a-4-node-cluster-on-vms[[.underline]#PvC Data Service (ECS) Cluster with CDE: we will be installing a 4-node cluster on VMs:#]

link:#pvc-data-serviceecs-cluster-with-cdwcdecml-we-will-be-installing-a-11-node-cluster-on-vms[[.underline]#PvC Data Service(ECS) Cluster with CDW+CDE+CML: we will be installing a 11-node cluster on VMs:#]

link:#reference-architecture[[.underline]#Reference Architecture#]

link:#software-requirements[[.underline]#Software Requirements#]

link:#summary[[.underline]#Summary#]

link:#post-os-installation[[.underline]#Post OS Installation#]

link:#preliminary-work[[.underline]#Preliminary Work#]

link:#install-and-setup-of-ipa-services[[.underline]#Install and Setup of IPA services#]

link:#install-cloudera-data-platform-private-cloud-cdp-pvc[[.underline]#Install Cloudera Data Platform Private Cloud (CDP PvC)#]

____
link:#cdp-pvc-cloudera-manager-server-setup[[.underline]#CDP PvC Cloudera Manager Server Setup#]

link:#configure-cloudera-manager-for-external-authentication-using-ldap-ldap-integration[[.underline]#Configure Cloudera Manager for external authentication using LDAP (LDAP integration):#]

link:#setup-private-cloud-pvc-base-cluster[[.underline]#Setup Private Cloud (PvC) Base Cluster#]

link:#private-cloud-base-cluster-data-lakecontrol-plane-creation[[.underline]#Private Cloud Base Cluster (Data Lake/Control Plane) Creation#]

link:#additional-requirements-and-details-for-private-cloud-base-cluster-services[[.underline]#Additional requirements and details for Private Cloud Base Cluster services:#]

link:#configure-ranger-with-ssltls-enabled-postgresql-database[[.underline]#Configure Ranger with SSL/TLS enabled PostgreSQL Database#]

link:#configure-hive-metastore-with-ssltls-enabled-postgresql-database-mandatory-step-for-cdw[[.underline]#Configure Hive metastore with SSL/TLS enabled PostgreSQL Database (Mandatory Step for CDW)#]

link:#scale-the-cluster-optional-skip-this-step[[.underline]#Scale the Cluster (Optional– Skip this step)#]

link:#enable-high-availability-optional-skip-this-step[[.underline]#Enable High Availability (Optional– Skip this step)#]

link:#cdp-private-cloud-base-checklist[[.underline]#CDP Private Cloud Base checklist#]

link:#configure-ranger-authentication-for-ldap-optional-skip-this-step[[.underline]#Configure Ranger authentication for LDAP (Optional– Skip this Step)#]

link:#configure-hue-for-ldap-authentication-optional-skip-this-step[[.underline]#Configure Hue for LDAP Authentication (Optional– Skip this Step)#]

link:#configure-atlas-for-ldap-authentication-optional-skip-this-step[[.underline]#Configure Atlas for LDAP authentication (Optional– Skip this Step)#]

link:#configure-hive-for-ldap-authentication-optional-skip-this-step[[.underline]#Configure Hive for LDAP Authentication (Optional– Skip this Step)#]

link:#configure-hdfs-properties-to-optimize-log-collection-optional-skip-this-step[[.underline]#Configure HDFS properties to optimize log collection (Optional– Skip this Step)#]
____

link:#cdp-private-cloud-pvc-data-services-ds-installation[[.underline]#CDP Private Cloud (PvC) Data Services (DS) Installation#]

____
link:#embedded-container-service-ecs-checklist[[.underline]#Embedded Container Service (ECS) checklist#]

link:#installing-cdp-private-cloud-data-services-using-ecs[[.underline]#Installing CDP Private Cloud Data Services using ECS#]

link:#installing-ecs-cluster[[.underline]#Installing ECS Cluster#]

link:#additional-steps-for-ecs-cluster-setup-optional-skip-this-step[[.underline]#Additional Steps for ECS Cluster Setup: (Optional&#44; Skip this step)#]

link:#dedicating-ecs-nodes-for-specific-workloads-optional-skip-this-step[[.underline]#Dedicating ECS nodes for specific workloads (Optional&#44; Skip this step)#]
____

link:#accessing-cdp-private-cloud[[.underline]#Accessing CDP Private Cloud#]

link:#cdp-private-cloud-machine-learning-cml[[.underline]#CDP Private Cloud Machine Learning (CML)#]

____
link:#ml-workspace-creation[[.underline]#ML Workspace Creation:#]

link:#creation-of-project-in-cml-workspace[[.underline]#Creation of Project in CML Workspace:#]
____

link:#cdp-private-cloud-data-warehouse-cdw[[.underline]#CDP Private Cloud Data Warehouse (CDW)#]

____
link:#enable-cdw-environment-and-creation-of-database-catalog[[.underline]#Enable CDW environment and creation of Database Catalog#]

link:#create-virtual-warehouse[[.underline]#Create Virtual Warehouse#]
____

link:#cdp-private-cloud-data-engineering-cde[[.underline]#CDP Private Cloud Data Engineering (CDE)#]

____
link:#cdp-base-cluster-requirements[[.underline]#CDP Base cluster requirements:#]

link:#enabling-cde-service[[.underline]#Enabling CDE Service:#]

link:#create-virtual-cluster[[.underline]#Create Virtual Cluster:#]

link:#initializing-virtual-cluster[[.underline]#Initializing Virtual Cluster#]

link:#configuring-ldap-users-on-cde[[.underline]#Configuring LDAP Users on CDE#]
____

link:#appendix[[.underline]#Appendix#]

____
link:#appendix-a-references-used-in-guide[[.underline]#Appendix A – References Used in Guide#]

link:#appendix-b-glossary-of-terms[[.underline]#Appendix B – Glossary of Terms#]

link:#appendix-c-glossary-of-acronyms[[.underline]#Appendix C – Glossary of Acronyms#]
____

link:#freeipa-reference[[.underline]#FreeIPA Reference#]

____
link:#add-users-on-freeipa[[.underline]#Add users on FreeIPA#]
____

link:#perform-the-pvc-base-cluster-validation[[.underline]#Perform the PvC Base Cluster Validation:#]

link:#cleanup-cdp-pvc-base-cluster[[.underline]#Cleanup CDP PvC Base Cluster:#]

link:#cleanup-cdp-pvc-data-services-ecs-cluster[[.underline]#Cleanup CDP PvC Data Services-ECS Cluster:#]

link:#cdp-pvc-base-cluster-error-handling[[.underline]#CDP PvC Base Cluster Error Handling#]

link:#cdp-pvc-data-services-ecs-cluster-error-handling[[.underline]#CDP PvC Data Services ECS Cluster Error Handling:#]

link:#kubernetes-command-reference[[.underline]#Kubernetes Command Reference:#]

link:#acknowledgements[[.underline]#Acknowledgements#]

---

== Author History

[width="100%",cols="31%,28%,41%",options="header",]
|===
a|
____
*Name
____

a|
____
*Version
____

a|
____
*Date
____

a|
____
Kuldeep Sahu
____

a|
____
1.0
____

a|
____
23-May-2024
____

|===

---

== CDP PvC Installation and Configuration on VMware Infrastructure Platform:

This document provides all the required information for setup and install CDP Private Cloud.

=== Important URLs:

**Install CDP PvC Base and Data Service Clusters:**

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-installation.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-installation.html#]

https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation-ecs/topics/cdppvc-installation-ecs-steps.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation-ecs/topics/cdppvc-installation-ecs-steps.html#]

**Uninstall and cleanup CDP PvC Base, Data Service Clusters and PostgreSQL DB:**

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-uninstallation.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-uninstallation.html#]

https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ecs-uninstall-pvc.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ecs-uninstall-pvc.html#]

https://kb.objectrocket.com/postgresql/how-to-completely-uninstall-postgresql-757[[.underline]#https://kb.objectrocket.com/postgresql/how-to-completely-uninstall-postgresql-757#]

**Internal documentation: Prerequisites list by Dennis Lee**

https://dennislee22.github.io/docs/cdppvc[[.underline]#https://dennislee22.github.io/docs/cdppvc#]

https://docs.google.com/document/d/1OSKBChSTbc8NhuQ8YXRN-YxFnaVBj47Lz4cWro-zTVs/edit[[.underline]#https://docs.google.com/document/d/1OSKBChSTbc8NhuQ8YXRN-YxFnaVBj47Lz4cWro-zTVs/edit#]

++=================================================================================++

=== [.underline]#Introduction#:

CDP Private Cloud is an integrated analytics and data management platform deployed in private data centers. Cloudera Data Platform is a single platform that has two form factors CDP Public and CDP Private cloud.

It consists of CDP Private Cloud Base and CDP Private Cloud Data Services and offers broad data analytics and artificial intelligence functionality along with secure user access and data governance features.

CDP Private Cloud (PvC) data services components run on containerized clusters and thus require a container orchestration engine to manage all the workloads.

There will be two major components in CDP PvC Installation:

* CDP PvC Base Cluster
* CDP PvC Data Services Cluster

CDP PvC DS offers installation with two orchestration engines.

* RedHat Openshift Container Platform (OCP)
* Embedded Container Service (Cloudera managed-ECS)

In this document, we focus on CDP Private Cloud Data Service Cluster setup with ECS.

=== CDP PVC setup consists of the following three parts.

image:image133.png[image133,width=509,height=225]

* *FreeIPA server:-* It provides the Identity and Authentication to the cluster. It includes Kerberos as the authentication provider and LDAP as directory service provider. All the cluster nodes (both Base and ECS) act as FreeIPA agents. (FreeIPA server includes Private DNS Server, MIT Kerberos KDC, Directory Server, Chrony, Dogtag certificate system, SSSD)

* *CDP Base Cluster:*- It consists of all the prerequisite services that form the basis for CDP Data Lake for Data Services.

* Atlas
* Solr
* HBase
* HDFS
* Hive (Metastore Server)
* Hive-on-Tez (HiveServer2)
* Hue (Required for CDW data service)
* Impala(Used as Client)
* Kafka
* Ozone (Required for CDE data service)
* Phoenix
* Ranger
* Tez
* YARN
* Yarn Queue Manager (Optional)
* ZooKeeper

* *CDP ECS Data Services Cluster:-* This is the Rancher (RKE) based Kubernetes cluster that forms the basis for all the containerized workloads of CDP Data Services. It consists of ECS master, ECS agents, and Docker servers.

Let’s have a look at the prerequisites before proceeding with the actual setup**.*

++**************************************************************************************************************++

== Solution Summary

This RA document details the process of installing CDP Private Cloud on VMware Servers and configuration details of fully tested and validated workloads in the cluster.

=== Prerequisites:

[.underline]#Entitlements#

Your License key must have the PvC DS entitlement. A current key without the entitlement will block access to ECS bits. Please raise a ticket or reach out to the Cloudera POC to get the necessary entitlements.

[.underline]#Virtual Machines#

Administrator access to virtual machines.

[.underline]#Infrastructure Setup: Hardware and Software Requirements#

Below table summarizes the machines used for this POC. This is a minimum requirement, One can increase the number of machines to achieve High Availability and Fault Tolerance. If this cluster is not meant to perform any benchmarking or performance test, one can proceed ahead with this infrastructure.

*Note: _The cluster configurations used in this document are designed and decided considering the installation/configuration and management of all 3 data services’ i.e. CML, CDW, CDE. with minimalistic workloads on a single ECS data services cluster for the PoC purpose. The hardware specs should be redetermined and recalculated for the clusters to set up for a different purpose from above mentioned._

[arabic]
. {blank}

[width="100%",cols="10%,90%",options="header",]
|===
a|
____
*Count
____

a|
____
*CDP Role
____

a|
____
1
____

a|
____
FreeIPA Server (Will be used for FreeIPA, Kerberos, Private DNS, LDAP, NTP, KDC, and will be used as an ansible controller node for automation purpose)
____

a|
____
1
____

a|
____
Cloudera-Manager Server (with external PostgreSQL Database server, will be used for downloading bits as well)
____

a|
____
6
____

a|
____
CDP Base Cluster (1 Master and 5 Worker Nodes)
____

a|
____
4-11
____

a|
____
CDP ECS Data Services Cluster (1 Master and 3-10 Worker Nodes)
____

|===

*DNS Server (In case we are not going with FreeIPA)

An external DNS server must contain the forward and reverse zones of the company domain name. The external DNS server must be able to resolve the hostname of all CDP PvC hosts and the 3rd party components (includes Kerberos, LDAP server, external database, NFS server) and perform reverse DNS lookup.

Wildcard DNS entry must be configured; e.g. *.apps.cldrsetup.local. This helps to reduce Day-2 operational tasks to set separate DNS entries for each newly provisioned external-facing application/service.

The external DNS server is expected to be ready prior to installing the CDP Private Cloud solution and its installation procedure is not covered in this document.

image:image44.png[image44,width=445,height=211]

*Kerberos + LDAP Server/AD + Certificate (Required only, in case we are not going with FreeIPA)

An external Kerberos server and the Kerberos key distribution center (KDC) (with a realm established) must be available to provide authentication to CDP services, users and hosts.

An external secured LDAP-compliant identity/directory server (LDAPS) is required to enable the CDP Private Cloud solution to look up for the user accounts and groups in the directory. This is expected to be ready prior to installing the CDP Private Cloud solution and its installation procedure is not covered in this document.

Auto-TLS should be enabled using certificates created and managed by a Cloudera Manager certificate authority (CA), or certificates signed by a trusted public CA or your own internal CA. Prepare the certificate of your choice.

The total number of CA certificates must not exceed 10. Otherwise, pods will be evicted during initialization due to limited memory (1Gi) to process the configmap file.

*External NFS (Preferable but optional; needed for CML use case)

CML requires an external NFS server to store the project files and directories. NFS version 4.1 must be supported.

The external NFS storage is expected to be ready prior to installing the CDP Private Cloud solution. External NFS storage installation is not covered in this document.

_This document covers the CDP Private Cloud setup and testing of the Data Services._

[width="100%",cols="50%,50%",options="header",]
|===
|image:image38.png[image38,width=281,height=210] |image:image86.png[image86,width=269,height=202]
|===

++**************************************************************************************************************++

== Hardware Requirements

NOTE: **Hardware specs e.g. CPU, memory, disk, etc. should be analyzed and re-determined as per the setup requirement e.g. POC, demo, HA, DR etc. Current setup is for POC/Demo purpose only.

For detailed requirements, refer to: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-requirements-supported-versions.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-requirements-supported-versions.html#]

=== [.underline]#Reverse Proxy Server (Optional: For external URLs, best practice perspective)#

[cols="2,1,1,1,1,1", options="header"]
|===
| Role | HostName | CPU | RAM | Disk | Partitions

| reverse proxy | proxy | 8 | 16GB | OS disk (100GB) | NA
|===

=== [.underline]#FreeIPA/Kerberos & Private DNS Server:# (In case we are not going with FreeIPA: External Kerberos/KDC Reqd.)

[width="100%",cols="27%,14%,8%,9%,15%,27%",options="header",]
|===
|*Role* |*HostName* |*CPU* |*RAM* |*Disk* |*Partitions
|ipaserver+ansible-controller |ipaserver |16 |32GB |OS disk (250GB) |large root partition
|cldr-mngr, postgres db, bits |cldr-mngr |32 |64GB |1.5TB a|
large root partition, /var=600GB

/opt=600GB

|===

=== PvC Base Cluster we will be installing a 6-node cluster on VMs:

** Here BaseMaster Node will also host Gateway and Utility hosts’ services as per public documentation at

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-runtime-cluster-hosts-role-assignments.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-runtime-cluster-hosts-role-assignments.html#]

** The Role assignment strategy for Control Plane Services’ (e.g. HDFS, YARN, Spark, etc.) is discussed in the later steps of PvC Base Cluster Setup.

[width="100%",cols="17%,16%,7%,8%,25%,27%",options="header",]
|===
|*Role* |*HostName* |*CPU* |*RAM* |*Disk* |*Partitions
|*BASE CLUSTER* | | | | |
|Base-Master |pvcbase-master |32 |64GB |root partition (1TB) |/hdfs /opt /var /yarn
|Base-Worker |pvcbase-worker1 |32 |64GB |root partition (1TB) |/hdfs /opt /var /yarn
|Base-Worker |pvcbase-worker2 |32 |64GB |root partition (1TB) |/hdfs /opt /var /yarn
|Base-Worker |pvcbase-worker3 |32 |64GB |root partition (1TB) |/hdfs /opt /var /yarn
|Base-Worker |pvcbase-worker4 |32 |64GB |root partition (1TB) |/hdfs /opt /var /yarn
|Base-Worker |pvcbase-worker5 |32 |64GB |root partition (1TB) |/hdfs /opt /var /yarn
|===

=== PvC Data Service (ECS) Cluster with CDW: we will be installing a 6-node cluster on VMs:

** Specs upgraded for concurrent tests and higher data volume tests and assumes only CDW services will be deployed

** Assuming Specs for 1 CDW Data Catalog, 1 CDV (DataViz) Small Instance, 1 Hive LLAP and 1 Impala Virtual Warehouse each with 1 coordinator and 2 executors.

[width="100%",cols="15%,13%,6%,6%,16%,44%",options="header",]
|===
|*Role* |*HostName* |*CPU* |*RAM* |*Disk* |*Partitions
|*ECS DS CLUSTER* |*CDW* | | | |
|ECS-Master |pvcecs-master |32 |128GB |root partition + 2.3TB |/cdwdata 500GB, /docker 400GB, /lhdata 1TB, /var 400GB
|ECS-Worker |pvcecs-worker1 |32 |128GB |root partition + 2.3TB |/cdwdata 500GB, /docker 400GB, /lhdata 1TB, /var 400GB
|ECS-Worker |pvcecs-worker2 |32 |128GB |root partition + 2.3TB |/cdwdata 500GB, /docker 400GB, /lhdata 1TB, /var 400GB
|ECS-Worker |pvcecs-worker3 |32 |128GB |root partition + 2.3TB |/cdwdata 500GB, /docker 400GB, /lhdata 1TB, /var 400GB
|ECS-Worker |pvcecs-worker4 |32 |128GB |root partition + 2.3TB |/cdwdata 500GB, /docker 400GB, /lhdata 1TB, /var 400GB
|ECS-Worker |pvcecs-worker5 |32 |128GB |root partition + 2.3TB |/cdwdata 500GB, /docker 400GB, /lhdata 1TB, /var 400GB
|ECS-Worker |pvcecs-worker6 |32 |128GB |root partition + 2.3TB |/cdwdata 500GB, /docker 400GB, /lhdata 1TB, /var 400GB
|===

=== PvC Data Service (ECS) Cluster with CML: we will be installing a 4-node cluster on VMs:

** Specs upgraded for concurrent tests and higher data volume tests and assumes only CML services will be deployed

** Assuming Specs for 1 CML Workspace with 10 small and 2 Average sized CML Concurrent Sessions.

[width="100%",cols="14%,13%,6%,6%,17%,44%",options="header",]
|===
|*Role* |*HostName* |*CPU* |*RAM* |*Disk* |*Partitions
|*ECS DS CLUSTER* |*CML* | | | |
|ECS-Master |pvcecs-master |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB *+ 1000GB NFS
|ECS-Worker |pvcecs-worker1 |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB
|ECS-Worker |pvcecs-worker2 |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB
|ECS-Worker |pvcecs-worker3 |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB
|===

=== PvC Data Service (ECS) Cluster with CDE: we will be installing a 4-node cluster on VMs:

** Specs upgraded for concurrent tests and higher data volume tests and assumes only CDE services will be deployed.

** Assuming Specs for 1 CDE Virtual service along with 1 Virtual Cluster with 5 small and 2 Average sized CDE Concurrent Jobs.

[width="100%",cols="14%,13%,6%,6%,17%,44%",options="header",]
|===
|*Role* |*HostName* |*CPU* |*RAM* |*Disk* |*Partitions
|*ECS DS CLUSTER* |*CDE* | | | |
|ECS-Master |pvcecs-master |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB *+ 500GB NFS
|ECS-Worker |pvcecs-worker1 |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB
|ECS-Worker |pvcecs-worker2 |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB
|ECS-Worker |pvcecs-worker3 |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB
|ECS-Worker |pvcecs-worker4 |32 |128GB |root partition + 2.6TB |/docker 400GB, /lhdata 1.5TB, /var 400GB
|===

=== PvC Data Service(ECS) Cluster with CDW+CDE+CML: we will be installing a 11-node cluster on VMs:

** Specs upgraded for concurrent tests and higher data volume tests and assumes all 3 services will be deployed (CDW, CDE, CML)

** Assuming Specs for 1 CDW Data Catalog, 1 CDV (DataViz Small) Instance, 1 Hive LLAP and 1 Impala Virtual Warehouse each with 1 coordinator and 2 executors.

** Assuming Specs for 1 CML Workspace with 10 small and 2 Average sized CML Concurrent Sessions.

** Assuming Specs for 1 CDE Virtual service along with 1 Virtual Cluster with 5 small and 2 Average sized CDE Concurrent Jobs.

[width="99%",cols="14%,14%,5%,7%,16%,44%",options="header",]
|===
|*Role* |*HostName* |*CPU* |*RAM* |*Disk* |*Partitions
|*ECS DS CLUSTER* |*CML+CDW+CDE* | | | |*+ 1500GB NFS
|ECS-Master |pvcecs-master |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker1 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker2 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker3 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker4 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker5 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker6 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker7 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker8 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker9 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|ECS-Worker |pvcecs-worker10 |32 |128GB |root partition + 2TB |/cdwdata 400GB, /docker 400GB, /lhdata 600GB, /var 400GB
|===

== Reference Architecture

*Data Lake (CDP Private Cloud Base) Reference Architecture

* Cloudera Data Platform: Private Cloud (CDP-PvC) Base *7.1.9 SP1 CHF4
* Cloudera Data Platform: Private Cloud (CDP-PvC) Data Services *1.5.4 CHF3

This RA document explains the architecture and deployment procedures for Cloudera Data Platform Private Cloud on cluster using VMware Infrastructure for Big Data and Analytics. The solution provides the details to configure CDP Private Cloud on the bare metal RHEL9 based infrastructure.

== Software Requirements

*Software Components

link:#kix.tzyxwjwv5hd0[[.underline]#Table#] 1 lists the software components and the versions required for a single cluster of the Servers running in VMware, as tested, and validated in this document.

Below table summarizes the list of softwares/packages and their use for setting up CDP PvC cluster.

[arabic, start=2]
. *Software Distributions and Firmware Versions

[width="100%",cols="39%,28%,33%",options="header",]
|===
a|
____
*Software Component
____

a|
____
*Version or Release
____

a|
____
*Host to be Installed
____

a|
____
OS: Red Hat Enterprise Linux Server (RHEL)
____

a|
____
9.x
____

a|
____
All Servers
____

a|
____
OpenJDK
____

|11.0.24.0.8-2 >= a|
____
All Servers
____

a|
____
Python3
____

a|
____
3.9.18 >=
____

a|
____
All Servers
____

a|
____
PostgreSQL DB
____

a|
____
14.13 >=
____

a|
____
Cldr-Mngr
____

a|
____
Psycopg2-binary
____

a|
____
2.9.9 >=
____

a|
____
All Servers
____

|Postgres-JDBC-Connector a|
____
42.7.3 >=
____

a|
____
All Servers
____

a|
____
Cloudera Manager
____

a|
____
7.11.3-CHF11 (7.11.3.28-60766845)
____

a|
____
Cldr-Mngr
____

a|
____
Cloudera Private Cloud Base (RunTime)
____

a|
____
7.1.9 SP1 CHF4
____

a|
____
PvC Base Cluster Nodes
____

a|
____
Cloudera Private Cloud Data Services
____

a|
____
1.5.4 CHF3 *(1.5.4-h4-b27)
____

a|
____
PvC Data Service Cluster Nodes
____

a|
____
CDP Parcel
____

a|
____
7.1.9-1.cdh7.1.9.p1023.60818430
____

a|
____
PvC Base Cluster Nodes
____

a|
____
Hadoop (Includes YARN and HDFS)
____

a|
____
3.1.1.7.1.9.1000-103
____

a|
____
PvC Base Cluster Nodes
____

a|
____
Spark2
____

a|
____
2.4.8.7.1.9.1000-103
____

a|
____
PvC Base Cluster Nodes
____

a|
____
Spark3
____

a|
____
3.3.2.3.3.7191000.4-2-1 https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/cds-3/topics/spark-spark-3-overview.html[[.underline]#CDS#]
____

a|
____
PvC Base Cluster Nodes
____

a|
____
Ozone
____

a|
____
1.3.0.7.1.9.1000-103
____

a|
____
PvC Base Cluster Nodes
____

a|
____
FreeIPA Server
____

a|
____
Latest
____

a|
____
ipa server node
____

a|
____
FreeIPA Client
____

a|
____
Latest
____

a|
____
All nodes except ipaserver
____

a|
____
NFS Utility Package
____

a|
____
Latest
____

a|
____
PvC Data Service Cluster Nodes
____

|===

[arabic]
. Please check the CDP Private Cloud requirements and supported versions for information about hardware, operating system, and database requirements, as well as product compatibility matrices, here: https://supportmatrix.cloudera.com/%20[[.underline]#https://supportmatrix.cloudera.com/#] and here:

____
https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/release-guide/topics/cdpdc-requirements-supported-versions.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/release-guide/topics/cdpdc-requirements-supported-versions.html#]
____

[arabic, start=2]
.   For Cloudera Private Cloud Base and Experiences versions and supported features, go to:

____
https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/runtime-release-notes/topics/rt-pvc-runtime-component-versions.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/runtime-release-notes/topics/rt-pvc-runtime-component-versions.html#]
____

[arabic, start=3]
. For Cloudera Private Cloud Base requirements and supported version, go to:

____
https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-requirements-supported-versions.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-requirements-supported-versions.html#]
____

[arabic, start=4]
. Dedicated *_NVMe/SSD drives_* are recommended to store *_Ozone metadata, Ozone mgmt_* configuration for the admin/mgmt. nodes and worker/data nodes and *_CDW data service storage_* for virtual warehouses for local attached Storage Tiering Cache.

== [.underline]#Summary#

The below table contains the names assigned to the VM instances and to some other required components. Going forward in this document will refer to them by name.

Note: The domain name, and the hostnames mentioned here are just for reference. You may choose to have the hostnames as per your requirements.

[arabic, start=3]
. {blank}

[width="100%",cols="52%,48%",options="header",]
|===
a|
____
*NodeName
____

a|
____
*Details
____

a|
____
pvcbase-master
____

a|
____
CDP Private Cloud Base Master
____

a|
____
pvcbase-worker1 to pvcbase-worker5
____

a|
____
CDP Base Cluster Worker Nodes
____

a|
____
ipaserver (OR Existing LDAP/AD + DNS + Kerberos + KDC)
____

a|
____
FreeIPA Server
____

a|
____
cldr-mngr
____

a|
____
Cloudera-Manager and PostgreSQL DB Server
____

a|
____
pvcecs-master
____

a|
____
ECS Master Node
____

a|
____
pvcecs-worker1 to pvcecs-worker10
____

a|
____
ECS Worker Nodes
____

a|
____
CLDRSETUP.LOCAL (Replace with your ORG DOMAIN)
____

a|
____
Dummy Domain For POC Purpose
____

|===

*Once you have familiarized yourself with all the information mentioned above, you can start with the preliminary work for CDP Base Cluster setup.

++**************************************************************************************************************++

== Post OS Installation

=== Preliminary Work

Before getting into the actual installation of CDP Private Cloud Base & Data Services clusters, we need to prepare our machines and perform some steps to meet the prerequisites.

Choose one of the nodes of the cluster or a separate node as the Ansible Admin/Controller Node for management. In this document, we configured the ipaserver for this purpose.

[arabic]
. Configure individual servers’ static hostnames and prepare /etc/hosts file

[arabic]
. Setup the hostname *_for each individual node_* by logging in using the IP addresses provided by the VMware team, so we can refer to them with names instead of IP addresses for simplicity and ease of identification. While you are configuring the hostname, also follow *Step 2* while logging in to each host. *Replace your ORG DOMAIN

[source,bash]
----
[root@ipaserver ~]# sudo hostnamectl set-hostname --static ipaserver.cldrsetup.local
[root@cldr-mngr ~]# sudo hostnamectl set-hostname --static cldr-mngr.cldrsetup.local

[root@pvcbase-master ~]# sudo hostnamectl set-hostname --static pvcbase-master.cldrsetup.local
[root@pvcbase-worker1 ~]# sudo hostnamectl set-hostname --static pvcbase-worker1.cldrsetup.local
[root@pvcbase-worker2 ~]# sudo hostnamectl set-hostname --static pvcbase-worker2.cldrsetup.local
[root@pvcbase-worker3 ~]# sudo hostnamectl set-hostname --static pvcbase-worker3.cldrsetup.local
[root@pvcbase-worker4 ~]# sudo hostnamectl set-hostname --static pvcbase-worker4.cldrsetup.local
[root@pvcbase-worker5 ~]# sudo hostnamectl set-hostname --static pvcbase-worker5.cldrsetup.local

[root@pvcecs-master ~]# sudo hostnamectl set-hostname --static pvcecs-master.cldrsetup.local
[root@pvcecs-worker1 ~]# sudo hostnamectl set-hostname --static pvcecs-worker1.cldrsetup.local
[root@pvcecs-worker2 ~]# sudo hostnamectl set-hostname --static pvcecs-worker2.cldrsetup.local
[root@pvcecs-worker3 ~]# sudo hostnamectl set-hostname --static pvcecs-worker3.cldrsetup.local
[root@pvcecs-worker4 ~]# sudo hostnamectl set-hostname --static pvcecs-worker4.cldrsetup.local
[root@pvcecs-worker5 ~]# sudo hostnamectl set-hostname --static pvcecs-worker5.cldrsetup.local
[root@pvcecs-worker6 ~]# sudo hostnamectl set-hostname --static pvcecs-worker6.cldrsetup.local
[root@pvcecs-worker7 ~]# sudo hostnamectl set-hostname --static pvcecs-worker7.cldrsetup.local
[root@pvcecs-worker8 ~]# sudo hostnamectl set-hostname --static pvcecs-worker8.cldrsetup.local
[root@pvcecs-worker9 ~]# sudo hostnamectl set-hostname --static pvcecs-worker9.cldrsetup.local
[root@pvcecs-worker10 ~]# sudo hostnamectl set-hostname --static pvcecs-worker10.cldrsetup.local
----

[arabic, start=2]
. While you set the hostnames by logging in to each individual hosts, make sure to run the dnf update and install python3 dependencies as well, since these are fresh nodes:

** Python3 can be installed manually on bare minimum (ipaserver/ansible admin) and can be later installed using ansible on the rest of the nodes. (Only, If you don't want it to install on each individual node)

[arabic, start=3]
. Log into the ipaserver Node using IP provided previously by the VMware team.

[source,bash]
----
[root@ipaserver ~]# ssh 172.31.24.240
----

[arabic, start=4]
. Setup /etc/hosts on the ipaserver node; this is a pre-configuration to setup Private DNS as shown in the next section. In large scale production grade deployment, DNS server setup is highly recommended.

Populate the host file with IP addresses and corresponding hostnames on the ipaserver node by taking the private IP of machine and add an entry in /etc/hosts file as follows: *_(All of below mentioned IPs are private IP addresses)_

(We will later copy the same hosts file to all other nodes with the help of ansible)

[source,bash]
----
[root@ipaserver ~]# sudo vi /etc/hosts
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6

# Free-IPA Server
172.31.24.240 ipaserver.cldrsetup.local ipaserver

# Cloudera Manager Server
172.31.1.38 cldr-mngr.cldrsetup.local cldr-mngr
172.31.1.38 postgresdb.cldrsetup.local postgresdb

# PvC Base Cluster Nodes
172.31.1.34 pvcbase-master.cldrsetup.local pvcbase-master
172.31.1.35 pvcbase-worker1.cldrsetup.local pvcbase-worker1
172.31.1.36 pvcbase-worker2.cldrsetup.local pvcbase-worker2
172.31.1.37 pvcbase-worker3.cldrsetup.local pvcbase-worker3
172.31.1.30 pvcbase-worker4.cldrsetup.local pvcbase-worker4
172.31.1.31 pvcbase-worker5.cldrsetup.local pvcbase-worker5

# PvC Data Services ECS Cluster Nodes
172.31.30.239 pvcecs-master.cldrsetup.local pvcecs-master
172.31.22.43 pvcecs-worker1.cldrsetup.local pvcecs-worker1
172.31.30.249 pvcecs-worker2.cldrsetup.local pvcecs-worker2
172.31.26.24 pvcecs-worker3.cldrsetup.local pvcecs-worker3
172.31.24.198 pvcecs-worker4.cldrsetup.local pvcecs-worker4
172.31.24.53 pvcecs-worker5.cldrsetup.local pvcecs-worker5
172.31.22.44 pvcecs-worker6.cldrsetup.local pvcecs-worker6
172.31.30.250 pvcecs-worker7.cldrsetup.local pvcecs-worker7
172.31.26.25 pvcecs-worker8.cldrsetup.local pvcecs-worker8
172.31.24.199 pvcecs-worker9.cldrsetup.local pvcecs-worker9
172.31.24.54 pvcecs-worker10.cldrsetup.local pvcecs-worker10
----

[arabic, start=5]
. Perform the basic validation of OS version and hostname/IP configurations:

[source,bash]
----
## Ensure that the OS version is RHEL 9.x.
## To verify the version, run the below command. It should return RedHat Linux version 9.x.

[root@ipaserver ~]# cat /etc/*rel* |grep -E 'NAME|VERSION'
NAME="Red Hat Enterprise Linux"
VERSION="9.4 (Plow)"
VERSION_ID="9.4"
PRETTY_NAME="Red Hat Enterprise Linux 9.4 (Plow)"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"

## Verify Hostname and IP addresses

[root@ipaserver ~]# hostname -f
ipaserver.cldrsetup.local

[root@ipaserver ~]# hostname -i
172.31.24.240

[root@ipaserver ~]# cat /etc/hostname
ipaserver.cldrsetup.local

[root@ipaserver ~]# ip addr show eth0 | grep -e inet
inet 10.0.2.2/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0
inet6 fe80::c3:16ff:fe00:9/64 scope link noprefixroute

[root@ipaserver ~]# ip addr show eth1 | grep -e inet
inet 172.31.24.240/24 brd 172.31.1.255 scope global dynamic noprefixroute eth1
inet6 fe80::65b3:25c6:8b2a:b4ae/64 scope link noprefixroute

[root@ipaserver ~]# ip addr show |grep $(hostname -i)
inet 172.31.24.240/24 brd 172.31.1.255 scope global dynamic noprefixroute eth1

[root@ipaserver ~]# host -v -t A $(hostname) | grep -A2 ANSWER
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
--
;; ANSWER SECTION:
ipaserver.cldrsetup.local. 1200 IN A *172.31.24.240

[root@ipaserver ~]#
[root@ipaserver ~]# uname -a
Linux ipaserver.cldrsetup.local 5.14.0-427.26.1.el9_4.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Jul 5 11:34:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
----

[arabic, start=2]
. Setup ipaserver (which includes Private DNS Server, MIT Kerberos KDC, Directory Server, Chrony, Dogtag certificate system, SSSD)

=== Install and Setup of IPA services

In this step, a Private DNS server and other services like KDC, Directory Service will be configured on the ipaserver.

Also, please note that the hostnames used in this installation can be modified as per your requirements.

*Follow the on screen instructions and provide the inputs for the parameters as per the table below.

[width="100%",cols="60%,40%",options="header",]
|===
a|
____
*Parameter
____

a|
____
*Value
____

a|
____
Server host name [ipaserver.cldrsetup.local]:
____

a|
____
ipaserver.cldrsetup.local
____

a|
____
Please confirm the domain name [cldrsetup.local]:
____

a|
____
cldrsetup.local
____

a|
____
Please provide a realm name [CLDRSETUP.LOCAL]:
____

a|
____
CLDRSETUP.LOCAL
____

a|
____
Directory Manager password:
____

a|
____
<Password For Directory Manager> *_(vmware123)_
____

a|
____
Password (confirm):
____

a|
____
<Confirm Password> *_(vmware123)_
____

a|
____
IPA admin password:
____

a|
____
<Password For IPA Admin> *_(vmware123)_
____

a|
____
Password (confirm):
____

a|
____
<Confirm Password> *_(vmware123)_
____

a|
____
Do you want to configure DNS forwarders? [yes]:
____

a|
____
<ENTER>
____

a|
____
Do you want to search for missing reverse zones?[yes]:
____

a|
____
no
____

a|
____
NetBIOS domain name [CLDRSETUP]:
____

a|
____
CLDRSETUP
____

a|
____
Do you want to configure chrony with NTP server or pool address? [no]:
____

a|
____
yes
____

a|
____
Enter NTP source server addresses separated by comma, or press Enter to skip:
____

a|
____
<ENTER>
____

a|
____
Enter a NTP source pool address, or press Enter to skip:
____

a|
____
<ENTER>
____

a|
____
Continue to configure the system with these values?[no]:
____

a|
____
yes
____

|===

Please keep the same password for both Directory manager and IPA admin so that there is no confusion in future while using the same. Also, note down the password separately.

[arabic]
. Login to IPAServer node and Install ipa-server packages:

[source,bash]
----
# Install ipa server dependencies packages through dnf using the below command.
[root@ipaserver ~]# sudo dnf install -y ipa-server bind bind-dyndb-ldap ipa-server-dns firewalld

# If required, use below command to set the java version
[root@ipaserver ~]# update-alternatives --config java

# Install ipa server dependencies packages through dnf using the below command.
[root@ipaserver ~]# sudo dnf install -y ipa-server bind bind-dyndb-ldap ipa-server-dns firewalld

# If required, use below command to set the java version
[root@ipaserver ~]# update-alternatives --config java

# Configure ipa-server and DNS by using command: ipa-server-install --setup-dns
[root@ipaserver ~]# ipa-server-install --setup-dns

The log file for this installation can be found in /var/log/ipaserver-install.log
==============================================================================
This program will set up the IPA Server.
Version 4.11.0

This includes:
  * Configure a stand-alone CA (dogtag) for certificate management
  * Configure the NTP client (chronyd)
  * Create and configure an instance of Directory Server
  * Create and configure a Kerberos Key Distribution Center (KDC)
  * Configure Apache (httpd)
  * Configure DNS (bind)
  * Configure SID generation
  * Configure the KDC to enable PKINIT

To accept the default shown in brackets, press the Enter key.

Enter the fully qualified domain name of the computer
on which you're setting up server software. Using the form
<hostname>.<domainname>
Example: ipaserver.cldrsetup.local

Server host name [ipaserver.cldrsetup.local]: <ENTER>

Warning: skipping DNS resolution of host ipaserver.cldrsetup.local
The domain name has been determined based on the host name.

Please confirm the domain name [cldrsetup.local]: <ENTER>

The kerberos protocol requires a Realm name to be defined.
This is typically the domain name converted to uppercase.

Please provide a realm name [CLDRSETUP.LOCAL]: <ENTER>
Certain directory server operations require an administrative user.
This user is referred to as the Directory Manager and has full access
to the Directory for system management tasks and will be added to the
instance of directory server created for IPA.
The password must be at least 8 characters long.

Directory Manager password: <vmware123>
Password (confirm): <vmware123>

The IPA server requires an administrative user, named 'admin'.
This user is a regular system account used for IPA server administration.

IPA admin password: <vmware123>
Password (confirm): <vmware123>

Checking DNS domain cldrsetup.local., please wait ...
Do you want to configure DNS forwarders? [yes]: no
No DNS forwarders configured
Do you want to search for missing reverse zones? [yes]: no
Trust is configured but no NetBIOS domain name found, setting it now.
Enter the NetBIOS name for the IPA domain.
Only up to 15 uppercase ASCII letters, digits and dashes are allowed.
Example: EXAMPLE.

NetBIOS domain name [CLDRSETUP]: <ENTER>

Do you want to configure chrony with NTP server or pool address? [no]: yes
Enter NTP source server addresses separated by comma, or press Enter to skip: <ENTER>
Enter a NTP source pool address, or press Enter to skip: <ENTER>

The IPA Master Server will be configured with:
Hostname:       ipaserver.cldrsetup.local
IP address(es): 172.31.24.240
Domain name:    cldrsetup.local
Realm name:     CLDRSETUP.LOCAL

The CA will be configured with:
Subject DN:   CN=Certificate Authority,O=CLDRSETUP.LOCAL
Subject base: O=CLDRSETUP.LOCAL
Chaining:     self-signed

BIND DNS server will be configured to serve IPA domain with:
Forwarders:       No forwarders
Forward policy:   only
Reverse zone(s):  No reverse zone

Continue to configure the system with these values? [no]: yes

The following operations may take some minutes to complete.
Please wait until the prompt is returned.

Disabled p11-kit-proxy
Synchronizing time
No SRV records of NTP servers were found and no NTP server or pool address was provided.
Using default chrony configuration.
Attempting to sync time with chronyc.
Time synchronization was successful.
Configuring directory server (dirsrv). Estimated time: 30 seconds
  [1/43]: creating directory server instance
Validate installation settings ...
Create file system structures ...
Perform SELinux labeling ...
Create database backend: dc=cldrsetup,dc=local ...
Perform post-installation tasks ...
  [2/43]: tune ldbm plugin
  [3/43]: adding default schema
—-----
—-----
—-----
—-----
—-----
  [6/8]: restarting Directory Server to take MS PAC and LDAP plugins changes into account
  [7/8]: adding fallback group
  [8/8]: adding SIDs to existing users and groups
This step may take a considerable amount of time, please wait..
Done.
Configuring client side components
This program will set up an IPA client.
Version 4.11.0

Using the existing certificate '/etc/ipa/ca.crt'.
Client hostname: ipaserver.cldrsetup.local
Realm: CLDRSETUP.LOCAL
DNS Domain: cldrsetup.local
IPA Server: ipaserver.cldrsetup.local
BaseDN: dc=cldrsetup,dc=local

Configured /etc/sssd/sssd.conf
Systemwide CA database updated.
Adding SSH public key from /etc/ssh/ssh_host_rsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ecdsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ed25519_key.pub
SSSD enabled
Configured /etc/openldap/ldap.conf
Configured /etc/ssh/ssh_config
Configured /etc/ssh/sshd_config.d/04-ipa.conf
Configuring cldrsetup.local as NIS domain.
Client configuration complete.
The ipa-client-install command was successful

==============================================================================
Setup complete

Next steps:
        1. You must make sure these network ports are open:
                TCP Ports:
                  * 80, 443: HTTP/HTTPS
                  * 389, 636: LDAP/LDAPS
                  * 88, 464: kerberos
                  * 53: bind
                UDP Ports:
                  * 88, 464: kerberos
                  * 53: bind
                  * 123: ntp

        2. You can now obtain a kerberos ticket using the command: 'kinit admin'
           This ticket will allow you to use the IPA tools (e.g., ipa user-add)
           and the web user interface.

Be sure to back up the CA certificates stored in /root/cacert.p12
These files are required to create replicas. The password for these
files is the Directory Manager password
The ipa-server-install command was successful
[root@ipaserver ~]#

##### Disable the firewall on ipaserver to be able to connect from rest of hosts
[root@ipaserver ~]# systemctl stop firewalld
[root@ipaserver ~]# systemctl disable firewalld
Removed "/etc/systemd/system/multi-user.target.wants/firewalld.service".
Removed "/etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service".
[root@ipaserver ~]# 

##### If Fail, do: If the installation fails, then run the below command to uninstall and retry with the above command for installation.
[root@ipaserver ~]# ipa-server-install --uninstall
[root@ipaserver ~]# ipa-server-install --setup-dns (again)

##### Disable the firewall on ipaserver to be able to connect from rest of hosts
[root@ipaserver ~]# systemctl stop firewalld
[root@ipaserver ~]# systemctl disable firewalld
Removed "/etc/systemd/system/multi-user.target.wants/firewalld.service".
Removed "/etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service".
[root@ipaserver ~]#
----

The setup will take 10-15 Minutes. If everything goes fine then you should get an output similar to the below screenshot.
image:image114.png[ipa server setup,width=504,height=272]

[arabic, start=2]
. Verify KDC setup: kerberos ticket is working fine by generating a ticket for the admin user.

[source,bash]
----
##### Run the kinit admin command to authenticate as admin and enter the directory password provided during ipa server installation. The command should generate the ticket and should be listed by executing klist -e. 

[root@ipaserver ~]# kinit admin 
Password for admin@CLDRSETUP.LOCAL: <vmware123>

[root@ipaserver ~]# klist -e
Ticket cache: KCM:0
Default principal: admin@CLDRSETUP.LOCAL

Valid starting       Expires              Service principal
05/13/2024 04:07:15  05/14/2024 03:30:48  krbtgt/CLDRSETUP.LOCAL@CLDRSETUP.LOCAL
        Etype (skey, tkt): aes256-cts-hmac-sha384-192, aes256-cts-hmac-sha384-192 

##### try kinit admin@CLDRSETUP.LOCAL
##### (if fails anytime, run below commands)
[root@ipaserver ~]# ipactl stop && ipactl start && ipactl status

##### Verify the status of ipa services installed

[root@ipaserver ~]# ipactl status
Directory Service: RUNNING
krb5kdc Service: RUNNING
kadmin Service: RUNNING
named Service: RUNNING
httpd Service: RUNNING
ipa-custodia Service: RUNNING
pki-tomcatd Service: RUNNING
ipa-otpd Service: RUNNING
ipa-dnskeysyncd Service: RUNNING
ipa: INFO: The ipactl command was successful
----

This command should return the below output:

image:image23.png[klist verify,width=624,height=184]

[arabic, start=3]
. *(Optional)* Enable WebUI for IPAServer Administration:

##### Add IPAserver IP address mapping to your local system’s (Laptop) /etc/hosts file, similar to as below.

[source,bash]
----
*ksahu@Kuldeeps-MacBook-Air % sudo vi /etc/hosts
35.83.155.109 ipaserver.cldrsetup.local ipaserver
----

##### Access the below URL on browser, and the IPA Admin console will open.
https://ipaserver.cldrsetup.local/ipa/ui/[[.underline]#https://ipaserver.cldrsetup.local/ipa/ui/#]

[arabic, start=4]
. You will see below WebUI for IPAServer Administration. Enter the same admin credentials used for CLI authentication: *_(i.e. admin/vmware123)_

image:image12.png[image12,width=564,height=339]

[arabic, start=5]
. Below management console will get appear after the successful authentication:

image:image163.png[image163,width=635,height=363]

[arabic, start=3]
. Set Up Password-less Login

To manage all the nodes in a cluster from the admin/controller node, password-less login needs to be set up. It assists in automating common tasks with Ansible, and shell-scripts without having to use passwords.

Enable the passwordless login across all the nodes when Red Hat Linux is installed across all the nodes in the cluster.

*Step 1.* Log into the ipaserver Node.

[source,bash]
----
[root@ipaserver ~]# ssh 172.31.24.240
----

*Step 2.* Run the ssh-keygen command to create both public and private SSH key-pair on the ansible-controller node.

[source,bash]
----
[root@ipaserver ~]# ssh-keygen -N '' -f ~/.ssh/id_rsa
[root@ipaserver ~]# ls -l /root/.ssh
[root@ipaserver ~]# chmod 600 /root/.ssh/id_rsa
----

image:image45.png[ssh keygen pw,width=417,height=65]

*Step 3.* Run the following command from the ansible-controller/ipaserver node to copy the public key id_rsa.pub to all the nodes of the cluster. ssh-copy-id appends the keys to the remote-hosts .ssh/authorized_keys.

_*(NA in AWS EC2, need to be done manually, as right now password based authentication for non-root users is disabled*)_

[source,bash]
----
[root@ipaserver ~]# for i in {1}; do echo "copying ipaserver.cldrsetup.local"; ssh-copy-id -i ~/.ssh/id_rsa.pub root@ipaserver.cldrsetup.local; done;
[root@ipaserver ~]# for i in {1}; do echo "copying cldr-mngr.cldrsetup.local"; ssh-copy-id -i ~/.ssh/id_rsa.pub root@cldr-mngr.cldrsetup.local; done;
[root@ipaserver ~]# for i in {1}; do echo "copying pvcbase-master.cldrsetup.local"; ssh-copy-id -i ~/.ssh/id_rsa.pub root@pvcbase-master.cldrsetup.local; done;
[root@ipaserver ~]# for i in {1..5}; do echo "copying pvcbase-worker$i.cldrsetup.local"; ssh-copy-id -i ~/.ssh/id_rsa.pub root@pvcbase-worker$i.cldrsetup.local; done;
[root@ipaserver ~]# for i in {1}; do echo "copying pvcecs-master.cldrsetup.local"; ssh-copy-id -i ~/.ssh/id_rsa.pub root@pvcecs-master.cldrsetup.local; done;
[root@ipaserver ~]# for i in {1..10}; do echo "copying pvcecs-worker$i.cldrsetup.local"; ssh-copy-id -i ~/.ssh/id_rsa.pub root@pvcecs-worker$i.cldrsetup.local; done;

##### Alternate way is to add pub key to authorized_keys file manually on ipaserver node and copy the entire .ssh directory to all other NODES; otherwise login into each hosts and manually update authorized_keys:
[root@ipaserver ~]# cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

##### copy the entire .ssh directory to all NODES
[root@ipaserver ~]# scp -r /root/.ssh mailto:root@cldr-mngr.cldrsetup.local[[.underline]#root@cldr-mngr.cldrsetup.local#]:/root/.

##### (provide root user password when prompted)
##### Download the id_rsa and id_rsa.pub to your local machine by either using scp or sftp (as it will be required later)
----

*Step 4.* Enter yes for *_Are you sure you want to continue connecting (yes/no)_*?

*Step 5.* Enter the password of the remote host.

[arabic, start=4]
. Set up Ansible (We will be using ipaserver as ansible controller/admin node)

*Step 1.* Login to IPAServer node and Install ansible-core

[source,bash]
----
[root@ipaserver ~]# dnf install -y ansible-core
[root@ipaserver ~]# ansible --version
ansible [core 2.14.14]
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3.9/site-packages/ansible
  ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections
  executable location = /bin/ansible
  python version = 3.9.18 (main, Jan 24 2024, 00:00:00) [GCC 11.4.1 20231218 (Red Hat 11.4.1-3)] (/usr/bin/python3)
  jinja version = 3.1.2
  libyaml = True
[root@ipaserver ~]# echo "export ANSIBLE_HOST_KEY_CHECKING=False" >> ~/.bashrc && source ~/.bashrc
----

*Step 2.* Prepare the host inventory file for Ansible as shown below. Various host groups have been created based on any specific installation requirements of certain hosts.

[source,bash]
----
[root@ipaserver ~]# vi /etc/ansible/hosts

[admin]
ipaserver.cldrsetup.local

[ipaserver]
ipaserver.cldrsetup.local

[cldr-mngr]
cldr-mngr.cldrsetup.local

[namenodes]
pvcbase-master.cldrsetup.local

[datanodes]
pvcbase-worker1.cldrsetup.local
pvcbase-worker2.cldrsetup.local
pvcbase-worker3.cldrsetup.local
pvcbase-worker4.cldrsetup.local
pvcbase-worker5.cldrsetup.local

[ecsmasternodes]
pvcecs-master.cldrsetup.local

[ecsnodes]
pvcecs-worker1.cldrsetup.local
pvcecs-worker2.cldrsetup.local
pvcecs-worker3.cldrsetup.local
pvcecs-worker4.cldrsetup.local
pvcecs-worker5.cldrsetup.local
pvcecs-worker6.cldrsetup.local
pvcecs-worker7.cldrsetup.local
pvcecs-worker8.cldrsetup.local
pvcecs-worker9.cldrsetup.local
pvcecs-worker10.cldrsetup.local

[nodes]
pvcbase-master.cldrsetup.local
pvcbase-worker1.cldrsetup.local
pvcbase-worker2.cldrsetup.local
pvcbase-worker3.cldrsetup.local
pvcbase-worker4.cldrsetup.local
pvcbase-worker5.cldrsetup.local
pvcecs-master.cldrsetup.local
pvcecs-worker1.cldrsetup.local
pvcecs-worker2.cldrsetup.local
pvcecs-worker3.cldrsetup.local
pvcecs-worker4.cldrsetup.local
pvcecs-worker5.cldrsetup.local
pvcecs-worker6.cldrsetup.local
pvcecs-worker7.cldrsetup.local
pvcecs-worker8.cldrsetup.local
pvcecs-worker9.cldrsetup.local
pvcecs-worker10.cldrsetup.local
----

*Step 3.* Verify the host group by running the following commands.

[source,bash]
----
[root@ipaserver ~]# ansible datanodes -m ping
pvcbase-worker2.cldrsetup.local | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
pvcbase-worker4.cldrsetup.local | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
pvcbase-worker3.cldrsetup.local | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
pvcbase-worker1.cldrsetup.local | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
pvcbase-worker5.cldrsetup.local | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
----

*Step 4.* Copy /etc/hosts file to each node part of the cloudera deployment to resolve fqdn across the cluster

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/hosts dest=/etc/hosts"
----

[arabic, start=5]
. Set up Network configuration files and DNS Zones/Records

[arabic]
. We will update the network configuration file *_/etc/resolv.conf_* on the IPA server to use the Name Server created in previous steps and will later copy this file to the rest of nodes using ansible (after installing freeipa-client, as it overrides resolv.conf and may lead to rework) to make them able to resolve FQDNs across the cluster:

(Open the file /etc/resolv.conf in edit mode and add the following. Make sure the new entry is added above any other nameserver entry. The contents of the file must look similar to the below.)

*Note*: ​​Make sure that the *_/etc/resolv.conf_* file on the ECS hosts *_contains a maximum of 2 active search domains_*.

https://docs.cloudera.com/data-warehouse/1.5.4/release-notes/topics/dw-private-cloud-known-issues-ecs-environments.html[[.underline]#https://docs.cloudera.com/data-warehouse/1.5.4/release-notes/topics/dw-private-cloud-known-issues-ecs-environments.html#]

[source,bash]
----
[root@ipaserver ~]# cat /etc/resolv.conf
search ap-southeast-1.compute.internal cldrsetup.local
nameserver 172.31.24.240 # PrivateIP of FreeIPA Server must be first nameserver entry after search
nameserver 172.31.0.2 # DNS of AWS i.e. in case of PvC Configured on EC2
nameserver 127.0.0.1

[root@ipaserver ~]#
----

image:image32.png[resolv conf,width=428,height=59]

[arabic, start=2]
. We will update the network configuration file *_/etc/sysconfig/network_* on the IPA server to use the Name Server created in previous steps and will later copy this file to the rest of nodes to make them able to resolve FQDNs across the cluster:

(The changes in *_/etc/resolv.conf_* above are temporary and would get overwritten if the machine is rebooted. In order to keep the nameserver entry persistent, open the file *_/etc/sysconfig/network_* in edit mode and add below entries.)

[source,bash]
----
[root@ipaserver ~]# cat /etc/sysconfig/network
NETWORKING=yes
NISDOMAIN=cldrsetup.local # our DNS DOMAIN
DNS1=172.31.24.240 # PRIVATE_IP_OF_IPASERVER
NOZEROCONF=yes

[root@ipaserver ~]#
----

image:image55.png[sysconfig network,width=213,height=79]

[arabic, start=3]
. Copy */etc/resolv.conf* file to each node to make them able to resolve FQDNs across the cluster:

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/resolv.conf dest=/etc/resolv.conf" --become
----

[arabic, start=4]
. Copy */etc/sysconfig/network* file to each node to make them able to resolve FQDNs across the cluster: (/etc/resolv.conf changes may vanished after the reboot, so to persist those changes, we need the below configuration)

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/sysconfig/network dest=/etc/sysconfig/network" --become
----

[arabic, start=5]
. Update Network config to make sure DNS entries persist, even after reboot:

[source,bash]
----
# Extract DNS entries
[root@ipaserver ~]# grep '^nameserver' /etc/resolv.conf | awk '{print "DNS" NR "=" $2}' > /tmp/dns_entries.txt

# Update ifcfg-eth0 with DNS entries
[root@ipaserver ~]# while IFS= read -r line; do
ansible all -m lineinfile -a "path=/etc/sysconfig/network-scripts/ifcfg-eth0 line='${line}' state=present backup=true" --become
done < /tmp/dns_entries.txt

# Clean up
[root@ipaserver ~]# rm -vf /tmp/dns_entries.txt
----

[arabic, start=6]
. Setup Reverse DNS Zone on ipaserver, –from-ip is VPC-CIDR In this step we will be setting up a reverse DNS zone on the FreeIPA server for reverse lookup:

[source,bash]
----
##### Take the CIDR block of the network in which the instances are created and create a reverse DNS zone by executing the below command on the IPA Server machine.
##### ipa dnszone-add --name-from-ip=<YOUR_VPC_CIDR>

##### If your VPC has a CIDR 172.16.0.0/16, then the command looks as below.

[root@ipaserver ~]# ipa dnszone-add --name-from-ip=172.31.0.0/16
Zone name [31.172.in-addr.arpa.]: 
  Zone name: 31.172.in-addr.arpa.
  Active zone: True
  Authoritative nameserver: ipaserver.cldrsetup.local.
  Administrator e-mail address: hostmaster
  SOA serial: 1715598489
  SOA refresh: 3600
  SOA retry: 900
  SOA expire: 1209600
  SOA minimum: 3600
  BIND update policy: grant CLDRSETUP.LOCAL krb5-subdomain 31.172.in-addr.arpa. PTR;
  Dynamic update: False
  Allow query: any;
  Allow transfer: none;

##### Once you execute the above command, accept the default value by hitting the enter key. It will create a reverse DNS zone by name 16.172.in-addr.arpa. (with a trailing dot)
----

##### Once you execute the above command, accept the default value by hitting the enter key. It will create a reverse DNS zone by name 16.172.in-addr.arpa. (with a trailing dot)

image:image79.png[ipa dns zone add,width=624,height=241]

[arabic, start=7]
. Disable krb5 ccache config and verify:

[source,bash]
----
##### OPEN /etc/krb5.conf on IPASERVER and comment ccache conf: (this step is not needed on any cluster node, as CDP will manage the krb5.conf in further steps config) ​​

#### After the setup is complete, we need to make a kerberos config change which gets enabled automatically post the ipa server setup.
##### Open the file /etc/krb5.conf in edit mode and comment out the line related to ccache_name as shown below.
[root@ipaserver ~]# vi /etc/krb5.conf

##### Comment the below ccache config
#default_ccache_name = KEYRING:persistent:%{uid}

##### After any changes of /etc/krb5.conf anytime, do run the below commands to restart all the IPA services.
[root@ipaserver ~]# ipactl restart
----

image:image48.png[ipa kerb keyring,width=365,height=270]

[arabic, start=8]
. Prepare the commands for adding dnsrecord and configuring reverse lookup:

[source,bash]
----
##### ADD The entry of all individual machines (separate IP separate command) to reverse DNS zone:
##### We need to create a record for each machine in the reverse DNS zone, created previously.
##### Use the below command as reference and make changes as per your configuration/machine’s private IP and Hostname.
##### Add the entry of this e.g. IPA server machine to the reverse DNS zone.

##### We need to add the IPV4 address in reverse order. The first two octets are already added in the reverse zone above. Now we need to create a record for this machine inside that zone by using the last two octets.

##### In the command below you need to add the record by providing the last two octets of your machine’s private IPV4 in reverse order. Include the trailing dot after the machine name FQDN in the above command.

##### Generate the command as shown below and run the same for all the FreeIPA agents, that includes all the nodes of Base and ECS cluster.
ipa dnsrecord-add <2nd>.<1st>.in-addr.arpa. <4th>.<3rd> --ptr-rec <server FQDN>.

##### Example:
ipa dnsrecord-add 16.172.in-addr.arpa. 226.31 --ptr-rec ipaserver.cldrsetup.local.

##### Following the same, The record for the machine should be created in the Reverse DNS zone.
----

image:image184.png[ipa dnsrecord add,width=697,height=52]

image:image116.png[ipa dns record ipa agent,width=697,height=57]

[source,bash]
----
[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 240.24  --ptr-rec ipaserver.cldrsetup.local.
  Record name: 240.24
  PTR record: ipaserver.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 139.27 --ptr-rec cldr-mngr.cldrsetup.local.
  Record name: 139.27
  PTR record: cldr-mngr.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 104.21 --ptr-rec pvcbase-master.cldrsetup.local.
  Record name: 104.21
  PTR record: pvcbase-master.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 185.16 --ptr-rec pvcbase-worker1.cldrsetup.local.
  Record name: 185.16
  PTR record: pvcbase-worker1.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 0.23 --ptr-rec pvcbase-worker2.cldrsetup.local.
  Record name: 0.23
  PTR record: pvcbase-worker2.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 240.18 --ptr-rec pvcbase-worker3.cldrsetup.local.
  Record name: 240.18
  PTR record: pvcbase-worker3.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 1.23 --ptr-rec pvcbase-worker4.cldrsetup.local.
  Record name: 1.23
  PTR record: pvcbase-worker4.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 241.18 --ptr-rec pvcbase-worker5.cldrsetup.local.
  Record name: 241.18
  PTR record: pvcbase-worker5.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 239.30 --ptr-rec pvcecs-master.cldrsetup.local.
  Record name: 239.30
  PTR record: pvcecs-master.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 43.22 --ptr-rec pvcecs-worker1.cldrsetup.local.
  Record name: 43.22
  PTR record: pvcecs-worker1.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 249.30 --ptr-rec pvcecs-worker2.cldrsetup.local.
  Record name: 249.30
  PTR record: pvcecs-worker2.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 24.26 --ptr-rec pvcecs-worker3.cldrsetup.local.
  Record name: 24.26
  PTR record: pvcecs-worker3.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 198.24 --ptr-rec pvcecs-worker4.cldrsetup.local.
  Record name: 198.24
  PTR record: pvcecs-worker4.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 53.24 --ptr-rec pvcecs-worker5.cldrsetup.local.
  Record name: 53.24
  PTR record: pvcecs-worker5.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 43.22 --ptr-rec pvcecs-worker6.cldrsetup.local.
  Record name: 43.22
  PTR record: pvcecs-worker6.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 249.30 --ptr-rec pvcecs-worker7.cldrsetup.local.
  Record name: 249.30
  PTR record: pvcecs-worker7.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 24.26 --ptr-rec pvcecs-worker8.cldrsetup.local.
  Record name: 24.26
  PTR record: pvcecs-worker8.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 198.24 --ptr-rec pvcecs-worker9.cldrsetup.local.
  Record name: 198.24
  PTR record: pvcecs-worker9.cldrsetup.local.

[root@ipaserver ~]# ipa dnsrecord-add 31.172.in-addr.arpa. 53.24 --ptr-rec pvcecs-worker10.cldrsetup.local.
  Record name: 53.24
  PTR record: pvcecs-worker10.cldrsetup.local.
----

[arabic, start=9]
. Verify the DNS records have been added successfully:

[source,bash]
----
[root@ipaserver ~]# ipa dnsrecord-find 31.172.in-addr.arpa.
  Record name: @
  NS record: ipaserver.cldrsetup.local.

  Record name: 240.24
  PTR record: ipaserver.cldrsetup.local.

  Record name: 139.27
  PTR record: cldr-mngr.cldrsetup.local.

  Record name: 104.21
  PTR record: pvcbase-master.cldrsetup.local.

  Record name: 185.16
  PTR record: pvcbase-worker1.cldrsetup.local.

  Record name: 0.23
  PTR record: pvcbase-worker2.cldrsetup.local.

  Record name: 240.18
  PTR record: pvcbase-worker3.cldrsetup.local.

  Record name: 185.16
  PTR record: pvcbase-worker4.cldrsetup.local.

  Record name: 240.18
  PTR record: pvcbase-worker5.cldrsetup.local.

  Record name: 239.30
  PTR record: pvcecs-master.cldrsetup.local.

  Record name: 43.22
  PTR record: pvcecs-worker1.cldrsetup.local.

  Record name: 249.30
  PTR record: pvcecs-worker2.cldrsetup.local.

  Record name: 198.24
  PTR record: pvcecs-worker3.cldrsetup.local.

  Record name: 53.24
  PTR record: pvcecs-worker4.cldrsetup.local.

  Record name: 24.26
  PTR record: pvcecs-worker5.cldrsetup.local.

  Record name: 43.22
  PTR record: pvcecs-worker6.cldrsetup.local.

  Record name: 0.23
  PTR record: pvcecs-worker7.cldrsetup.local.

  Record name: 198.24
  PTR record: pvcecs-worker8.cldrsetup.local.

  Record name: 53.24
  PTR record: pvcecs-worker9.cldrsetup.local.

  Record name: 24.26
  PTR record: pvcecs-worker10.cldrsetup.local.
  
-----------------------------
Number of entries returned 20
-----------------------------
[root@ipaserver ~]# 
----

[arabic, start=6]
. Configure freeipa-client on all other nodes to get them managed by ipa-server

*Step 1:* Install free-ipa client along with other packages needed on all hosts except ipaserver:

*Note:* Setup ipaserver client and krb5 libs on each node before copying resolv.conf, as installation of ipa-client will override this. *_(UDP port 123 and TCP port 389 need to be enabled for ipa services, ntp and timesync)_

*Note:* Remove chrony from all hosts using ansible as it creates issues in installing and configuring ipa services successfully.

*Note:* Please review JAVA requirement in CDP Private Cloud Base Requirements and Supported Versions sections: (We installed OpenJDK11 for this solution validation, ipa-client will also require and auto install java 11 on all hosts, if it is not present or any different version is installed e.g. java17)

https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/release-guide/topics/cdpdc-java-requirements.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/release-guide/topics/cdpdc-java-requirements.html#]

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "sudo subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms && sudo subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms && sudo dnf install -y java-17-openjdk java-17-openjdk-devel python3-pip wget telnet mlocate tar traceroute net-tools bind-utils traceroute nc && java -version && python3 -V && pip3 install --upgrade pip && pip3 -V && pip3 install psycopg2-binary && pip3 list |grep psy"

[root@ipaserver ~]# ansible all -m shell -a "sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm && sudo dnf install -y postgresql14"

[root@ipaserver ~]# ansible all -m shell -a "sudo subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms && sudo subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms && sudo dnf install -y freeipa-client openldap-clients krb5-workstation krb5-libs && chronyc tracking && chronyc sources" -l 'all:!admin'
----

*Step 2:* Install and Setup IPA services by configuring the free-ipa client on all machines (except ipaserver) and add all the machines to the DNS server, by running the command “*_ipa-client-install_*” to set up the IPA client.

Enter the values for these parameters as below. After entering these values, it should return the message as *_"The ipa-client-install command was successful"._

[width="100%",cols="66%,34%",options="header",]
|===
a|
____
*Parameter
____

a|
____
*Value
____

a|
____
Do you want to configure chrony with NTP server or pool address? [no]:
____

a|
____
yes
____

a|
____
Enter NTP source server addresses separated by comma, or press Enter to skip:
____

a|
____
<ENTER>
____

a|
____
Enter a NTP source pool address, or press Enter to skip:
____

a|
____
<ENTER>
____

a|
____
Continue to configure the system with these values? [no]:
____

a|
____
yes
____

a|
____
User authorized to enroll computers:
____

a|
____
admin
____

a|
____
Password for admin@<Your_Domain>:
____

a|
____
<Password created earlier> (*_vmware123_*)
____

|===

[source,bash]
----
[root@pvcbase-master ~]# ipa-client-install --force-join
This program will set up IPA client.
Version 4.11.0

Discovery was successful!
Do you want to configure chrony with NTP server or pool address? [no]: yes
Enter NTP source server addresses separated by comma, or press Enter to skip: <ENTER>
Enter a NTP source pool address, or press Enter to skip: <ENTER>
Client hostname: cldr-mngr.cldrsetup.local
Realm: CLDRSETUP.LOCAL
DNS Domain: cldrsetup.local
IPA Server: ipaserver.cldrsetup.local
BaseDN: dc=cldrsetup,dc=local

Continue to configure the system with these values? [no]: yes
Synchronizing time
No SRV records of NTP servers were found and no NTP server or pool address was provided.
Using default chrony configuration.
Attempting to sync time with chronyc.
Time synchronization was successful.
User authorized to enroll computers: <admin>
Password for admin@CLDRSETUP.LOCAL: <vmware123>
Successfully retrieved CA cert
    Subject:     CN=Certificate Authority,O=CLDRSETUP.LOCAL
    Issuer:      CN=Certificate Authority,O=CLDRSETUP.LOCAL
    Valid From:  2024-05-13 10:59:53+00:00
    Valid Until: 2044-05-13 10:59:53+00:00

Enrolled in IPA realm CLDRSETUP.LOCAL
Created /etc/ipa/default.conf
Configured /etc/sssd/sssd.conf
Systemwide CA database updated.
Hostname (pvcbase-master.cldrsetup.local) does not have A/AAAA record.
Adding SSH public key from /etc/ssh/ssh_host_rsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ecdsa_key.pub
Adding SSH public key from /etc/ssh/ssh_host_ed25519_key.pub
SSSD enabled
Configured /etc/openldap/ldap.conf
Configured /etc/ssh/ssh_config
Configured /etc/ssh/sshd_config.d/04-ipa.conf
Configuring cldrsetup.local as NIS domain.
Configured /etc/krb5.conf for IPA realm CLDRSETUP.LOCAL
Client configuration complete.
The ipa-client-install command was successful
[root@pvcbase-master ~]# 
----

image:image87.png[ipa client install,width=616,height=459]

*Step 3:* Verify KDC setup: kerberos ticket generation is working fine by generating a ticket for the admin user from all individual hosts.

[source,bash]
----
##### Run the kinit admin command to authenticate as admin and enter the directory password provided during ipa server installation. The command should generate the ticket and should be listed by executing klist -e. 

[root@ipaserver ~]# kinit admin 
Password for admin@CLDRSETUP.LOCAL: <vmware123>

[root@ipaserver ~]# klist -e
Ticket cache: KCM:0
Default principal: admin@CLDRSETUP.LOCAL

Valid starting       Expires              Service principal
05/13/2024 04:07:15  05/14/2024 03:30:48  krbtgt/CLDRSETUP.LOCAL@CLDRSETUP.LOCAL
        Etype (skey, tkt): aes256-cts-hmac-sha384-192, aes256-cts-hmac-sha384-192 

##### try kinit admin@CLDRSETUP.LOCAL
##### (if fails anytime, run below commands
[root@ipaserver ~]# ipactl stop && ipactl start && ipactl status

##### Verify the status of ipa services installed
[root@ipaserver ~]# ipactl status
Directory Service: RUNNING
krb5kdc Service: RUNNING
kadmin Service: RUNNING
named Service: RUNNING
httpd Service: RUNNING
ipa-custodia Service: RUNNING
ntpd Service: RUNNING
pki-tomcatd Service: RUNNING
ipa-otpd Service: RUNNING
ipa-dnskeysyncd Service: RUNNING
ipa: INFO: The ipactl command was successful
----

This command should return the below output:

image:image23.png[klist verify,width=582,height=171]

*Step 4:* Verify the network configuration file *_/etc/resolv.conf_* on the IPA server to use the Name Server created in previous steps (after installing freeipa-client, as it overrides resolv.conf and may lead to rework) to make them able to resolve FQDNs across the cluster:

(Open the file /etc/resolv.conf in edit mode and verify the following. Make sure the new entry is added above any other nameserver entry. The contents of the file must look similar to the below.)

*Note*: ​​Make sure that the *_/etc/resolv.conf_* file on the ECS hosts *_contains a maximum of 2 active search domains_*.

https://docs.cloudera.com/data-warehouse/1.5.4/release-notes/topics/dw-private-cloud-known-issues-ecs-environments.html[[.underline]#https://docs.cloudera.com/data-warehouse/1.5.4/release-notes/topics/dw-private-cloud-known-issues-ecs-environments.html#]

[source,bash]
----
[root@ipaserver ~]# cat /etc/resolv.conf
search ap-southeast-1.compute.internal cldrsetup.local
nameserver 172.31.24.240  # PrivateIP of FreeIPA Server must be first nameserver entry after search
nameserver 172.31.0.2     # DNS of AWS i.e. in case of PvC Configured on EC2
nameserver 127.0.0.1
[root@ipaserver ~]#
----

image:image32.png[resolv conf,width=428,height=59]

*Step 5:* Verify the network configuration file *_/etc/sysconfig/network_* on the IPA server to use the Name Server created in previous steps:

(The changes in *_/etc/resolv.conf_* above are temporary and would get overwritten if the machine is rebooted. In order to keep the nameserver entry persistent, open the file *_/etc/sysconfig/network_* in edit mode and verify the entries below.)

[source,bash]
----
[root@ipaserver ~]# cat /etc/sysconfig/network
NETWORKING=yes
NISDOMAIN=cldrsetup.local  # our DNS DOMAIN
DNS1=172.31.24.240         # PRIVATE_IP_OF_IPASERVER
NOZEROCONF=yes
[root@ipaserver ~]#
----

image:image55.png[sysconfig network,width=213,height=79]

*Step 6:* Copy */etc/resolv.conf* file to each node again, to make them able to resolve FQDNs across the cluster:

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/resolv.conf dest=/etc/resolv.conf"
----

*Step 7:* Copy */etc/sysconfig/network* file again, to each node to make them able to resolve FQDNs across the cluster: (/etc/resolv.conf changes may vanished after the reboot, so to persist those changes, we need the below configuration)

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/sysconfig/network dest=/etc/sysconfig/network"
----

*Step 8:* Enable permissions for HDFS and for PAM Authentication:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "chmod 1777 /tmp && chmod 444 /etc/shadow"
----

*Step 9:* Login to IPAServer node and verify forward and reverse DNS lookup is working fine from each machine:

[source,bash]
----
[root@ipaserver ~]# nslookup cldr-mngr.cldrsetup.local
Server:         172.31.24.240
Address:        172.31.24.240#53

Name:   cldr-mngr.cldrsetup.local
Address: 172.31.27.139

#(forward lookup)Running the below command should return the IPV4 of the machine in the Answer Section.

# dig <FQDN of the SERVER> A
# dig $(hostname) A | grep -A2 ANSWER
# Ex:- dig ipaserver.cldrsetup.local A

[root@ipaserver ~]# dig $(hostname -f) A | grep -A2 ANSWER
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
--
;; ANSWER SECTION:
ipaserver.cldrsetup.local. 1200    IN      A       172.31.24.240

#(reverse lookup) Running the below command should return the hostname of the machine in the Answer Section.

# dig -x <Private_IP_of_SERVER>
# dig -x $(hostname -i)|grep -A2 ANSWER
# Ex:- dig -x 172.31.40.119

[root@ipaserver ~]# dig -x $(hostname -i) | grep -A2 ANSWER
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
--
;; ANSWER SECTION:
240.24.31.172.in-addr.arpa. 86400 IN    PTR     ipaserver.cldrsetup.local.

[root@ipaserver ~]$ 
----

*Step 10:* Login on ipaserver, configure and validate wildcard DNS record is working fine and resolvable, which is required later for the ECS data service cluster (if not set properly, chances of ECS installation getting corrupt):

[source,bash]
----
[root@ipaserver ~]#  ipa dnsrecord-add cldrsetup.local *.apps
Please choose a type of DNS resource record to be added
The most common types for this type of zone are: A, AAAA

DNS resource record type: A
A IP Address: 172.31.30.239       #Provide the IP address of ecs-master node
  Record name: *.apps
  A record: 172.31.30.239

[root@ipaserver ~]# nslookup console-cdp.apps.cldrsetup.local
Server:         172.31.24.240
Address:        172.31.24.240#53

Name:   console-cdp.apps.cldrsetup.local
Address: 172.31.30.239

[root@ipaserver ~]# dig console-cdp.apps.cldrsetup.local A | grep -A2 ANSWER
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
--
;; ANSWER SECTION:
console-cdp.apps.cldrsetup.local. 86400 IN A       172.31.30.239

[root@ipaserver ~]# dig -x 172.31.30.239 | grep -A2 ANSWER
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
--
;; ANSWER SECTION:
239.30.31.172.in-addr.arpa. 86400 IN    PTR     pvcecs-master.cldrsetup.local.

[root@ipaserver ~]# 
----

*Step 11:* Download and copy postgresql-jdbc driver to all hosts:

[source,bash]
----
[root@ipaserver ~]# wget https://jdbc.postgresql.org/download/postgresql-42.7.3.jar
[root@ipaserver ~]# chmod 644 postgresql-42.7.3.jar
[root@ipaserver ~]# ansible all -m copy -a "src=postgresql-42.7.3.jar dest=/usr/share/java/postgresql-connector-java.jar"
[root@ipaserver ~]# ansible all -m shell -a "sudo ls -l /usr/share/java/postgresql-connector-java.jar"
[root@ipaserver ~]#
----

[arabic, start=7]
. Disable the Linux Firewall

[arabic, start=5]
. The default Linux firewall settings are too restrictive for any Hadoop deployment. Since the CDP PvC deployment will be in its own isolated network in the VMware environment, there is no need for that additional firewall. *(NA in AWS EC2)

[source,bash]
----
##### Either disable the firewall or update the rules: (ON ALL HOSTS) 
[root@ipaserver ~]# ansible all -m command -a "firewall-cmd --zone=public --add-port=80/tcp --permanent"
[root@ipaserver ~]# ansible all -m command -a "firewall-cmd --zone=public --add-port=443/tcp --permanent"
[root@ipaserver ~]# ansible all -m command -a "firewall-cmd --reload"
[root@ipaserver ~]# ansible all -m command -a "systemctl disable firewalld && systemctl stop firewalld && systemctl status firewalld | grep -e disabled -e inactive"
[root@ipaserver ~]
----

[arabic, start=8]
. Disable SELinux

[arabic, start=6]
. SELinux must be disabled during the install procedure and cluster setup. SELinux can be enabled after installation and while the cluster is running.

*Step 1:* SELinux can be disabled by editing *_/etc/selinux/config_* (in some systems it would be *_/etc/sysconfig/selinux_*) To disable SELinux, change SELINUX=enforcing to SELINUX=disabled or SELINUX=permissive. follow these steps:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config"
[root@ipaserver ~]# ansible all -m shell -a "setenforce 0"
[root@ipaserver ~]# ansible all -m shell -a "getenforce"
----

[arabic, start=7]
. This command may fail if SELinux is already disabled. This requires reboot to take effect.
. While the suggested configuration is to disable SELinux as shown below, if for any reason SELinux needs to be enabled on the cluster, run the following command to make sure that the httpd can read the *_Yum_* profiles.

[source,bash]
----
[root@ipaserver ~]# chcon -R -t httpd_sys_content_t /var/www/html/
----

image:image155.png[selinux,width=577,height=172]

[arabic, start=9]
. Enable Syslog

Syslog must be enabled on each node to preserve logs regarding killed processes or failed jobs. Modern versions such as syslog-ng and rsyslog are possible, making it more difficult to be sure that a syslog daemon is present.

[arabic]
. Use one of the following commands to confirm that the service is properly configured:

[source,bash]
----
[root@ipaserver ~]# ansible all -m command -a "rsyslogd -v"
[root@ipaserver ~]# ansible all -m command -a "service rsyslog status"
----

[arabic, start=10]
. Set ulimit

On each node, ulimit -n specifies the number of inodes that can be opened simultaneously. With the default value of 1024, the system appears to be out of disk space and shows no inodes available. This value should be set to 64000 on every node. Higher values are unlikely to result in an appreciable performance gain.

[arabic]
. For setting the ulimit on RedHat, edit */etc/security/limits.conf* on admin node and add the following lines:

[source,bash]
----
[root@ipaserver ~]# vi /etc/security/limits.conf
* soft nofile 1048576
* hard nofile 1048576
----

[arabic, start=2]
. Copy the /etc/security/limits.conf file from admin node (ipaserver) to all the nodes using the following command:

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/security/limits.conf dest=/etc/security/limits.conf"
----

[arabic, start=3]
. Make sure that the /etc/pam.d/su file contains the following settings:

[source,bash]
----
[root@ipaserver ~]# vi /etc/pam.d/su
#%PAM-1.0
auth            required        pam_env.so
auth            sufficient      pam_rootok.so
# Uncomment the following line to implicitly trust users in the "wheel" group.
#auth           sufficient      pam_wheel.so trust use_uid
# Uncomment the following line to require a user to be in the "wheel" group.
#auth           required        pam_wheel.so use_uid
auth            include         system-auth
auth            include         postlogin
account         sufficient      pam_succeed_if.so uid = 0 use_uid quiet
account         include         system-auth
password        include         system-auth
session         include         system-auth
session         include         postlogin
session         optional        pam_xauth.so
----

[arabic, start=4]
. Copy the /etc/pam.d/su file from admin node (ipaserver) to all the nodes using the following command:

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/pam.d/su dest=/etc/pam.d/su"
----

[arabic, start=9]
. The ulimit values are applied on a new shell, running the command on a node on an earlier instance of a shell will show old values.

[arabic, start=11]
. Set TCP Retries

Adjusting the tcp_retries parameter for the system network enables faster detection of failed nodes. Given the advanced network-ing features of UCS, this is a safe and recommended change (failures observed at the operating system layer are most likely serious rather than transitory).

*Note*: On each node, setting the number of TCP retries to 5 can help detect unreachable nodes with less latency.

[arabic]
. Edit the file /etc/sysctl.conf on ipaserver node and add the following lines:

[source,bash]
----
[root@ipaserver ~]# vi /etc/sysctl.conf
net.ipv4.tcp_retries2=5
----

[arabic, start=2]
. Copy the /etc/sysctl.conf file from admin node to all the nodes using the following command:

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/sysctl.conf dest=/etc/sysctl.conf"
----

[arabic, start=12]
. Disable IPv6 Defaults

[arabic]
. Run the following command:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "echo 'net.ipv6.conf.all.disable_ipv6 = 1' >> /etc/sysctl.conf" -l 'all:!ecsmasternodes:!ecsnodes'
[root@ipaserver ~]# ansible all -m shell -a "echo 'net.ipv6.conf.default.disable_ipv6 = 1' >> /etc/sysctl.conf" -l 'all:!ecsmasternodes:!ecsnodes'
[root@ipaserver ~]# ansible all -m shell -a "echo 'net.ipv6.conf.lo.disable_ipv6 = 0' >> /etc/sysctl.conf" -l 'all:!ecsmasternodes:!ecsnodes'
----

[arabic, start=13]
. Disable Swapping

[arabic]
. Run the following to set VM swappiness to 1, by updating /etc/sysctl.conf file on all nodes:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "echo 'vm.swappiness=1' >> /etc/sysctl.conf"
----

[arabic, start=14]
. Disable Memory Overcommit

[arabic]
. Run the following on all nodes. Variable vm.overcommit_memory=0

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "echo 'vm.overcommit_memory=0' >> /etc/sysctl.conf"
----

[arabic, start=2]
. Load the settings from default sysctl file /etc/sysctl.conf and verify the content of sysctl.conf:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "sysctl -p" ## Reload sysctl.conf
[root@ipaserver ~]# ansible all -m shell -a "cat /etc/sysctl.conf"
net.ipv4.tcp_retries2=5
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 0
vm.swappiness=1
vm.overcommit_memory=0
[root@ipaserver ~]#
----

[arabic, start=15]
. Disable Transparent Huge Pages

Disabling Transparent Huge Pages (THP) reduces elevated CPU usage caused by THP.

[arabic]
. You must run the following commands for every reboot:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "echo never > /sys/kernel/mm/transparent_hugepage/enabled"
[root@ipaserver ~]# ansible all -m shell -a "echo never > /sys/kernel/mm/transparent_hugepage/defrag"
----

[arabic, start=2]
. On the Ansible-controller/ ipaserver node, run the following commands:

[source,bash]
----
[root@ipaserver ~]# rm –f /root/thp_disable
[root@ipaserver ~]# echo "echo never > /sys/kernel/mm/transparent_hugepage/enabled" >> /root/thp_disable
[root@ipaserver ~]# echo "echo never > /sys/kernel/mm/transparent_hugepage/defrag" >> /root/thp_disable

##### Disable IPV6
[root@ipaserver ~]# echo "sysctl -w net.ipv6.conf.all.disable_ipv6=1" >> /root/thp_disable
[root@ipaserver ~]# echo "sysctl -w net.ipv6.conf.default.disable_ipv6=1" >> /root/thp_disable
[root@ipaserver ~]# echo "sysctl -w net.ipv6.conf.lo.disable_ipv6=0" >> /root/thp_disable
----

[arabic, start=3]
. Copy file to each node to copy the command to *_/etc/rc.d/rc.local_* so they are executed automatically for every reboot:

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/root/thp_disable dest=/root/thp_disable"

##### Append the content of file thp_disable to /etc/rc.d/rc.local:
[root@ipaserver ~]# ansible all -m shell -a "cat /root/thp_disable >> /etc/rc.d/rc.local"
[root@ipaserver ~]# ansible all -m shell -a "chmod +x /etc/rc.d/rc.local"
[root@ipaserver ~]# ansible all -m shell -a "cat /etc/rc.d/rc.local"
#!/bin/bash
# Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure
# that this script will be executed during boot.
touch /var/lock/subsys/local
# Disable Transparent Huge Pages
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
# Disable IPV6
sysctl -w net.ipv6.conf.all.disable_ipv6=1
sysctl -w net.ipv6.conf.default.disable_ipv6=1
sysctl -w net.ipv6.conf.lo.disable_ipv6=0
[root@ipaserver ~]#
----

image:image65.png[huge page ipv6,width=453,height=249]

[arabic, start=16]
. Disable tuned service

For Cloudera cluster with hosts are running RHEL/CentOS 7.x or 8.x or 9.x, disable the "tuned" service by running the following commands:

[arabic]
. Ensure that the tuned service is started.

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "systemctl start tuned"
----

[arabic, start=2]
. Turn the tuned service off.

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "tuned-adm off"
----

[arabic, start=3]
. Ensure that there are no active profiles.

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "tuned-adm list"
# The output should contain the following line:
# pvcecs-worker4.cldrsetup.local | CHANGED | rc=0 >>
Available profiles:
- accelerator-performance     - Throughput performance based tuning with disabled higher latency STOP states
- aws                         - Optimize for aws ec2 instances
- balanced                    - General non-specialized tuned profile
- desktop                     - Optimize for the desktop use-case
- hpc-compute                 - Optimize for HPC compute workloads
- intel-sst                   - Configure for Intel Speed Select Base Frequency
- latency-performance         - Optimize for deterministic performance at the cost of increased power consumption
- network-latency             - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance
- network-throughput          - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks
- optimize-serial-console     - Optimize for serial console use.
- powersave                   - Optimize for low power consumption
- throughput-performance      - Broadly applicable tuning that provides excellent performance across a variety of common server workloads
- virtual-guest               - Optimize for running inside a virtual guest
- virtual-host                - Optimize for running KVM guests
No current active profile.
----

[arabic, start=4]
. Shutdown and disable the tuned service.

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "systemctl stop tuned"
[root@ipaserver ~]# ansible all -m shell -a "systemctl disable tuned"
----

[arabic, start=17]
. Turning off TCP checksum offload on the all ecs nodes’ (both master and worker) network interface (For VMWare Machines)

The default CNI (Canal) that comes with ECS (RKE2 from SUSE) does not support VM. Customers have reported escalations with the default CNI when they use VMWARE (WF for example). Customers may experience network connectivity issues or degraded performance when deploying ECS clusters on VMWARE environments with the default CNI. So, they use a workaround by turning off TCP checksum offload on the interface.

*Note:* Disabling TCP checksum offload may have implications on network performance or security. Evaluate the impact of this workaround in the specific environment and consider reverting the changes once a permanent solution or alternative workaround is available. Additionally, consult with the VMWARE documentation or support resources for guidance on network configuration and optimization in VMWARE environments.

Here's how you can perform the workaround for customers experiencing issues with the default CNI in ECS (RKE2 from SUSE) when using VMWARE infrastructure, by disabling TCP checksum offload on the network interface used by the affected ECS nodes:

*Step 1:* Identify the network interface:

[source,bash]
----
##### Determine the network interface used by the affected ECS nodes. This can typically be found using the `ifconfig` or `ip addr` command:
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "ifconfig | grep flags | grep -v lo && ip addr | grep mtu | grep -v lo"

[root@ipaserver ~]$ ifconfig | grep flags | grep -v lo
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 9001
[root@ipaserver ~]$ ip addr | grep mtu | grep -v lo
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000
[root@ipaserver ~]$ 
----

*Step 2:* Disable TCP Checksum Offload: Use the appropriate commands or configuration settings to disable TCP checksum offload on the identified network interface. For example, using ethtool:

[source,bash]
----
##### Replace `<interface_name>` with the name of the network interface identified in step 1.
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo ethtool -K <interface_name> tx off"
##### After disabling TCP checksum offload, verify that the changes have been applied correctly and that the affected ECS nodes no longer experience the reported issues.
----

[arabic, start=18]
. Create partitions on ECS nodes (master and worker) manually, if not present: (Do df -h) *(Skip this)

*Step 1:* Create partitions on disk attached. We require three partitions i.e. /docker, /cdwdata and /lhdata:

[source,bash]
----
[root@pvcecs-master ~]# lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
xvda    202:0    0   600G  0 disk 
├─xvda1 202:1    0     1M  0 part 
├─xvda2 202:2    0   200M  0 part /boot/efi
├─xvda3 202:3    0   600M  0 part /boot
└─xvda4 202:4    0 599.2G  0 part /
xvdb    202:16   0   600G  0 disk 
xvdc    202:32   0   600G  0 disk 
xvdd    202:48   0   600G  0 disk 
xvde    202:48   0   600G  0 disk 

[root@pvcecs-master ~]# fdisk -l
Disk /dev/sda: 268.4 GB, 268435456000 bytes, 524288000 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000a04a5

Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048     2099199     1048576   83  Linux
/dev/sda2         2099200   524287999   261094400   8e  Linux LVM
/dev/xvdb         2099200   524287999   261094400   8e  Linux LVM
/dev/xvdc         2099200   524287999   261094400   8e  Linux LVM
/dev/xvdd         2099200   524287999   261094400   8e  Linux LVM
/dev/xvde         2099200   524287999   261094400   8e  Linux LVM

Disk /dev/sdb: 644.2 GB, 644245094400 bytes, 1258291200 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
[root@pvcecs-master ~]# 

[root@pvcecs-master ~]# sudo parted /dev/xvdb mklabel gpt
Information: You may need to update /etc/fstab.

[root@pvcecs-master ~]# sudo parted /dev/xvdc mklabel gpt
[root@pvcecs-master ~]# sudo parted /dev/xvdd mklabel gpt

[root@pvcecs-master ~]# sudo parted -a opt /dev/xvdb mkpart primary ext4 0% 100%
Information: You may need to update /etc/fstab.

[root@pvcecs-master ~]# sudo parted -a opt /dev/xvdc mkpart primary ext4 0% 100%
[root@pvcecs-master ~]# sudo parted -a opt /dev/xvdd mkpart primary ext4 0% 100%

[root@pvcecs-master ~]# sudo mkfs.ext4 /dev/xvdb
mke2fs 1.46.5 (30-Dec-2021)
Found a gpt partition table in /dev/xvdb
Proceed anyway? (y,N) y    
Creating filesystem with 157286400 4k blocks and 39321600 inodes
Filesystem UUID: 934fece5-074e-4102-8481-6ed3a3b99931
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 
        102400000

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (262144 blocks): 
done
Writing superblocks and filesystem accounting information: done     

[root@pvcecs-master ~]# sudo mkfs.ext4 /dev/xvdc
[root@pvcecs-master ~]# sudo mkfs.ext4 /dev/xvdd

[root@pvcecs-master ~]# sudo mkdir -p /docker /cdwdata /lhdata

[root@pvcecs-master ~]# sudo mount /dev/xvdb /docker
[root@pvcecs-master ~]# sudo mount /dev/xvdc /cdwdata
[root@pvcecs-master ~]# sudo mount /dev/xvdd /lhdata

[root@pvcecs-master ~]# sudo blkid /dev/xvdb
[root@pvcecs-master ~]# sudo blkid /dev/xvdc
[root@pvcecs-master ~]# sudo blkid /dev/xvdd

[root@pvcecs-master ~]# cat /etc/fstab
UUID=497ad222-04fa-453f-b110-ba8001d38788       /       xfs     defaults        0       0
UUID=2e0d9ec9-a82a-43cc-a8e2-c6db30e7f6a4       /boot   xfs     defaults        0       0
UUID=7B77-95E7  /boot/efi       vfat    defaults,uid=0,gid=0,umask=077,shortname=winnt  0       2
[root@pvcecs-master ~]# 

[root@pvcecs-master ~]# cat>> /etc/fstab
UUID=497ad222-04fa-453f-b110-ba8001d38788       /       xfs     defaults        0       0
UUID=2e0d9ec9-a82a-43cc-a8e2-c6db30e7f6a4       /boot   xfs     defaults        0       0
UUID=7B77-95E7  /boot/efi       vfat    defaults,uid=0,gid=0,umask=077,shortname=winnt  0       2
UUID="934fece5-074e-4102-8481-6ed3a3b99931"   /docker  xfs  defaults  0  0
UUID="52f4940d-3dca-4b90-a223-3bef6eee2b74"  /cdwdata   xfs     defaults        0       0
UUID="d7e9640b-cc12-49ac-b6c2-6eb3136679a1"  /lhdata   xfs     defaults        0       0

[root@pvcecs-master ~]# mount -av
[root@pvcecsmaster overlay2]# lvs
  LV      VG     Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  home    centos -wi-ao----  183.24g                                                    
  root    centos -wi-ao----   50.00g                                                    
  swap    centos -wi-ao----   15.75g                                                    
  cdwdata vgcdp  -wi-ao----  600.00g                                                    
  docker  vgcdp  -wi-ao---- <600.00g                                                    
  var     vgcdp  -wi-ao---- <600.00g                                                    
  lhdata  vgcdp  -wi-ao----  1200.00g                                                    
[root@pvcecsmaster overlay2]# 
                                                
[root@pvcecsmaster overlay2]# pvs
  PV         VG     Fmt  Attr PSize    PFree   
  /dev/sda2  centos lvm2 a--  <249.00g    4.00m
  /dev/sdb   vgcdp  lvm2 a--  <600.00g <224.96g
  /dev/sdc   vgcdp  lvm2 a--  <600.00g <224.96g
  /dev/sdd   vgcdp  lvm2 a--  <1200.00g<224.96g
  /dev/sde   vgcdp  lvm2 a--  <600.00g <224.96g
[root@pvcecs-master ~]# 
----

[arabic, start=19]
. Install httpd on Cloudera-Manager node i.e. cldr-mngr to host a local Parcel repository

Setting up the RHEL repository on the cloudera-manager node requires httpd.

[arabic]
. Install httpd on the cloudera-manager i.e. *cldr-mngr* node to host repositories:

[arabic, start=10]
. The Red Hat repository is hosted using HTTP on the admin node; this machine is accessible by all the hosts in the cluster.

[source,bash]
----
[root@cldr-mngr ~]# dnf install -y httpd mod_ssl createrepo_c
----

[arabic, start=2]
. Generate CA certificate.

[source,bash]
----
[root@cldr-mngr ~]# openssl req -newkey rsa:2048 -nodes -keyout /etc/pki/tls/private/httpd.key -x509 -days 3650 -out /etc/pki/tls/certs/httpd.crt \
-subj "/C=US/ST=North Carolina/L=Raleigh/O=Red Hat Inc/OU=CLDR/CN=cldr-mngr.cldrsetup.local"
...+.+..+...+.+......+...+............+..+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.......+.....+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.+.....+....+.........+.....+......+...+.......+.....+....+..+....+......+..+......+....+...+..+.......+..............+.+...............+...+..+.........+....+..+...+.+...........+...+.+.....+.......+..+.......+........+.......+.........+..+....+...........+...+.+......+........+....+......+......+...+............+......+.....+...+......+.+...+..............+.+............+........+..........+..+.......+..+.............+..+...+.+..............+...+...+.+.....+.........+.........+............+..........+......+.................+.+..+.......+......+............+...+...............+......+.....+.+..+.+......+........+......+.......+.....+...+...................+...........+..........+..+...+....+...+.................+......+.........+.......+...+..+.+...+...........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
...+.........+...........+.+..............+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*....+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.+.......+...+..+............+....+.....+......+...+.+..+............+.+......+...+...+...+......+.....+...................+......+...+..+.+.....+....+..................+..+.+.....+....+...............+...+.....+...+..................+.+...........+...+.+.....+...................+.........+............+........+......+....+...........+.+........+.............+..+...+.......+...+..............+.............+......+.........+...........+.........+.............+..+.+......+.........+...........+.+.....+......................+...............+..+....+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----

[root@cldr-mngr ~]# ls -l /etc/pki/tls/private/ /etc/pki/tls/certs/
/etc/pki/tls/certs/:
total 8
lrwxrwxrwx. 1 root root   49 Jul 28  2022 ca-bundle.crt -> /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
lrwxrwxrwx. 1 root root   55 Jul 28  2022 ca-bundle.trust.crt -> /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt
-rw-r--r--. 1 root root 1432 Mar  4 13:34 httpd.crt
-rw-r--r--. 1 root root 2260 Mar  1 16:36 postfix.pem

/etc/pki/tls/private/:
total 8
-rw-------. 1 root root 1700 Mar  4 13:33 httpd.key
-rw-------. 1 root root 3268 Mar  1 16:36 postfix.key
[root@cldr-mngr ~]# 
----

[arabic, start=3]
. Create certificate directory to server content from.

[source,bash]
----
[root@cldr-mngr ~]# mkdir -p /var/www/https/
[root@cldr-mngr ~]# echo secure content > /var/www/https/index.html
[root@cldr-mngr ~]# cat /var/www/https/index.html
secure content
----

[arabic, start=4]
. Edit httpd.conf file; add ServerName and make the necessary changes to the server configuration file:

[source,bash]
----
[root@cldr-mngr ~]# vi /etc/httpd/conf/httpd.conf
ServerName cldr-mngr.cldrsetup.local:80
----

[arabic, start=5]
. Start httpd service.

[source,bash]
----
[root@cldr-mngr ~]# systemctl start httpd
[root@cldr-mngr ~]# systemctl enable httpd
[root@cldr-mngr ~]# systemctl is-enabled httpd
----

++**************************************************************************************************************++


=== Install Cloudera Data Platform Private Cloud (CDP PvC)

This chapter contains the following:

* Cloudera Runtime
* Install CDP Private Cloud Base *7.1.9 SP1 CHF4
* Install CDP Data Services *1.5.4 CHF3

*Cloudera Runtime

Cloudera Runtime is the core open-source software distribution within CDP Private Cloud Base. Cloudera Runtime includes approximately 50 open-source projects that comprise the core distribution of data management tools within CDP.

For more information review Cloudera Runtime Release notes:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/runtime-release-notes/topics/rt-pvc-whats-new.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/runtime-release-notes/topics/rt-Private Cloud-whats-new.html#]

Please review runtime cluster hosts and role assignments:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-runtime-cluster-hosts-role-assignments.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-runtime-cluster-hosts-role-assignments.html#]

*Cloudera Data Platform Private Cloud Installation Requirements (Pre-requisites):

*NTP/Chrony

Both CDP Private Cloud Base and CDP Private Cloud DS cluster should have their time synched with the NTP Clock time from the same NTP source. Also make sure, Active Directory server where Kerberos is setup for data lake and for other services must also be synced with the same NTP source.

*JDK 11

The cluster must be configured with JDK 11, JDK8 is not supported. You can use Oracle, OpenJDK 11.04, or higher. JAVA 11 is a JKS requirement and must be met. In this setup we used OpenJDK 17.0.13.

*Kerberos

Kerberos must be configured using an Active Directory (AD), RedHat FreeIPA or MIT KDC. Kerberos will be enabled for all services in the cluster.

*Database Requirements

Cloudera Manager and Runtime come packaged with an embedded PostgreSQL database for use in non-production environments. The embedded PostgreSQL database is not supported in production environments. For production environments, you must configure your cluster to use dedicated external databases.

For detailed information about supported database go to: https://supportmatrix.cloudera.com/[[.underline]#https://supportmatrix.cloudera.com/#]

*Configure Cloudera Manager with TLS/SSL

TLS/SSL provides privacy and data integrity between applications communicating over a network by encrypting the packets transmitted between endpoints (ports on a host, for example). Configuring TLS/SSL for any system typically involves creating a private key and public key for use by server and client processes to negotiate an encrypted connection at runtime. In addition, TLS/SSL can use certificates to verify the trustworthiness of keys presented during the negotiation to prevent spoofing and mitigate other potential security issues.

For detailed information on encrypting data in transit, go to:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.7/security-encrypting-data-in-transit/topics/cm-security-guide-ssl-certs.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-encrypting-data-in-transit/topics/cm-security-guide-ssl-certs.html#]

The Auto-TLS feature automates all the steps required to enable TLS encryption at a cluster level. Using Auto-TLS, you can let Cloudera manage the Certificate Authority (CA) for all the certificates in the cluster or use the company’s existing CA. In most cases, all the necessary steps can be enabled easily via the Cloudera Manager UI. This feature automates the following processes when Cloudera Manager is used as a Certificate Authority:

* Creates the root Certificate Authority or a Certificate Signing Request (CSR) for creating an intermediate Certificate Authority to be signed by company’s existing Certificate Authority (CA)
* Generates the CSRs for hosts and signs them

Configuring TLS Encryption for Cloudera Manager Using Auto-TLS for detailed information:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.7/security-encrypting-data-in-transit/topics/cm-security-how-to-configure-cm-tls.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-encrypting-data-in-transit/topics/cm-security-how-to-configure-cm-tls.html#]

Manually Configuring TLS Encryption for Cloudera Manager for detailed information:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.7/security-encrypting-data-in-transit/topics/cm-security-how-to-configure-cm-tls.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-encrypting-data-in-transit/topics/cm-security-how-to-configure-cm-tls.html#]

*TLS uses JKS-format (Java KeyStore)

Cloudera Manager Server, Cloudera Management Service, and many other CDP services use JKS formatted key-stores and certificates. Java 11 is required for JKS.

*Licensing Requirements

The cluster must be setup with a license with entitlements for installing Cloudera Private Cloud. 60 days evaluation license for Cloudera Data Platform Private Cloud Base does not allow you to set up CDP Private Cloud Data Services.

Refer to the https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/release-guide/topics/cdpdc-requirements-supported-versions.html[[.underline]#CDP Private Cloud Base Requirements and Supported Versions#] for information about hardware, operating system, and database requirements, as well as product compatibility matrices.

Refer Cloudera Manager release note for new feature and support:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/manager-release-notes/topics/cm-whats-new-7113.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/manager-release-notes/topics/cm-whats-new-7113.html#]

Please review before install steps:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-before-you-install.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-before-you-install.html#]

Please review CDP Private Cloud Base requirements and supported versions for information about hardware, operating system, and database requirements, as well as product compatibility matrices:

https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade/topics/cdpdc-requirements-supported-versions.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade/topics/cdpdc-requirements-supported-versions.html#]

++**************************************************************************************************************++

=== CDP PvC Cloudera Manager Server Setup

This section outlines the steps needed to set up a 6 node Private Cloud Base cluster. Below are the prerequisites which base cluster should have before installing/configuring Data Services.

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-installation.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-installation.html#]

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-prod-installation.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-prod-installation.html#]

*Ensure to verify compatibility matrix of Cloudera-Manager, Cloudera RunTime, DataServices/ECS, JDK, Python, PostgreSQL etc. all together:

https://supportmatrix.cloudera.com/[[.underline]#https://supportmatrix.cloudera.com/#]

[arabic]
. Setup Cloudera Manager Repository

[arabic, start=11]
. These steps require a Cloudera username and password to access: https://archive.cloudera.com/p/cm7/[[.underline]#https://archive.cloudera.com/p/cm7/#]

*Step 1:* From a host connected to the Internet, download the Cloudera’s repositories as shown below and transfer it to the cldr-mngr node. We will directly login to *_cldr-mngr_* and perform below steps::

[source,bash]
----
[root@ipaserver ~]# ssh root@cldr-mngr
[root@cldr-mngr ~]# mkdir -p /var/www/html/cloudera-repos/cloudera-manager/
----

*Step 2:* Download Cloudera Manager Repository:

[source,bash]
----
[root@cldr-mngr ~]# cd /var/www/html/cloudera-repos/cloudera-manager/
[root@cldr-mngr cloudera-manager]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/cloudera-manager.repo
[root@cldr-mngr cloudera-manager]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/cloudera-manager-trial.repo
[root@cldr-mngr cloudera-manager]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/RPM-GPG-KEY-cloudera
[root@cldr-mngr cloudera-manager]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/allkeys.asc
[root@cldr-mngr cloudera-manager]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/allkeyssha256.asc
[root@cldr-mngr cloudera-manager]# 
----

*Step 3:* Edit cloudera-manager.repo file baseurl and GPG key with username and password provided by Cloudera and edit URL to match repository location *(OR)* Verify, if username and password are already present, so no action needed.

##### Verify if username and password are already present, so no action needed.

[source,bash]
----
##### Verify if username and password are already present, so no action needed.
[root@cldr-mngr cloudera-manager]# vi cloudera-manager.repo
[cloudera-manager]
name=Cloudera Manager 7.11.3.28
baseurl=https://archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/
gpgkey=https://archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/RPM-GPG-KEY-cloudera
username=<username>
password=<password>
gpgcheck=1
enabled=1
autorefresh=0
type=rpm-md

[postgresql10]
name=Postgresql 10
baseurl=https://archive.cloudera.com/postgresql10/redhat9/
gpgkey=https://archive.cloudera.com/postgresql10/redhat9/RPM-GPG-KEY-PGDG-10
enabled=1
gpgcheck=1
module_hotfixes=true

##### If not, update the cloudera-manager.repo to look like below.
[root@cldr-mngr cloudera-manager]# vi cloudera-manager.repo
[cloudera-manager]
name=Cloudera Manager 7.11.3.28
baseurl=https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/
gpgkey=https://<username>:<password>@@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/RPM-GPG-KEY-cloudera
gpgcheck=1
enabled=1
autorefresh=0
type=rpm-md
[root@cldr-mngr cloudera-manager]# cd
----

*Step 4:* Create directory to download cloudera manager agent, daemon, and server files

[source,bash]
----
[root@cldr-mngr ~]# mkdir -p /var/www/html/cloudera-repos/cloudera-manager/cm7.11.3/redhat9/yum/RPMS/x86_64/
[root@cldr-mngr ~]# cd /var/www/html/cloudera-repos/cloudera-manager/cm7.11.3/redhat9/yum/RPMS/x86_64/
[root@cldr-mngr x86_64]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/RPMS/x86_64/cloudera-manager-agent-7.11.3.28-60766845.el9.x86_64.rpm
[root@cldr-mngr x86_64]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/RPMS/x86_64/cloudera-manager-daemons-7.11.3.28-60766845.el9.x86_64.rpm
[root@cldr-mngr x86_64]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/RPMS/x86_64/cloudera-manager-server-7.11.3.28-60766845.el9.x86_64.rpm
[root@cldr-mngr x86_64]# wget https://<username>:<password>@archive.cloudera.com/p/cm7/7.11.3.28/redhat9/yum/RPMS/x86_64/cloudera-manager-server-db-2-7.11.3.28-60766845.el9.x86_64.rpm

[root@cldr-mngr x86_64]# ls -lt /var/www/html/cloudera-repos/cloudera-manager/cm7.11.3/redhat9/yum/RPMS/x86_64 && cd
total 1147504
-rw-r--r-- 1 root root   80901377 Aug 16 12:45 cloudera-manager-agent-7.11.3.28-60766845.el9.x86_64.rpm
-rw-r--r-- 1 root root      20588 Aug 16 12:45 cloudera-manager-server-7.11.3.28-60766845.el9.x86_64.rpm
-rw-r--r-- 1 root root      15058 Aug 16 12:45 cloudera-manager-server-db-2-7.11.3.28-60766845.el9.x86_64.rpm
-rw-r--r-- 1 root root 1094095967 Aug 16 12:45 cloudera-manager-daemons-7.11.3.28-60766845.el9.x86_64.rpm
[root@cldr-mngr ~]# 
----

*Step 5:* Run createrepo command to create a local repository.
[source,bash]
----
[root@cldr-mngr ~]# createrepo --baseurl http://$(hostname -i)/cloudera-repos/cloudera-manager/ /var/www/html/cloudera-repos/cloudera-manager/
----

[arabic, start=12]
. In a web browser please check and verify cloudera manager repository created by entering baseurl http://10.29.148.150/cloudera-repos/cloudera-manager/[[.underline]#http://13.251.65.11/cloudera-repos/cloudera-manager/#]

*Step 6:* Copy cloudera-manager.repo file to /etc/yum.repos.d/ on all nodes to enable it to find the packages that are locally hosted on the admin node.

[source,bash]
----
[root@cldr-mngr ~]# cp /var/www/html/cloudera-repos/cloudera-manager/cloudera-manager.repo /etc/yum.repos.d/cloudera-manager.repo
----

*Step 7:* Edit cloudera-manager.repo. file as per the customer repository location configuration in the step above. Copy the updated repo file to the ipaserver node so it can be copied to the rest of servers using ansible.

[source,bash]
----
[root@cldr-mngr ~]# vi /etc/yum.repos.d/cloudera-manager.repo
[cloudera-manager]
name=Cloudera Manager 7.11.3.28
baseurl=http://<ip_of_cldr_mngr>/cloudera-repos/cloudera-manager/
#Update IP of Repo/cldr-mngr server
gpgcheck=0
enabled=1
[root@cldr-mngr ~]# scp -r /etc/yum.repos.d/cloudera-manager.repo root@ipaserver:/etc/yum.repos.d/cloudera-manager.repo
----

*Step 8:* From the ansible control node copy the repo files to /etc/yum.repos.d/ of all the nodes of the cluster:

[source,bash]
----
[root@ipaserver ~]# ansible all -m copy -a "src=/etc/yum.repos.d/cloudera-manager.repo dest=/etc/yum.repos.d/cloudera-manager.repo"
----

[arabic, start=2]
. Set Up the Local Parcels for CDP Private Cloud Base 7.1.9

[arabic]
. From a host connected the internet, download CDP Private Cloud Base 7.1.9 parcels for RHEL9 from the URL: https://archive.cloudera.com/p/cdh7/7.1.9.9/parcels/[[.underline]#https://archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/#] and place them in the directory /var/www/html/cloudera-repos/cdh7.1.9/ of the cldr-mngr node.
. Create a directory to download CDH parcels.

[source,bash]
----
[root@cldr-mngr ~]# mkdir -p /var/www/html/cloudera-repos/cdh7.1.9/
----

*Step 3.* Download CDH parcels as highlighted below:

[source,bash]
----
[root@cldr-mngr ~]# cd /var/www/html/cloudera-repos/cdh7.1.9/
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/CDH-7.1.9-1.cdh7.1.9.p1023.60818430-el9.parcel
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/CDH-7.1.9-1.cdh7.1.9.p1023.60818430-el9.parcel.sha1
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/CDH-7.1.9-1.cdh7.1.9.p1023.60818430-el9.parcel.sha256 
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel.sha
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel.sha1
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel.sha256
[root@cldr-mngr cdh7.1.9]# wget https://<username>:<password>@archive.cloudera.com/p/cdh7/7.1.9.1023/parcels/manifest.json
[root@cldr-mngr cdh7.1.9]# chmod -R ugo+rX /var/www/html/cloudera-repos/cdh7.1.9/ && cd

[root@cldr-mngr ~]# ls -l /var/www/html/cloudera-repos/cdh7.1.9/
total 8818000
-rw-r--r-- 1 root root 8984189751 Jun  5 09:44 CDH-7.1.9-1.cdh7.1.9.p1023.60818430-el9.parcel
-rw-r--r-- 1 root root         40 Jun  5 09:42 CDH-7.1.9-1.cdh7.1.9.p1023.60818430-el9.parcel.sha1
-rw-r--r-- 1 root root         64 Jun  5 09:42 CDH-7.1.9-1.cdh7.1.9.p1023.60818430-el9.parcel.sha256
-rw-r--r-- 1 root root   45387086 Jul 30 05:14 KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel
-rw-r--r-- 1 root root         41 Jul 30 05:13 KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel.sha
-rw-r--r-- 1 root root         41 Jul 30 05:14 KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel.sha1
-rw-r--r-- 1 root root         65 Jul 30 05:14 KEYTRUSTEE_SERVER-7.1.9.1023-1.keytrustee7.1.9.1023.p0.60818430-el9.parcel.sha256
-rw-r--r-- 1 root root      31493 Jul 30 05:17 manifest.json
[root@cldr-mngr ~]# 
----

[arabic, start=13]
. In a web browser please check and verify cloudera manager repository created by entering baseurl: http://10.29.148.150/cloudera-repos/cdh7.1.9/[[.underline]#http://13.251.65.11/cloudera-repos/cdh7.1.9/#] (IP is of Cloudera-Manager)

[arabic, start=3]
. Set Up the Local Parcels for CDS 3.3 powered by Apache Spark

[arabic]
. From a host connected the internet, download CDS 3.3 Powered by Apache Spark parcels for RHEL9 from the URL: https://archive.cloudera.com/p/spark3/3.3.7190.4/parcels/[[.underline]#https://archive.cloudera.com/p/spark3/3.3.7191000.4/parcels/#]

[arabic, start=14]
. Although Spark 2 and Spark 3 can coexist in the same CDP Private Cloud Base cluster, you cannot use multiple Spark 3 versions simultaneously. All clusters managed by the same Cloudera Manager Server must use exactly the same version of CDS 3.3 Powered by Apache Spark.

[arabic, start=2]
. Create a directory to download CDS parcels.

[source,bash]
----
[root@cldr-mngr ~]# mkdir -p /var/www/html/cloudera-repos/spark3/3.3.7191000.4
[root@cldr-mngr ~]# cd /var/www/html/cloudera-repos/spark3/3.3.7191000.4
----

*Step 3.* Download CDS parcels as highlighted below:

[source,bash]
----
[root@cldr-mngr 3.3.7191000.4]# wget https://<username>:<password>@archive.cloudera.com/p/spark3/3.3.7191000.4/parcels/SPARK3-3.3.2.3.3.7191000.4-1-1.p0.60728639-el9.parcel
[root@cldr-mngr 3.3.7191000.4]# wget https://<username>:<password>@archive.cloudera.com/p/spark3/3.3.7191000.4/parcels/SPARK3-3.3.2.3.3.7191000.4-1-1.p0.60728639-el9.parcel.sha1
[root@cldr-mngr 3.3.7191000.4]# wget https://<username>:<password>@archive.cloudera.com/p/spark3/3.3.7191000.4/parcels/manifest.json
[root@cldr-mngr 3.3.7191000.4]# chmod -R ugo+rX /var/www/html/cloudera-repos/spark3/ && cd

[root@cldr-mngr ~]# ls -l /var/www/html/cloudera-repos/spark3/3.3.7191000.4
total 1998888
-rw-r--r-- 1 root root 2046842555 Jul 30 07:49 SPARK3-3.3.2.3.3.7191000.4-1-1.p0.60728639-el9.parcel
-rw-r--r-- 1 root root         41 Jul 30 07:49 SPARK3-3.3.2.3.3.7191000.4-1-1.p0.60728639-el9.parcel.sha1
-rw-r--r-- 1 root root       8962 Jul 30 07:48 manifest.json
[root@cldr-mngr ~]# 
----

*Step 4.* In a web browser please check and verify cloudera manager repository created by entering baseurl: http://10.29.148.150/cloudera-repos/spark3.3[[.underline]#http://13.251.65.11/cloudera-repos/spark3#]

[arabic, start=4]
. Install Python 3.9 ***see if directly python3 could be installed by dnf *(Skip if installed previously, as we did)

For support and requirement on minimum python version please refer to: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-cm-install-python-3.8.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-cm-install-python-3.9.html#]

[arabic, start=15]
. Python 3.9 is the default Python implementation provided by RHEL 9 and is usually installed by default. Perform this task to install or re-install it manually. We will run below ansible commands from ipaserver/ ansible control node. Rest of commands need to be run manually on each server.

[arabic]
. To install Python 3.9 standard package on RHEL 9 run following command:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "dnf install -y python3 python3-pip"
[root@ipaserver ~]# ansible all -m shell -a "python3 --version"
Python 3.9.18
[root@ipaserver ~]#
----

*To install standard Python 3.9 binary on RHEL9 at standard or custom location, Follow steps below:

*Step 2.* Install the following packages before installing Python 3.9 from ansible control node:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "sudo dnf install gcc openssl-devel bzip2-devel libffi-devel zlib-devel -y"
----

*Step 3.* Download Python 3.9 and decompress the package by running the following commands:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "cd /opt/ && curl -O https://www.python.org/ftp/python/3.9.18/Python-3.9.18.tgz[[.underline]#https://www.python.org/ftp/python/3.9.18/Python-3.9.18.tgz#] && tar -zxvf Python-3.9.18.tgz"
----

*Step 4.* Go to decompressed Python directory and Install Python 3.9 as follows:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "cd /opt/Python-3.9.18/ && ./configure --enable-optimizations –-enable-shared"
----

[arabic, start=16]
. By default, Python could be installed in any one of the following locations. If you are installing Python 3.9 in any other location, then you must specify the path using the --prefix option.

[source,bash]
----
/usr/bin
/usr/local/python39/bin
/usr/local/bin
/opt/rh/rh-python39/root/usr/bin
----

[arabic, start=17]
. The --enabled-shared option is used to build a shared library instead of a static library.

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "echo $LD_LIBRARY_PATH"
[root@ipaserver ~]# ansible all -m shell -a "echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/' >> ~/.bashrc && source ~/.bashrc"
[root@ipaserver ~]# ansible all -m shell -a "cd /usr/local/bin/ && ls -l"
----

*Step 5.* Built Python 3.9 as follows:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "cd /opt/Python-3.9.18/ && make"
----

*Step 6.* Run the following command to put the compiled files in the default location or in the custom location that you specified using the --prefix option:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "cd /opt/Python-3.9.18/ && make install"
----

*Step 7.* Copy the shared compiled library files (libpython3.9.so) to the /lib64/ directory:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "cd /opt/Python-3.9.18/ && cp --no-clobber ./libpython3.9.so* /lib64/"
----

*Step 8.* Change the permissions of the libpython3.9.so files as follows:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "chmod 755 /lib64/libpython3.9.so*"
----

*Step 9.* If you see an error such as error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory, then run the following command:

[source,bash]
----
[root@ipaserver ~]# ansible all -m shell -a "echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/' >> ~/.bashrc && source ~/.bashrc"
----

*Step 10. _(For Hue)_* If you have installed Python 3.9 at a custom location, then you must append the custom path in Cloudera Manager > Clusters > Hue > Configuration > Hue Service Environment Advanced Configuration Snippet (Safety Valve) separated by colon (:) as follows *_(Later- after Base Cluster Installation)_*:

[source,bash]
----
Key: PATH
Value: [***CUSTOM-INSTALL-PATH***]:/usr/local/sbin:/usr/local/bin:/usr/sbin:
----

*Step 11.* Check Python version

[source,bash]
----
[root@ipaserver ~]# ansible all -m command -a "python3 --version"
pvcbase-worker2.cldrsetup.local | CHANGED | rc=0 >>
Python 3.9.18
pvcbase-worker3.cldrsetup.local | CHANGED | rc=0 >>
Python 3.9.18
pvcbase-master.cldrsetup.local | CHANGED | rc=0 >>
Python 3.9.18
----

[arabic, start=5]
. Install and Configure Database for Cloudera Manager

Cloudera Manager uses various databases and datastores to store information about the Cloudera Manager configuration, as well as information such as the health of the system, or task progress.

Please review https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-install-configure-databases.html[[.underline]#Database Requirement for CDP Private Cloud Base#].

This procedure highlights the installation and configuration steps with PostgreSQL. Please review Install and Configure Databases for CDP Private Cloud Base for more details:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-install-config-postgresql-for-cdp.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-install-config-postgresql-for-cdp.html#]

[arabic, start=18]
. If you already have a PostgreSQL database set up, you can skip to the section Configuring and Starting the PostgreSQL Server to verify that your PostgreSQL configurations meet the requirements for Cloudera Manager.
. We will be installing the external PostgreSQL DB server on the cldr-mngr host.

[arabic]
. Login on *_cldr-mngr server_* and Install PostgreSQL as shown in the steps below.

##### Install and configure POSTGRESQL DB on cldr-mngr server :

**** When you restart any process in future, the configuration for each of the services is redeployed using information saved in the Cloudera Manager database. If this information is not available, your cluster cannot start or function correctly. So, you must schedule and maintain regular backups of the Cloudera Manager database to recover the cluster in the event of the loss of this database.

##### Install the repository RPM:

[source,bash]
----
[root@cldr-mngr ~]# sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm
----

##### Disable the built-in PostgreSQL module:

[source,bash]
----
[root@cldr-mngr ~]# sudo dnf -qy module disable postgresql
----

##### Install PostgreSQL:

[source,bash]
----
[root@cldr-mngr ~]# sudo dnf install -y postgresql14 postgresql14-server postgresql14-libs
[root@cldr-mngr ~]#
----

[arabic, start=2]
. Install the PostgreSQL JDBC driver by running the following command on ansible-controller/ipaserver node. Rename the Postgres JDBC driver .jar file to postgresql-connector-java.jar and copy it to the /usr/share/java directory. The following copy command can be used if the Postgres JDBC driver .jar file is installed from the OS repositories: *_(Skip this, as we already performed it in prior steps)_

[source,bash]
----
##### Install JDBC Connector
[root@ipaserver ~]# wget https://jdbc.postgresql.org/download/postgresql-42.7.3.jar 
[root@ipaserver ~]# mv -v postgresql-42.7.3.jar postgresql-connector-java.jar
[root@ipaserver ~]# ansible all -m copy -a "src=postgresql-connector-java.jar dest=/usr/share/java/postgresql-connector-java.jar"
[root@ipaserver ~]# ansible all -m shell -a "sudo chmod 644 /usr/share/java/postgresql-connector-java.jar"

[root@ipaserver ~]# ansible all -m shell -a "sudo ls -l /usr/share/java/postgresql*.jar"
ipaserver.cldrsetup.local | CHANGED | rc=0 >>
-rw-r--r-- 1 root root 1089312 Jun  3 08:06 /usr/share/java/postgresql-connector-java.jar

##### Alternate way (Not recommended)
[root@ipaserver ~]# ansible all -m shell -a "sudo dnf install postgresql-jdbc -y"
[root@ipaserver ~]# ansible all -m shell -a "java -version"
----

*Step 4.* Make sure that the data directory, which by default is /var/lib/pgsql/14/data/, is on a partition that has sufficient free space.

[arabic, start=20]
. Cloudera Manager supports the use of a custom schema name for the Cloudera Manager Server database. By default, PostgreSQL only accepts connections on the loopback interface. You must reconfigure PostgreSQL to accept connections from external hosts.

*Step 5.* Installing the psycopg2 Python package for PostgreSQL-backed Hue. *_(Skip this, as we already performed it in prior steps)_

[arabic, start=21]
. If you are installing Runtime 7 and using PostgreSQL as a backend database for Hue, then you must install the 2.9.3 version (or greater) of the psycopg2 package on all Hue hosts. The psycopg2 package is automatically installed as a dependency of Cloudera Manager Agent, but the version installed is often lower than 2.9.3

*Step 6.* Make sure the psycopg2 package dependencies for RHEL 9 is installed on all required hosts, by running the following commands:

[source,bash]
----
##### Install the psycopg2-binary package as follows:
[root@ipaserver ~]# ansible all -m shell -a "pip3 install psycopg2-binary && pip3 list | grep psyc"

ipaserver.cldrsetup.local | CHANGED | rc=0 >>
Requirement already satisfied: psycopg2-binary in /usr/local/lib64/python3.9/site-packages (2.9.9)
psycopg2-binary 2.9.9

WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv

[root@ipaserver ~]#
----

*Step 7.* Initialize the database:

[source,bash]
----
# Initialize the DB
[root@cldr-mngr ~]# sudo /usr/pgsql-14/bin/postgresql-14-setup initdb

# Verify PG Version
[root@cldr-mngr ~]# cat /var/lib/pgsql/14/data/PG_VERSION
14

# Verify Psycopg Version
[root@cldr-mngr ~]# pip3 list |grep psycopg
psycopg2-binary 2.9.9

# data directory is very critical, if you want to cleanup postgres simply rename or remove /var/lib/pgsql/14/data directory
----

*Step 8.* Make sure that LC_ALL is set to C.UTF-8 to enable UTF-8 CHARSET and initialize the database as follows:

[source,bash]
----
[root@cldr-mngr ~]# echo 'LC_ALL="C.UTF-8"' >> /etc/locale.conf
----

*Step 9.* To enable MD5 authentication, edit /var/lib/pgsql/14/data/pg_hba.conf by adding the following lines, to enable connection from all outside hosts:

(Enable md5 auth to serve password authentication and TLS/SSL encryption from outside world)

[source,bash]
----
[root@cldr-mngr ~]# vi /var/lib/pgsql/14/data/pg_hba.conf
host    all             all             0.0.0.0/0               md5 # Enable md5 authentication
host    ranger          rangeradmin     0.0.0.0/0               md5 # Allow ranger database connection from any host
hostssl all             all             0.0.0.0/0               md5 # Allow SSL connection from client(s)
# replace 127.0.0.1 with host IP if PostgreSQL access from a different host is required. 
# Edit section for replication privilege. HA not documented in this solution.

##### Backup the config so you can use it in case of re-setup.
[root@cldr-mngr ~]# cp /var/lib/pgsql/14/data/pg_hba.conf ~

##### If you have the file backed up, copy it
[root@cldr-mngr ~]# cp /var/lib/pgsql/14/data/pg_hba.conf /var/lib/pgsql/14/data/pg_hba.conf_orig
[root@cldr-mngr ~]# cp ~/pg_hba.conf /var/lib/pgsql/14/data/pg_hba.conf
----

*Step 10.* Configure settings to ensure your system performs as expected. Update these settings in the /var/lib/pgsql/14/data/postgresql.conf file. Settings vary based on cluster size and resources as follows:

[source,bash]
----
[root@cldr-mngr ~]# vi /var/lib/pgsql/14/data/postgresql.conf
port = 5432                             # (change requires restart)    #### uncomment
listen_addresses = '*'                  # what IP address(es) to listen on;
max_connections = 1000                  # (change requires restart)
shared_buffers = 1024MB                 # min 128kB
wal_buffers = 16MB                      # min 32kB, -1 sets based on shared_buffers
max_wal_size = 6GB
min_wal_size = 512MB
checkpoint_completion_target = 0.9      # checkpoint target duration, 0.0 - 1.0 #### uncomment
standard_conforming_strings = off
jit = off

##### Backup the config so you can use it in case of re-setup.
[root@cldr-mngr ~]# cp /var/lib/pgsql/14/data/postgresql.conf ~

##### If you have the file backed up, copy it
[root@cldr-mngr ~]# cp /var/lib/pgsql/14/data/postgresql.conf /var/lib/pgsql/14/data/postgresql.conf_orig
[root@cldr-mngr ~]# cp ~/postgresql.conf /var/lib/pgsql/14/data/postgresql.conf
----

[arabic, start=22]
. Settings vary based on cluster size and resources.

*Step 11.* Start the PostgreSQL Server and configure it to start at boot.

[source,bash]
----
[root@cldr-mngr ~]# systemctl start postgresql-14.service
[root@cldr-mngr ~]# systemctl enable postgresql-14.service
[root@cldr-mngr ~]# systemctl status postgresql-14.service -l
[root@cldr-mngr ~]# netstat -ltnupa | grep LIST | grep -E '5432|postgres'
----

*Step 12.* Create or verify login

[source,bash]
----
[root@cldr-mngr ~]# sudo -u postgres psql
could not change directory to "/root": Permission denied
psql (14.13)
Type "help" for help.

postgres=# ALTER USER postgres PASSWORD 'postgres';
ALTER ROLE
postgres=# \q

[root@cldr-mngr ~]# psql -h cldr-mngr.cldrsetup.local -d postgres -U postgres
Password for user postgres:
psql (14.13)
Type "help" for help.

postgres=#\q
----

*Step 13.* Enable TLS 1.2 for PostgreSQL database before setting up Cloudera Manager.

[source,bash]
----
##### Verify TLS is enabled or not:
[root@cldr-mngr ~]# sudo -u postgres psql
could not change directory to "/root": Permission denied
psql (14.13)
Type "help" for help.

postgres=# SELECT * FROM pg_stat_ssl;
  pid  | ssl | version | cipher | bits | client_dn | client_serial | issuer_dn
-------+-----+---------+--------+------+-----------+---------------+-----------
 41275 | f   |         |        |      |           |               |
(1 row)

postgres=# SHOW ssl;
 ssl
-----
 off
(1 row)

postgres=# \q

[root@cldr-mngr ~]# sudo dnf install -y mod_ssl

##### Stop Postgres DB service. 
[root@cldr-mngr ~]# systemctl stop postgresql-14

[root@cldr-mngr ~]# cd /var/lib/pgsql/14/data/

##### Generate CA-signed certificates for clients to verify with openssl command line tool. 
##### Update value for "-days 3650". Currently set for 3650 days = 10 years.

##### create a certificate signing request (CSR) and a public/private key file
[root@cldr-mngr data]# openssl req -new -nodes -text -out root.csr -keyout root.key -subj '/C=US/ST=California/L=Santa Clara/O=Cloudera Inc/OU=CLDR/CN=cldr-mngr.cldrsetup.local'

##### Output for above command
[root@cldr-mngr data]# ls -ltr root*
total 8
-rw------- 1 root root 1704 Jun  3 07:43 root.key
-rw-r--r-- 1 root root 3589 Jun  3 07:43 root.csr
[root@cldr-mngr data]# 

[root@cldr-mngr data]# chmod 400 root.key

[root@cldr-mngr data]# ls -ltr root*
total 8
-r-------- 1 root root 1704 Jun  3 07:43 root.key
-rw-r--r-- 1 root root 3589 Jun  3 07:43 root.csr
[root@cldr-mngr data]# 

##### create a root certificate authority
[root@cldr-mngr data]# openssl x509 -req -in root.csr -text -days 3650 -extfile /etc/ssl/openssl.cnf -extensions v3_ca -signkey root.key -out root.crt
Certificate request self-signature ok
subject=C = US, ST = California, L = Santa Clara, O = Cloudera Inc, OU = CLDR, CN = cldr-mngr.cldrsetup.local

[root@cldr-mngr data]# ls -l
total 16
-r-------- 1 root root 1704 Jun  3 07:43 root.key
-rw-r--r-- 1 root root 3589 Jun  3 07:43 root.csr
-rw-r--r-- 1 root root 4592 Jun  3 07:46 root.crt
[root@cldr-mngr data]# 

# create a server certificate signed by the new root certificate authority
[root@cldr-mngr data]# openssl req -new -nodes -text -out server.csr -keyout server.key -subj "/CN=cldr-mngr.cldrsetup.local" 

[root@cldr-mngr data]# ls -ltr root* server*
total 24
-r-------- 1 root root 1704 Jun  3 07:43 root.key
-rw-r--r-- 1 root root 3589 Jun  3 07:43 root.csr
-rw-r--r-- 1 root root 4592 Jun  3 07:46 root.crt
-rw------- 1 root root 1704 Jun  3 07:47 server.key
-rw-r--r-- 1 root root 3388 Jun  3 07:47 server.csr
[root@cldr-mngr data]#

[root@cldr-mngr data]# chmod 400 server.key 

[root@cldr-mngr data]# ls -ltr root* server*
total 24
-r-------- 1 root root 1704 Jun  3 07:43 root.key
-rw-r--r-- 1 root root 3589 Jun  3 07:43 root.csr
-rw-r--r-- 1 root root 4592 Jun  3 07:46 root.crt
-r-------- 1 root root 1704 Jun  3 07:47 server.key
-rw-r--r-- 1 root root 3388 Jun  3 07:47 server.csr
[root@cldr-mngr data]# 

[root@cldr-mngr data]# openssl x509 -req -in server.csr -text -days 3650 -CA root.crt -CAkey root.key -CAcreateserial -out server.crt
Certificate request self-signature ok
subject=CN = cldr-mngr.cldrsetup.local

[root@cldr-mngr data]# ls -ltr root* server*
total 32
-r-------- 1 root root 1704 Jun  3 07:43 root.key
-rw-r--r-- 1 root root 3589 Jun  3 07:43 root.csr
-rw-r--r-- 1 root root 4592 Jun  3 07:46 root.crt
-r-------- 1 root root 1704 Jun  3 07:47 server.key
-rw-r--r-- 1 root root 3388 Jun  3 07:47 server.csr
-rw-r--r-- 1 root root   41 Jun  3 07:51 root.srl
-rw-r--r-- 1 root root 3933 Jun  3 07:51 server.crt
[root@cldr-mngr data]# 

##### The above steps will create a server.crt and server.key file in that location.

[root@cldr-mngr data]# chown postgres:postgres server.crt server.key root.crt

##### Artifacts generated from above command:
[root@cldr-mngr data]# ls -l server\.* root\.*
-rw-r--r--. 1 postgres postgres 4586 Aug  9 14:34 root.crt
-rw-r--r--. 1 root     root     3584 Aug  9 14:33 root.csr
-r--------. 1 root     root     1704 Aug  9 14:33 root.key
-rw-r--r--. 1 root     root       41 Aug  9 14:37 root.srl
-rw-r--r--. 1 postgres postgres 3928 Aug  9 14:37 server.crt
-rw-r--r--. 1 root     root     3388 Aug  9 14:35 server.csr
-r--------. 1 postgres postgres 1704 Aug  9 14:35 server.key
[root@cldr-mngr data]# 

##### Verify Key and Certs generated fine
[root@cldr-mngr data]# openssl rsa -noout -text -in server.key
Private-Key: (2048 bit, 2 primes)
modulus:
    00:b3:30:86:66:49:8d:c4:de:62:c6:17:e2:50:6c:
    88:91:10:49:26:6a:7f:a7:1d:6a:33:3a:71:0d:2c:
    f0:08:1b:3d:88:bc:73:43:b9:82:00:1a:a3:15:0f:
    08:ed:53:94:be:1e:25:7b:dd:99:66:c0:f5:2d:42:
    92:f0:d6:52:67:18:80:ab:a1:86:e1:aa:5c:53:47:
    41:3c:e2:2e:e1:dd:f8:5d:b7:e0:d0:39:26:f4:23:
    3d:78:71:9f:75:66:a0:0e:c7:9a:bc:c2:fb:db:1b:
    d1:fe:b2:2e:5d:a5:72:54:5f:04:54:1a:d8:76:77:
    a8:04:9d:05:9a:f6:25:5b:ed:73:88:6b:1a:e6:0f:
    09:62:d3:19:07:7c:2b:77:d0:5d:af:c3:bd:ff:44:
    7f:a9:08:b9:b2:e3:8c:5a:fd:90:dd:c7:bf:db:1e:
    c9:fe:72:16:e2:09:c2:0c:90:de:31:8b:06:58:e8:
    6c:37:7a:a4:bf:91:7e:ca:d4:15:60:d8:6f:b7:0b:
    e5:a1:5c:a2:30:98:d4:34:9c:69:88:57:f4:d1:b8:
    2a:1d:a1:c6:1f:5c:1d:10:56:5a:80:b5:5d:f3:f1:
    59:7f:4b:42:2c:82:3d:96:6d:5d:91:88:2a:de:12:
    6b:b4:65:f3:9d:c0:b8:02:4b:a6:21:bc:3b:5c:3f:
    32:3b
publicExponent: 65537 (0x10001)
privateExponent:
    12:78:80:8a:1f:af:dc:e8:bd:8e:c4:dc:7f:c4:c8:
    49:07:c0:3a:95:04:c6:91:aa:26:50:b2:61:94:cd:
    c3:50:27:86:26:42:cd:6a:dc:63:2d:5b:bd:2a:79:
    15:99:a5:7d:f9:76:8c:af:99:85:f5:82:f0:60:e9:
    eb:a8:74:03:0b:8c:0b:e5:11:15:c6:ed:50:6a:4a:
———
———
———

[root@cldr-mngr data]# openssl x509 -noout -text -in server.crt
Certificate:
    Data:
        Version: 1 (0x0)
        Serial Number:
            40:26:f6:7b:84:d1:ad:30:65:2a:07:df:20:f8:4f:a3:91:0e:09:c7
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: C = US, ST = California, L = Santa Clara, O = Cloudera Inc, OU = CLDR, CN = cldr-mngr.cldrsetup.local
        Validity
            Not Before: May 14 06:33:09 2024 GMT
            Not After : May 12 06:33:09 2034 GMT
        Subject: CN = cldr-mngr.cldrsetup.local
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:b3:30:86:66:49:8d:c4:de:62:c6:17:e2:50:6c:
                    88:91:10:49:26:6a:7f:a7:1d:6a:33:3a:71:0d:2c:
                    f0:08:1b:3d:88:bc:73:43:b9:82:00:1a:a3:15:0f:
                    08:ed:53:94:be:1e:25:7b:dd:99:66:c0:f5:2d:42:
———
———
———

# Correct permissions for the private key file
[root@cldr-mngr data]# chmod 644 server.crt root.crt
[root@cldr-mngr data]# chmod 0600 /var/lib/pgsql/14/data/server.key

##### Edit Configuration file for PostgreSQL (postgresql.conf) to enable SSL 
[root@cldr-mngr data]# cat <<EOF >> /var/lib/pgsql/14/data/postgresql.conf
ssl = on
ssl_ca_file = 'root.crt'
ssl_cert_file = 'server.crt'
ssl_key_file = 'server.key'
EOF

# Find the location of the private key file, typically in the data directory
[root@cldr-mngr data]# ls -l /var/lib/pgsql/14/data/server.key

##### Restart PostgreSQL database service to pick up the SSL related configuration changes and verify login with SSL
[root@cldr-mngr data]# systemctl restart postgresql-14.service
[root@cldr-mngr data]# systemctl status postgresql-14.service -l

[root@cldr-mngr data]# psql -h cldr-mngr.cldrsetup.local -d postgres -U postgres
Password for user postgres: <postgres>
psql (14.13)
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
Type "help" for help.

postgres=# SELECT * FROM pg_stat_ssl;
  pid  | ssl | version |         cipher         | bits | client_dn | client_serial | issuer_dn
-------+-----+---------+------------------------+------+-----------+---------------+-----------
 43895 | t   | TLSv1.3 | TLS_AES_256_GCM_SHA384 |  256 |           |               |
(1 row)

postgres=# SHOW ssl;
 ssl
-----
 on
(1 row)

postgres=#
##### Verify SSL is actually applied for DB 
postgres=# SELECT name, setting FROM pg_settings WHERE name LIKE '%ssl%';
postgres-# 
postgres-# 
                  name                  |         setting
----------------------------------------+--------------------------
 ssl                                    | on
 ssl_ca_file                            | root.crt
 ssl_cert_file                          | server.crt
 ssl_ciphers                            | HIGH:MEDIUM:+3DES:!aNULL
 ssl_crl_dir                            |
 ssl_crl_file                           |
 ssl_dh_params_file                     |
 ssl_ecdh_curve                         | prime256v1
 ssl_key_file                           | server.key
 ssl_library                            | OpenSSL
 ssl_max_protocol_version               |
 ssl_min_protocol_version               | TLSv1.2
 ssl_passphrase_command                 |
 ssl_passphrase_command_supports_reload | off
 ssl_prefer_server_ciphers              | on
(15 rows)
postgres-# \q

##### Copy /var/lib/pgsql/14/data/root.crt to /root/.postgresql/root.crt
[root@cldr-mngr data]# mkdir -p /root/.postgresql/
[root@cldr-mngr data]# cp /var/lib/pgsql/14/data/root.crt /root/.postgresql/root.crt

##### Copy the root.crt to all other hosts with the help of ansible, for this copy the root.crt file to ipaserver/ansible control node
[root@cldr-mngr data]# scp -r /root/.postgresql/root.crt root@ipaserver:~/

##### Login to ipaserver and copy the root.crt Postgres DB certificate file to all other nodes  at location /root/.postgresql/ with the help of ansible.

[root@ipaserver ~]# ls -l
[root@ipaserver ~]# chmod 644 root.crt
[root@ipaserver ~]# ansible all -m shell -a "mkdir -p /root/.postgresql/ && chmod -R 755 /root/.postgresql/"
[root@ipaserver ~]# ansible all -m copy -a "src=root.crt dest=/root/.postgresql/root.crt"

[root@cldr-mngr data]# psql -h cldr-mngr.cldrsetup.local -p 5432 -U postgres "dbname=postgres sslmode=verify-full"
Password for user postgres: 
psql (14.13)
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
Type "help" for help.
postgres=# 
postgres=# \q

[root@cldr-mngr data]# psql -h cldr-mngr.cldrsetup.local -p 5432 -U postgres "dbname=postgres sslmode=verify-ca"
Password for user postgres: 
psql (14.13)
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
Type "help" for help.
postgres=# 
postgres=# \q
[root@cldr-mngr data]# 
----

*Step 14.* Create databases and service accounts for components that require databases. Following components requires databases: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-required-databases.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-required-databases.html#] https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-configuring-starting-postgresql-server.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-configuring-starting-postgresql-server.html#]

[arabic, start=23]
. The databases must be configured to support the PostgreSQL UTF8 character set encoding.
. Record the values you enter for database names, usernames, and passwords. The Cloudera Manager installation wizard requires this information to correctly connect to these databases.

[source,bash]
----
##### Create CM DB and USERS 
[root@cldr-mngr data]# sudo -u postgres psql

CREATE ROLE scm LOGIN PASSWORD 'scm';
CREATE DATABASE scm OWNER scm ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE scm TO scm;

CREATE ROLE rman LOGIN PASSWORD 'rman';
CREATE DATABASE rman OWNER rman ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE rman TO rman;

CREATE ROLE hue LOGIN PASSWORD 'hue';
CREATE DATABASE hue OWNER hue ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE hue TO hue;

CREATE ROLE hive LOGIN PASSWORD 'hive';
CREATE DATABASE hive OWNER hive ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE hive TO hive;

CREATE ROLE oozie LOGIN PASSWORD 'oozie';
CREATE DATABASE oozie OWNER oozie ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE oozie TO oozie;

CREATE ROLE rangeradmin LOGIN PASSWORD 'rangeradmin';
CREATE DATABASE ranger OWNER rangeradmin ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE ranger TO rangeradmin;

/*For Ranger KMS, use rangerkms rather than rangeradmin user.*/
CREATE ROLE rangerkms LOGIN PASSWORD 'rangerkms';
CREATE DATABASE rangerkms OWNER rangerkms ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE rangerkms TO rangerkms;

CREATE ROLE schemaregistry LOGIN PASSWORD 'schemaregistry';
CREATE DATABASE schemaregistry OWNER schemaregistry ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE schemaregistry TO schemaregistry;

CREATE ROLE yqm LOGIN PASSWORD 'yqm';
CREATE DATABASE yqm OWNER yqm ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE yqm TO yqm;

/*For the SMM metadata store, create a database called smm with the password smm:*/
CREATE ROLE smm LOGIN PASSWORD 'smm';
CREATE DATABASE smm OWNER smm ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE smm TO smm;

CREATE ROLE das LOGIN PASSWORD 'das';
CREATE DATABASE das OWNER das ENCODING 'UTF8';
GRANT ALL PRIVILEGES ON DATABASE das TO das;

ALTER DATABASE hive SET standard_conforming_strings=off;
ALTER DATABASE oozie SET standard_conforming_strings=off;
SELECT 1;
SHOW ssl;
\q

--- Alternate commands
--- CREATE USER registry WITH PASSWORD 'registry';
--- GRANT ALL PRIVILEGES ON DATABASE "registry" to registry;

----

[arabic, start=25]
. If you plan to use Apache Ranger, please visit https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-configuring-postgresql-db-for-ranger.html[[.underline]#Configuring a PostgreSQL Database for Ranger or Ranger KMS#] for instructions on creating and configuring the Ranger database. *included above- install JDBC driver, create DB for ranger etc.
. If you plan to use Schema Registry or Streams Messaging Manager, please visit https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-streaming-configuring-databases.html[[.underline]#Configuring the Database for Streaming Components#] for instructions on configuring the database. included above- create smm and registry db etc.

The following procedure describes how to install Cloudera Manager and then using Cloudera Manager to install Cloudera Data Platform Private Cloud Base 7.1.9.

[arabic, start=6]
. Install Cloudera Manager Server (CM-UI)

Cloudera Manager, an end-to-end management application, is used to install and configure CDP Private Cloud Base. During CDP Installation, Cloudera Manager's Wizard will help to install Hadoop services and any other role(s)/service(s) on all nodes using the following procedure:

* Discovery of the cluster nodes
* Configure the Cloudera parcel or package repositories
* Install Hadoop, Cloudera Manager Agent (CMA) and Impala on all the cluster nodes.
* Install the Oracle JDK or OpenJDK if it is not already installed across all the cluster nodes.
* Assign various services to nodes.
* Start the Hadoop services

[arabic, start=27]
. Please see the https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-java-requirements.html[[.underline]#JAVA requirements#] for CDP Private Cloud Base.

[arabic]
. Install the Cloudera Manager Server packages by running following command:

[source,bash]
----
[root@cldr-mngr data]# dnf install -y cloudera-manager-agent cloudera-manager-daemons cloudera-manager-server

# Recommendation: Always install agents via CM-UI only. Never install manually as it generates agent config as localhost and leads to heartbeat error. If HeartBeat error comes up, then run below command to update agent config (before start scm-server)

[root@cldr-mngr data]# sed -i 's/server_host=localhost/server_host=cldr-mngr.cldrsetup.local/g' /etc/cloudera-scm-agent/config.ini
----

[arabic, start=2]
. Enable TLS 1.2 on Cloudera Manager Server. https://docs.cloudera.com/cloudera-manager/7.11.3/installation/topics/cdpdc-enable-tls-12-cm-server.html[[.underline]#https://docs.cloudera.com/cloudera-manager/7.11.3/installation/topics/cdpdc-enable-tls-12-cm-server.html#]
. Import the PostgreSQL root certificate in Step 5.
. If the Database host and Cloudera Manager Server host are located on the same machine, then perform the following steps to import the PostgreSQL database root certificate, as mentioned below in Step 5:
. Go to the path where root certificates are stored. By default it is /var/lib/pgsql/14/data/.

[source,bash]
----
##### Configure CDP to use SSL Enabled DB 

# Create a new directory in the following path by running the following command:
[root@cldr-mngr data]# mkdir -p /var/lib/cloudera-scm-server/.postgresql
[root@cldr-mngr data]# chmod 755 /var/lib/cloudera-scm-server/.postgresql
[root@cldr-mngr data]# cd /var/lib/cloudera-scm-server/.postgresql

# Copy the PostgreSQL root certificate to the new directory on the Cloudera Manager server host by running the following command:
[root@cldr-mngr data]# cp /var/lib/pgsql/14/data/root.crt root.crt

# Change the ownership of the root certificate by running the following command:
[root@cldr-mngr data]# chown cloudera-scm: root.crt
[root@cldr-mngr data]# ls -lt
total 8
-rw-r--r-- 1 cloudera-scm cloudera-scm 4639 Mar  5 16:59 root.crt

# Include this root certificate path in the JDBC URL as follows:
# jdbc:postgresql://<DB HOSTNAME>:<DB-PORT>/<DB NAME>?ssl=true&sslmode=verify-ca&sslrootcert=<PATH_TO_ROOT_CERTIFICATE>
# jdbc:postgresql://cldr-mngr.cldrsetup.local:5432/scm?ssl=true&sslmode=verify-ca&sslrootcert=/var/lib/cloudera-scm-server/.postgresql/root.crt

##### Changes required for Ranger SSL

[root@cldr-mngr data]# cp /usr/share/java/postgresql-connector-java.jar /opt/cloudera/cm/lib/postgresql-connector.jar
[root@cldr-mngr data]# unlink /opt/cloudera/cm/lib/postgresql-42.*.jar
[root@cldr-mngr data]# ls -ltr /opt/cloudera/cm/lib/*postgres*
[root@cldr-mngr data]# chmod 755 /var/lib/cloudera-scm-server/
[root@cldr-mngr data]# ls -ltr /var/lib/cloudera-scm-server/.postgresql/*.crt
[root@cldr-mngr data]# 
----

[arabic, start=6]
. Run the scm_prepare_database.sh script to check and generate database configuration file for cloudera-manager i.e. db.properties and test the database connection between cloudera-manager and database server:

[source,bash]
----
# Run the script to configure PostgreSQL with TLS 1.2 enabled
###### sudo /opt/cloudera/cm/schema/scm_prepare_database.sh -h<DB HOSTNAME> --jdbc-url "jdbc:postgresql://db_server_host:db_port/db_name?ssl=true&sslmode=verify-ca&sslrootcert=<PATH_TO_DB_ROOT_CERTIFICATE>" <db_type:postgresql> <db_name> <db_role_user> <dn_user_password> --ssl

[root@cldr-mngr ~]# sudo /opt/cloudera/cm/schema/scm_prepare_database.sh -hcldr-mngr.cldrsetup.local --jdbc-url "jdbc:postgresql://cldr-mngr.cldrsetup.local:5432/scm?ssl=true&sslmode=verify-ca&sslrootcert=/var/lib/cloudera-scm-server/.postgresql/root.crt" postgresql scm scm scm --ssl

JAVA_HOME=/usr/lib/jvm/java-17-openjdk-11.0.24.0.8-2.el9.x86_64
Verifying that we can write to /etc/cloudera-scm-server
Creating SCM configuration file in /etc/cloudera-scm-server
Executing:  /usr/lib/jvm/java-17-openjdk-17.0.13.0.11-4.el9.x86_64/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.
[main] DbCommandExecutor              INFO  A JDBC URL override was specified. Using this as the URL to connect to the database and overriding all other values.
[main] DbCommandExecutor              INFO  Successfully connected to database.
All done, your SCM database is configured correctly!

[root@cldr-mngr ~]# 
----

[arabic, start=7]
. Upon successful connection, the scm_prepare_database.sh script writes the content of /etc/cloudera-scm-server/db.properties file as shown below, verify the content, should look like below:

[source,bash]
----
[root@cldr-mngr ~]# cat /etc/cloudera-scm-server/db.properties
# Auto-generated by scm_prepare_database.sh on Tue Mar  5 08:02:56 PM PST 2024
#
# For information describing how to configure the Cloudera Manager Server
# to connect to databases, see the "Cloudera Manager Installation Guide."
#
com.cloudera.cmf.db.type=postgresql
com.cloudera.cmf.db.host=cldr-mngr.cldrsetup.local
com.cloudera.cmf.db.name=scm
com.cloudera.cmf.db.user=scm
com.cloudera.cmf.db.setupType=EXTERNAL
com.cloudera.cmf.db.password=scm
com.cloudera.cmf.orm.hibernate.connection.url=jdbc:postgresql://cldr-mngr.cldrsetup.local:5432/scm?ssl=true&sslmode=verify-ca&sslrootcert=/var/lib/cloudera-scm-server/.postgresql/root.crt
[root@cldr-mngr ~]#
----

[arabic, start=8]
. Start the Cloudera Manager Server:

[source,bash]
----
[root@cldr-mngr ~]# systemctl start cloudera-scm-server cloudera-scm-agent
[root@cldr-mngr ~]# systemctl enable cloudera-scm-server cloudera-scm-agent
[root@cldr-mngr ~]# systemctl status cloudera-scm-server cloudera-scm-agent -l

##### Run the below command to check the logs of cloudera-scm-server starting up. Wait until you see the Started Jetty server message on the screen.
[root@cldr-mngr ~]# sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log
----

[arabic, start=9]
. The Cloudera Manager should show the below logs before the UI actually comes up.

image:image111.png[jetty started,width=635,height=66]

[arabic, start=10]
. Once the Cloudera-Manager(CM) installation is completed, open the endpoint URL [.underline]#http://<cldr-mgr_ip_addr>:7180/# of the Cloudera Manager WebUI and login to the CM using default credentials. (Username: admin, Password: admin)image:image170.png[image170,width=695,height=444]

[arabic, start=28]
. Default username and password for Cloudera Manager is admin/admin.

[arabic, start=11]
. After logging in to Cloudera-Manager WebUI using the above credentials and uploading the Licence Key, open a new tab of CM-UI on browser and search for JAVA PATH in search bar present at left hand side:

# Override JAVA PATH in CM-UI>* Search JAVA> Override the value> Save Changes> Restart Cloudera-SCM-Server

[source,bash]
----
/usr/lib/jvm/java-17-openjdk-17.0.13.0.11-4.el9.x86_64 (Take the correct Java Path from your system, where java is installed)_
----

[arabic, start=12]
. The Welcome to Cloudera Manager page appears. Since you would have received the CDP license before, select *_Upload Cloudera Data Platform License_* and upload the downloaded .txt or .zip file with the license information.

image:image85.png[A screenshot of a cloud computing software Description automatically generated,width=625,height=209]

[arabic, start=13]
. Activate your license for Cloudera Data Platform by clicking the *_Continue_* button. Click Continue.
. The Add Private Cloud Base Cluster page appears. Next, we will enable AutoTLS for CM.
. As a prerequisite step to *_enabling_* *_AutoTLS_*, login to the cldr-mngr node as user root, and verify *_cloudera-manager-agent_* software is installed and running successfully. Verify the logs in below file:

[source,bash]
----
[root@cldr-mngr ~]# tail -f /var/log/cloudera-scm-agent/cloudera-scm-agent.log

##### Verify the same by running the below command. This should return the output stating the service is active and in running state.
[root@cldr-mngr ~]# systemctl status cloudera-scm-agent -l
----

image:image180.png[image180,width=693,height=101]

***************************************************************************************************

[arabic, start=7]
. Enable AutoTLS

Auto-TLS is managed using the certmanager utility, which is included in the Cloudera Manager Agent software, and not the Cloudera Manager Server software. You must install the Cloudera Manager Agent software on the Cloudera Manager Server host to be able to use the utility. You can use certmanager to manage auto-TLS on a new installation. For more information, go to: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-encrypting-data-in-transit/topics/cm-security-auto-tls.html[[.underline]#Configuring TLS Encryption for Cloudera Manager Using Auto-TLS#]

[arabic]
. Click on the link *_here to setup Enable AutoTLS_* to set up AutoTLS through Cloudera Manager on the *_Add Private Cloud Base Cluster_* page.

image:image185.png[image185,width=627,height=381]

[arabic, start=2]
. Below screen will appear. Enter the values for the parameters as shown below. (*We will be using the private key approach*, you can use password option as well, both options should considerably work)

[width="100%",cols="39%,61%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Enable TLS for
____

a|
____
All existing and future clusters
____

a|
____
SSH username
____

a|
____
root
____

a|
____
Authentication method
____

a|
____
All hosts accept same private key / All hosts accept same password
____

a|
____
Private Key (If using Key approach)
____

a|
____
Choose the private key created and downloaded in earlier section
____

a|
____
Password (If using Password approach)
____

a|
____
Enter VM’s root users’ password
____

a|
____
Confirm Password
____

a|
____
Enter VM’s root users’ password (again)
____

|===

image:image175.png[A screenshot of a computer Description automatically generated,width=627,height=381]

[loweralpha]
. *Screenshot for using the Password based authentication method.

image:image122.png[image122,width=620,height=417]

[loweralpha, start=2]
. *Screenshot for using the Private Key based authentication method.

[arabic, start=3]
. Click *_Next_* to continue. Below screen will appear, if all the values are entered properly.

image:image107.png[A screenshot of a computer Description automatically generated,width=622,height=177]

[arabic, start=4]
. Click on Finish.
. After enabling the AutoTLS for the CM-UI through the browser, login to cldr-mngr node at backend as user root and restart Cloudera Manager Server, suggested in the previous screenshot.

[root@cldr-mngr ~]# systemctl restart cloudera-scm-server

[root@cldr-mngr ~]# systemctl status cloudera-scm-server -l

##### Run the below command to check the logs of cloudera-scm-server starting up. Wait until you see the *_Started Jetty server_* message on the screen.

[root@cldr-mngr ~]# sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log

[arabic, start=6]
. Once you see the message *_Started Jetty server_* in the logs, Login to Cloudera Manager using URL endpoint, *_[.underline]#http://<IP_for_CM_Server>:7180#_*, in a new incognito window.
. The URL should get redirected to https at 7183 port i.e. https://10.29.148.150:7183/[[.underline]#https://<CM_SRVR_IP_ADDR>:7183/#] This means that the AutoTLS configuration is successful. You might get a warning message on the browser related to the certificate. You can ignore the warning and visit the website as this is not a signed certificate.
. Enter the default credentials (admin/admin) and click on *_Login_*. You should see AutoTLS enabled as shown in the image below.

image:image134.png[A screenshot of a computer Description automatically generated,width=635,height=341]

[arabic, start=8]
. Enable Kerberos:- Kerberos Integration with CDP

Cloudera Manager provides a wizard for integrating your organization’s Kerberos with your cluster to provide authentication services. Cloudera Manager clusters can be integrated with MIT Kerberos, Red Hat Identity Management (or the upstream FreeIPA), or Microsoft Active Directory. For more information, see https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-kerberos-authentication/topics/cm-security-kerberos-enabling-intro.html[[.underline]#Enable Kerberos Authentication for CDP.#]

[arabic, start=29]
. In our lab, we configured RedHat FreeIPA based Kerberos authentication. We presume that FreeIPA is pre-configured with user(s) and proper authentication is set up for Kerberos Authentication.
. Before integrating Kerberos with your cluster, configure TLS encryption between Cloudera Manager Server and all Cloudera Manager Agent host systems in the cluster. During the Kerberos integration process, Cloudera Manager Server sends keytab files to the Cloudera Manager Agent hosts, and TLS encrypts the network communication, so these files are protected.
. For FreeIPA, you must have administrative privileges to the ipaserver instance for initial setup and for on-going management, or you will need to have the help of your LDAP administrator prior to and during the integration process. For example, administrative access is needed to access the FreeIPA Kerberos KDC, create principals, and troubleshoot Kerberos TGT/TGS-ticket-renewal and take care of any other issues that may arise.

[arabic]
. In case, you configure *_Active-Directory_* based Kerberos authentication. We presume that Active Directory is pre-configured with OU, user(s) and proper authentication is setup for Kerberos Authentication. LDAP users and bind users are expected to be in the same branch/OU.
. For *_Active Directory_*, you must have administrative privileges to the Active Directory instance for initial setup and for on-going management, or you will need to have the help of your AD administrator prior to and during the integration process. For example, administrative access is needed to access the Active Directory KDC, create principals, and troubleshoot Kerberos TGT/TGS-ticket-renewal and take care of any other issues that may arise.

[arabic]
. Before proceeding further with KDC setup, we need to ensure that the changes to *_krb5.conf_* related to the default cache is not reversed. View the contents of the file *_/etc/krb5.conf_* after logging in to both *_ipaserver_* node and *_cldr-mngr_* node and check whether the property *_default_ccache_name_* is commented out. If not, then open the file and comment it out.

[root@ipaserver ~]# sudo vi /etc/krb5.conf

#default_ccache_name = KEYRING:persistent:%{uid}

image:image48.png[ipa kerb keyring,width=398,height=155]

If you have made any changes, only then run the below commands to restart all the IPA services. If not, skip to the next step.

[root@ipaserver ~]# ipactl restart

[arabic, start=2]
. Now, move to CM-UI on your browser.
. In the Cloudera manager console click on *_here to set up a KDC_*, on the same TLS page, to enable the kerberos authentication on the cluster.

image:image134.png[image134,width=630,height=414]

[arabic, start=4]
. Click Continue.
. Select *_RedHat IPA_* as shown below and check the box for *_I have completed all the above steps._

We have already installed the required RedHat FreeIPA/ Kerberos dependency packages openldap-clients, krb5-workstation and krb5-libs in previous steps.

image:image89.png[image89,width=624,height=342]

[arabic, start=6]
. Select *_Active Directory_* as shown below, *if you’re proceeding with AD based Kerberos integration* and check the box for *_I have completed all the above steps._* Setting up AD is beyond the scope of this document.

We have already installed the required RedHat FreeIPA/ Kerberos dependency packages openldap-clients, krb5-workstation and krb5-libs in previous steps. *_(Skip this step, in our case)_

image:image162.png[A screenshot of a computer Description automatically generated,width=558,height=504]

[arabic, start=7]
. As recommended, install the following in all Cloudera Manager hosts by running the following command. Once completed, click the checkbox *_“I have completed all the above steps”_* and click *_Continue_*.

*_(Skip the below command execution step, as we already installed the dependencies in prior steps)_

[root@ipaserver ~]# ansible all -m command -a "dnf install -y openldap-clients krb5-workstation krb5-libs"

[arabic, start=8]
. For enabling Kerberos, under the Enter KDC Information page, provide below inputs for *_Enter KDC information_* for this Cloudera Manager. Use link:#kix.ivwdrt82tlzz[[.underline]#Table 6#] as an example to fill-in the KDC setup information, provide below inputs and click Next.

[arabic, start=4]
. *KDC Setup components and their corresponding value

[width="100%",cols="62%,38%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Kerberos Encryption Types
____

a|
____
aes256-cts-hmac-sha1-96
____

a|
____
Kerberos Security Realm
____

a|
____
CLDRSETUP.LOCAL
____

a|
____
KDC Server Host
____

a|
____
ipaserver.cldrsetup.local
____

a|
____
KDC Admin Server Host
____

a|
____
ipaserver.cldrsetup.local
____

a|
____
Domain Name(s)
____

a|
____
cldrsetup.local
____

a|
____
Base DN
____

a|
____
dc=cldrsetup,dc=local
____

a|
____
Active Directory Suffix *(Only for AD based Kerberos)
____

a|
____
OU=admin,DC=cldrsetup,DC=local
____

a|
____
Active Directory Delete Accounts on Credential Regeneration *(Only for AD based Kerberos)
____

a|
____
Select (Check)
____

|===

Check the picture below on where to populate the above mentioned fields:

image:image126.png[image126,width=634,height=218]

[arabic, start=32]
. In this setup, we used Kerberos authentication with *_RedHat FreeIPA_*.

image:image61.png[image61,width=628,height=276]

[arabic, start=9]
. On the Next Page, check the box for *_Manage “krb5.conf”_* to enable it through Cloudera Manager. This will install the krb5.conf file in all the hosts selected for the data lake.

image:image51.png[image51,width=624,height=203]

[arabic, start=10]
. Next, enter the details as per the configuration of FreeIPA you did before. i.e., provide the domain and password of the admin user configured earlier in the FreeIPA setup. Enter account credentials for the admin which you have created. This credential will be used to generate the keytabs. In our lab setup, “admin” user is created during the IPA server installation. Click Continue.

Enter the REALM portion of the principal in upper-case only to conform to Kerberos convention.

image:image66.png[image66,width=624,height=178]

[arabic, start=11]
. Click Finish to complete the KDC setup.
. KDC Account manager credentials should get imported successfully as shown below.

image:image201.png[A screenshot of a computer Description automatically generated,width=558,height=258]

##### For Red Hat IdM, make sure that all cluster hosts are joined to the IPA domain, after freeipa-client is installed.

## Kerberos client OS-specific packages must be installed on all cluster hosts and client hosts that will authenticate using Kerberos.

[root@ipaserver ~]# rpm -qa|grep ipa-client

ipa-client-4.6.8-5.el7.centos.16.x86_64

ipa-client-common-4.6.8-5.el7.centos.16.noarch

[root@ipaserver ~]#

##### If keytab error come up during kerberos configuration -- go to ipa server and run -

[root@ipaserver ~]# ipactl restart && ipactl status

[arabic, start=13]
. Once the KDC setup is completed, the Cloudera Manager wizard for adding a cluster will reflect the following:

image:image3.png[image3,width=657,height=438]

[arabic, start=14]
. Verify Kerberos configuration.

[root@ipaserver ~]# kinit admin@CLDRSETUP.LOCAL

Password for mailto:admin@cldrsetup.local[[.underline]#admin@CLDRSETUP.LOCAL#]: *_<vmware123>_

[root@ipaserver ~]# klist -e

Ticket cache: KCM:0

Default principal: admin@CLDRSETUP.LOCAL

Valid starting Expires Service principal

03/05/2024 20:35:11 03/06/2024 20:35:07 krbtgt/CLDRSETUP.LOCAL@CLDRSETUP.LOCAL

renew until 03/12/2024 21:35:07

[root@ipaserver ~]#

[arabic, start=15]
. Setup the *_Cloudera Management Services_*: *(Only if the status of hosts/services/charts are not visible / visible as (?) / or showing the errors on console)

##### Setup the *_Cloudera Management Services_* (need rman DB details), it will start service monitor and other services and enable charts view. If *_Cloudera Management Services_* are not installed/ enabled or not working properly, status of hosts, or installed services will not be updated on CM-UI.

[loweralpha]
. *Go to —> WebUI -> Top Right Corner -> (+)Add -> Add Cloudera Management Service

image:image35.png[image35,width=585,height=317]

[loweralpha, start=2]
. Assign Roles for different Cloudera Management Services to CLDR-MNGR Host (e.g. cldr-mngr.cldrsetup.local)

image:image103.png[image103,width=582,height=389]

[loweralpha, start=3]
. Setup Report Manager Database integration by providing the *_DBHostname_*, *_DBNAME_*, *_DBUser_* and *_DBPassword_*. After entering the details, click on *_Test Connection._* After the successful connection test, click *_Continue._

image:image67.png[image67,width=591,height=396]

[loweralpha, start=4]
. *_Summary page_* will come up. Click on *_Finish_*.

image:image179.png[image179,width=582,height=390]

[loweralpha, start=5]
. Now, Cloudera Management Service is visible as installed and status of different components is also visible.

image:image82.png[image82,width=621,height=351]

++**************************************************************************************************************++

=== Configure Cloudera Manager for external authentication using LDAP (LDAP integration):

An LDAP-compliant identity/directory service, such as OpenLDAP/FreeIPA, provides different options for enabling Cloudera Manager to look-up user accounts and groups in the directory:

* Use a single Distinguished Name (DN) as a base for matching usernames in the directory, or
* Search filter options let you search for a particular user based on somewhat broader search criteria – for example Cloudera Manager users could be members of different groups or organizational units (OUs), so a single pattern does not find all those users. Search filter options also let you find all the groups to which a user belongs, to help determine if that user should have login or admin access.

[arabic, start=33]
. The *_LDAP Distinguished Name Pattern_* property is deprecated. Leave this field empty while configuring authentication using LDAP in Cloudera Manager.

[arabic]
. Login to the Cloudera Manager (CM-UI) admin console.
. From Cloudera Manager, navigate to Administration→Settings.

image:image50.png[admin settings,width=351,height=212]

[arabic, start=3]
. In the filters section, click on External Authentication as shown in the screenshot below:

image:image83.png[ldap ext auth,width=264,height=350]

[arabic, start=4]
. Below page will get open, select the appropriate options as mentioned below in the screenshot:

image:image174.png[image174,width=624,height=396]

image:image108.png[image108,width=624,height=217]

[arabic, start=5]
. Search for “ldap” and enter values for ldap authentication, as mentioned in the below table.

[arabic, start=7]
. *LDAP Integration

[width="100%",cols="43%,57%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Authentication Backend Order:
____

a|
____
Database then EXTERNAL
____

a|
____
Authorization Backend Order:
____

a|
____
Database and EXTERNAL
____

a|
____
External Authentication Type:
____

a|
____
LDAP
____

a|
____
LDAP URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/
____

a|
____
LDAP Bind User Distinguished Name:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Bind Password:
____

a|
____
*_<vmware123>_* (password for KDC admin, configured earlier)
____

a|
____
Active Directory Domain: *(For AD Based LDAP)
____

a|
____
<AD DOMAIN>
____

a|
____
LDAP User Search filter:
____

a|
____
(&(uid={0})(objectClass=person))
____

a|
____
LDAP User Search Base:
____

a|
____
cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Group Search filter:
____

a|
____
(&(member={1})(objectClass=posixgroup))
____

a|
____
LDAP Group Search Base:
____

a|
____
cn=groups,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP DistName Pattern:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

|===

[arabic, start=6]
. Click *_Save_*. Once you click on the Save button, it will tell you to restart the CM Server, in order to bring the changes in effect. When you restart the CM Server from Backend. You will see the below entries in the logs.

[root@cldr-mngr ~]# systemctl restart cloudera-scm-server

[root@cldr-mngr ~]# systemctl status cloudera-scm-server -l

##### Run the below command to check the logs of cloudera-scm-server starting up. Wait until you see the *_Started Jetty server_* message on the screen.

[root@cldr-mngr ~]# sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log

2024-08-21 03:14:10,007 INFO WebServerImpl:com.cloudera.server.cmf.ExternalAuthenticationHelper: Using LDAP authentication with properties: DN pattern=(uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local) user search base=(cn=users,cn=accounts,dc=cldrsetup,dc=local) user search filter=((&(uid={0})(objectClass=person))) group search base=(cn=groups,cn=accounts,dc=cldrsetup,dc=local) group search filter=((&(member={1})(objectClass=posixgroup)))

2024-08-21 03:14:10,009 INFO WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Authenticating against database, then LDAP

[arabic, start=7]
. Login again to CM-UI and in *_Administration > Users & Roles > LDAP/PAM Groups_*, add LDAP/PAM Group mapping.

image:image22.png[A screenshot of a computer Description automatically generated,width=642,height=173]

[arabic, start=8]
. *_Add LDAP/PAM Group mapping_* value and Roles to assign. (*_LDAP/PAM Group: admin, Roles: Full Administrator_*)

image:image191.png[image191,width=482,height=186]

[arabic, start=9]
. Click on *_Test LDAP Connectivity._* Provide a username and password for an LDAP user to test whether that user can be authenticated. *_(username: admin, password: <vmware123>)_

image:image113.png[image113,width=490,height=160]

[arabic, start=10]
. Click on *_Test_* to verify LDAP configuration is set up and working fine, as expected.

image:image190.png[image190,width=679,height=108]

[arabic, start=11]
. Login to the cldr-mngr server at backend and restart the Cloudera Manager Server.

[root@cldr-mngr ~]# *systemctl restart cloudera-scm-server

[root@cldr-mngr ~]# *systemctl status cloudera-scm-server -l

[root@cldr-mngr ~]# *sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log

2024-03-19 00:56:15,620 INFO pool-7-thread-1:com.cloudera.server.cmf.components.CmServerStateSynchronizer: (30 skipped) Synced up

2024-03-19 00:46:49,935 INFO LDAP login monitor thread.: org.springframework.security.ldap.DefaultSpringSecurityContextSource: URL 'ldap://ipaserver.cldrsetup.local:389/cn=users,cn=accounts,dc=cldrsetup,dc=local', root DN is 'cn=users,cn=accounts,dc=cldrsetup,dc=local'

2024-03-19 00:56:13,528 INFO LDAP login monitor thread.: org.springframework.ldap.core.support.AbstractContextSource: Property 'password' not set - blank password will be used

2024-03-19 00:56:14,269 INFO CommandPusher-1:com.cloudera.server.cmf.CommandPusherThread: Acquired lease lock on DbCommand:1546344098

2024-03-19 00:56:14,620 INFO pool-7-thread-1:com.cloudera.server.cmf.components.CmServerStateSynchronizer: (30 skipped) Cleaned up

[root@cldr-mngr ~]#

[arabic, start=12]
. Login to Cloudera Manager WebUI and assign Roles for new users. *_(Only, If used a different user than admin, else skip this step)_

image:image202.png[image202,width=667,height=142]

++**************************************************************************************************************++

=== [.underline]#Setup Private Cloud (PvC) Base Cluster#

In this step, we will setup the Base cluster which will serve as the DataLake for the CDP Data Services that need the SDK capabilities for the cluster wide features like lineage, governance, security etc..,

[arabic, start=9]
. Install Cloudera Private Cloud Base using the Cloudera Manager WebUI

[arabic]
. In Cloudera Manager, on the top right corner, click *_(+) Add > Add Cluster_*. The Select Cluster Type page appears.

image:image74.png[image74,width=316,height=149]

[arabic, start=2]
. On the *_Select Cluster Type_* page, select the cluster type as *_Private Cloud Base Cluster_* (first option) and enter a cluster name. Click *_Continue_*.

image:image81.png[image81,width=684,height=456]

image:image140.png[image140,width=692,height=463]

[arabic, start=3]
. Enter the cluster host names or IP addresses in the Hostnames field. You can Provide host pattern pvcbase-master, pvcbase-worker[1-5] or pvcbase-worker[1-5].cldrsetup.local etc separated with a new line and Click on *_Search_*.
* *Note*: Host names must be in lowercase. If you use uppercase letters in any host name, the cluster services will not start after enabling Kerberos.
. Specify the hosts that are part of the cluster using their IP addresses or hostname. The figure below shows a pattern that specifies the IP addresses range. Cloudera Manager will "discover" the hosts based on matching the pattern provided by you to add in the cluster. Verify that all desired nodes have been found and *_selected for installation_*. Verify host entries, *_deselect_* any that you do not want to install services on, and click *_Continue_*.

pvcbase-master.cldrsetup.local

pvcbase-worker[1-5].cldrsetup.local

image:image105.png[image105,width=700,height=385]

[arabic, start=5]
. The Select Repository section appears. Select Cloudera Repository option as mentioned. Enter *_Custom Repository_* or Cloudera Repository to install Cloudera Manager Agent on all nodes in the cluster. We have earlier configured the private yum repository on *_cldr-mngr_* node. Please provide the path here:

i.e. http://13.251.65.11/cloudera-repos/cloudera-manager/cm7.11.3/[[.underline]#http://13.251.65.11/cloudera-repos/cloudera-manager/#] and click on *_Continue_*.

image:image208.png[image208,width=685,height=180]

[arabic, start=6]
. In the other software section, select *_Use Parcels (Recommended)_* and click *_Parcel Repository & Network Settings_* to provide a custom Parcels location to be installed *_(in a new tab in the same browser window)_*.

image:image10.png[image10,width=433,height=153]

[arabic, start=7]
. Enter custom repository URL for CDH7 and CDS 3.3 parcels. Click on *_Save and Verify Configuration_*. Close the Parcel Repository & Network Settings wizard.

i.e. *http://13.251.65.11/cloudera-repos/spark3/3.3.7190.4/[[.underline]#http://13.251.65.11/cloudera-repos/spark3/3.3.7191000.4/#]

http://13.251.65.11/cloudera-repos/cdh7.1.9/[[.underline]#http://13.251.65.11/cloudera-repos/cdh7.1.9/#]

image:image80.png[image80,width=626,height=312]

[arabic, start=8]
. Select the parcels for installation.

image:image18.png[image18,width=616,height=421]

[arabic, start=9]
. Click *_Continue_*.
. Select the appropriate option for JDK. (manual installation for JDK11 with CDH 7.1.x and JDK17 with 7.1.9 and above): (Select *_Manually manage JDK_* here, as we have already installed a System-provided version of OpenJDK11 manually on all servers. Click on *_Continue_*.

image:image173.png[A screenshot of a computer Description automatically generated,width=604,height=454]

[arabic, start=11]
. The *_Enter Login Credentials section_* appears. On this page, provide the required *_details_* as mentioned *_in the below table_*, for the hosts to install Cloudera packages. Click *_Continue_*.

[width="100%",cols="44%,56%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
SSH Username
____

a|
____
root
____

a|
____
Authentication Method
____

|All hosts accept same password / All hosts accept same private key
a|
____
Password (If selected password based auth)
____

a|
____
<password_for_vm_root_user> *_(e.g. vmware123)_
____

a|
____
Confirm Password (If selected password based auth)
____

a|
____
<password_for_vm_root_user> *_(e.g. vmware123) (again)_
____

a|
____
*Private Key (If selected private key based auth)
____

a|
____
Upload the private key e.g. *_id_rsa_* generated in earlier steps
____

a|
____
Passphrase (If selected private key based auth)
____

a|
____
If Applicable
____

a|
____
Repeat Passphrase (If selected private key based auth)
____

a|
____
If Applicable
____

|===

image:image153.png[A screenshot of a computer Description automatically generated,width=572,height=441]

[loweralpha]
. If Selected the *_Authentication method_* as *_All hosts accept same password_

image:image122.png[image122,width=566,height=358]

[loweralpha, start=2]
. If Selected the *_Authentication method_* as *_All hosts accept the same private key_*.

[arabic, start=12]
. The *_Install Agents_* section appears showing the progress of the installation. It will check for and install the *_JDK_* (If not already there) and *_cloudera-scm-agent_* software on all the Base Cluster nodes. Click *_Continue_* after the Cloudera Agent *_Installation completed successfully_* on all hosts.

image:image4.png[image4,width=617,height=336]

[arabic, start=13]
. Stop at this stage. Create a directory on base cluster nodes (for handling of a bug associated with p1000)

[root@cldr-mngr ~]# *ansible namenodes,datanodes -m shell -a "mkdir /var/lib/hadoop-hdfs"

[root@cldr-mngr ~]# *ansible namenodes,datanodes -m shell -a "ls -lart /var/lib/hadoop-hdfs"

[arabic, start=14]
. After the agents are installed, the *_Install Parcels_* section appears showing the progress of the parcels distribution, activation and installation on all hosts part of the cluster creation. Once the parcels are installed successfully for all hosts, click on *_Continue_*.

image:image17.png[image17,width=611,height=407]

[arabic, start=15]
. Stop at this stage. On base cluster nodes (for handling of a bug associated with p1000). Once Parcels are activated, follow below steps:

[root@cldr-mngr ~]# *ansible namenodes,datanodes -m shell -a "chown hdfs:hadoop /var/lib/hadoop-hdfs"

[root@cldr-mngr ~]# *ansible namenodes,datanodes -m shell -a "ls -lart /var/lib/hadoop-hdfs"

##### Verify if links are good for the CM version for hadoop-hdfs filesystem JAR.

[root@cldr-mngr ~]# *ansible namenodes,datanodes -m shell -a "namei -om /var/lib/hadoop-hdfs/ozone-filesystem-hadoop3.jar"

*If links are not working fine, deactivate and activate the parcel from CM Parcel Manager. Then Perform Below steps:

[root@cldr-mngr ~]# *ansible namenodes,datanodes -m shell -a "unlink /etc/alternatives/ozone-filesystem-hadoop3.jar; unlink /var/lib/hadoop-hdfs/ozone-filesystem-hadoop3.jar"

[Notes]

*Here are the notes from product/engineering team on the issue:

*Verify if /var/lib/hadoop-hdfs exists on all nodes.

*Check if hdfs user exists on all nodes. (grep hdfs /etc/passwd; grep hdfs /etc/group)

*Create the /var/lib/hadoop-hdfs directory on all nodes.

*Deactivate and activate the Cloudera Runtime parcel.

*Alternatively, if there is a separate Ozone parcel installed on the cluster, deactivate and activate the Ozone parcel instead.

*Ensure that the directory has at least the following permissions:

*/var/lib/hadoop-hdfs/ozone-filesystem-hadoop3.jar

*lrwxrwxrwx [owner: root, group: root]

*/var/lib/hadoop-hdfs

*drwxr-xr-x [owner: hdfs, group: hadoop]

*Output for permissions should look similar to:

*$ namei -om /var/lib/hadoop-hdfs/ozone-filesystem-hadoop3.jar

*f: /var/lib/hadoop-hdfs/ozone-filesystem-hadoop3.jar

*drwxr-xr-x root root /

*drwxr-xr-x root root var

*drwxr-xr-x root root lib

*drwxr-xr-x hdfs hadoop hadoop-hdfs

*lrwxrwxrwx root root ozone-filesystem-hadoop3.jar -> /etc/alternatives/ozone-filesystem-hadoop3.jar

*The issue can also be found in the Ozone Known issues documentation.

*Create the alternatives link manually for CDP binaries.

*This step would have failed during the activation of the CDP parcel due to the missing folder.

*$ alternatives --install /var/lib/hadoop-hdfs/ozone-filesystem-hadoop3.jar ozone-filesystem-hadoop3.jar /opt/cloudera/parcels/CDH-<path_to_active_parcel>/lib/hadoop-ozone/ozone-filesystem-hadoop3.jar 5

*If an Ozone parcel is installed, create the symbolic links for the Ozone parcel.

*$ alternatives --install /var/lib/hadoop-hdfs/ozone-filesystem-hadoop3.jar ozone-filesystem-hadoop3.jar /opt/cloudera/parcels/OZONE-<path_to_activated Ozone_parcel>/lib/hadoop-ozone/ozone-filesystem-hadoop3.jar 10

*Upgrade:

*Upgrade to a fixed release of CDP/Cloudera Manager once available.

*For the latest update on this issue see the corresponding Knowledge Article:

*TSB 2024-775: FileNotFoundException for the Ozone FS JAR during or after installation or upgrade

[arabic, start=16]
. After the parcels are installed the Inspect Cluster section appears.
* Inspect Cluster by running *_Inspect Network Performance_*. After the network inspector completes, click *_Show Inspector Results_* to view the results in a new tab. Address any reported issues (If there), and click *_Run Again_*.
+
*_Click Inspect Hosts_*. After the host inspector completes, click *_Show Inspector Results_* to view the results in a new tab. Address any reported issues (If there), and click *_Run Again_*.
+
Both the inspection tests should run successfully. Review inspector summary. Click *_Finish_*.

image:image188.png[A screenshot of a computer Description automatically generated,width=619,height=379]

*If java path mismatch error occurs in network/host inspection:

*Take - Java Path from the host ( /usr/lib/jvm/java-17-openjdk-17.0.13.0.11-4.el9.x86_64/ ), goto CM,

*open a new tab of CM-UI on browser and search for JAVA PATH in search bar present at left hand side:

# Override JAVA PATH in CM-UI> Search JAVA PATH> Override the value> Save Changes> Restart Cloudera-SCM-Server

*Re-run the inspect tools, this time all checks should be green.

[arabic, start=17]
. Click *_Continue_*.

=== [.underline]#Private Cloud Base Cluster (Data Lake/Control Plane) Creation#

[arabic, start=18]
. After the Private Cloud Base Cluster (runtime) setup is complete, if successful, it will automatically move to add services *_(Add Cluster -Configuration)_* wizard. It will ask to *_Select Services_* i.e. (a) Data Engineering, (b) Data Warehouse/ Data Mart, © Operational Database specific or (d) Custom control plane/base cluster services i.e. HDFS, YARN, Hive, Tez, HiveOnTez, Ozone, Zookeeper, Kafka, SOLR, Ranger, Atlas, HBase, Phoenix, Impala, HUE, Spark2, Spark3, YARN Queue Manager etc. Choose from a combination or services defined or select custom services. Services required based on selection will be automatically added.
. Select *_Custom Services_* option to install.

image:image34.png[A screenshot of a computer Description automatically generated,width=603,height=503]

[arabic, start=34]
. It is important to select host(s) to deploy services based on the role intended for it. For detailed information, go to: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-runtime-cluster-hosts-role-assignments.html[[.underline]#Runtime Cluster Hosts and Role Assignments#]

[arabic, start=20]
. Under the *_Custom Services_*, select the below custom Cloudera DataLake/Control Plane management services to be installed on the cluster. The Selection would look similar to below, Select services and Click *_Continue_*.

image:image97.png[A screenshot of a computer Description automatically generated,width=697,height=506]

image:image77.png[image77,width=705,height=205]

image:image196.png[image196,width=702,height=104]

image:image104.png[image104,width=691,height=519]

[arabic, start=21]
. Select host assignment for different services in the *_Add cluster - configuration_* wizard. You need to assign hosts to different roles across all the selected services. Use the below table as a reference to assign the roles.

*Table:* *Cloudera Data Platform Private Cloud Base host and Role assignment example

[width="100%",cols="32%,68%",options="header",]
|===
a|
____
*Service Name (Role Instance)
____

a|
____
*Host
____

a|
____
HDFS
____

a|
____
*NameNode* : pvcbase-master

*SecondaryNameNode* : pvcbase-master

*Balancer* : pvcbase-master

*DataNode* : pvcbase-worker[1-5]
____

a|
____
YARN
____

a|
____
*ResourceManager* : pvcbase-master

*NodeManager* : pvcbase-worker[1-5] *(Same as DataNode)

*JobHistoryServer* : pvcbase-master
____

a|
____
Core Configuration
____

a|
____
*Gateway* : pvcbase-master
____

a|
____
Iceberg Replication
____

a|
____
*Gateway* : pvcbase-master
____

a|
____
Knox
____

a|
____
*Gateway* : pvcbase-master
____

a|
____
[line-through]#Cloudera Management Service +
#

*This is already configured in

*previous steps
____

a|
____
[line-through]#Service Monitor :* cldr-mngr#

[line-through]#Host Monitor :* cldr-mngr#

[line-through]#Reports Manager :* cldr-mngr#

[line-through]#Event Server :* cldr-mngr#

[line-through]#Alert Publisher :* cldr-mngr#
____

a|
____
Spark2/Spark3
____

a|
____
*Spark History Server* : pvcbase-master

*Spark Gateway* : pvcbase-worker[1-5]
____

a|
____
Hive
____

a|
____
*Hive Metastore Server (HMS)* : pvcbase-master

*Gateway* : pvcbase-worker[1-5]
____

a|
____
Tez
____

a|
____
*Gateway* : pvcbase-worker[1-5]
____

a|
____
Hive on Tez
____

a|
____
*HiveServer2* : pvcbase-master

*Gateway* : pvcbase-worker[1-5]
____

a|
____
Impala
____

a|
____
*Impala Catalog Server* : pvcbase-master

*Impala State Store* : pvcbase-master

*Impala Daemon* : pvcbase-worker[1-5] *(Same as DataNode)
____

a|
____
HUE
____

a|
____
*HUE Server* : pvcbase-master

*LoadBalancer* : pvcbase-master
____

a|
____
HBase
____

a|
____
*HBase Master* : pvcbase-master

*RegionServer* : pvcbase-worker[1-5] *(Same as DataNode)
____

a|
____
Phoenix
____

a|
____
*Query Server* : pvcbase-master
____

a|
____
Ozone
____

a|
____
*Storage Container Manager* : pvcbase-master

*Ozone Manager* : pvcbase-master

*Ozone Recon* : pvcbase-master

*S3 Gateway* : pvcbase-master

*Gateway* : pvcbase-worker[1-5]

*OzoneDataNode* : pvcbase-worker[1-5] *(Same as DataNode)
____

a|
____
CDP-INFRA-SOLR
____

a|
____
*Solr Server* : pvcbase-master

(can be installed on all hosts if needed if there is a search use case)
____

a|
____
Kafka
____

a|
____
*Kafka Broker* : base-master, pvcbaseworker[1-5] *(Same as DataNode)
____

a|
____
ZooKeeper
____

a|
____
*Zookeeper Server* : pvcbase-master
____

a|
____
Ranger
____

a|
____
*Ranger Admin* : pvcbase-master

*UserSync* : pvcbase-master

*Ranger Tagsync* : pvcbase-master
____

a|
____
Atlas
____

a|
____
*Atlas Server* : pvcbase-master
____

a|
____
Oozie Server (Optional)
____

a|
____
pvcbase-master
____

a|
____
YARN Queue Manager (Optional)
____

|
|===

[arabic, start=22]
. Assign roles as updated above and shown as below.

image:image2.png[image2,width=539,height=337]

image:image169.png[image169,width=539,height=377]

image:image39.png[image39,width=555,height=392]image:image167.png[image167,width=572,height=429]

[arabic, start=23]
. Click *_Continue_*. When you’re doing the set-up for the first time on this CM Sever. *_Cloudera Management Services_* will be installed only once along with other control plane services.
. On the *_Setup Databases_* page, Select *_database type_* as *_Use Custom Database_*. Provide database hostname, username, and password (created in earlier steps) for different services and click on *_Test Connection_*. After a successful connection test, click *_Continue_* to install, configure and start services sequentially.

* Reports Manager (For Cloudera Management Service)
* Oozie Server (If selected to install, as optional)
* Ranger
* Hive
* YARN Queue Manager
* Hue

image:image21.png[image21,width=624,height=392]

[arabic, start=25]
. Next, the *_Enter Required Parameters_* page appears.
. Wait at the parameter screen, enter the required parameters. For all the remaining parameters, set a common password so that it becomes easier while using those services. This password must have 1 lowercase, 1 uppercase, and 1 numeric value. Failing to adhere to this, the final step in the cluster setup would fail. Please see the required inputs below:

* _Knox Master Secret: *VMware@123*_
* _Knox IDBroker Master Secret: *VMware@123*_
* _Enter a suitable name for Ozone Service ID. i.e.: *ozone11*_
* Ranger Admin: *_VMware@123_
* Ranger Usersync: *_VMware@123_
* Ranger Tagsync: *_VMware@123_
* Ranger KMS Keyadmin: *_VMware@123_

image:image199.png[image199,width=624,height=357]

[arabic, start=27]
. Click *_Continue_*.
. The *_Review Changes_* page appears. Set a *_password_* for *_Atlas_*. You can set it to the same value set for Ranger above.

* Atlas Admin password: *_VMware@123_

image:image128.png[atlas passwd,width=624,height=58]

[arabic, start=29]
. Scroll down and verify/update the HDFS disks configuration according to the below.

* DataNode Data Directory: *_/hdfs/dfs/dn_
* NameNode Data Directories: *_/hdfs/dfs/nn_
* HDFS Checkpoint Directories: *_/hdfs/dfs/snn_

image:image98.png[image98,width=605,height=155]

[arabic, start=30]
. Review the changes for all the services on the *_Review Changes_* page and verify/edit the configuration parameters as per your requirements. Click *_Continue_*.
. A few sets of commands are running in the background. Wait for them to get executed successfully. Once done, Click *_Continue_*.
. Configure Kerberos and Keep Review and customize the configuration changes based on your requirements. Check the box for *_Enable Kerberos for this cluster_*. *Required libraries i.e. krb5-workstation, krb5-libs and freeipa-client are already installed on all servers in prior steps.

image:image70.png[A screenshot of a computer Description automatically generated,width=583,height=563]

[arabic, start=33]
. Click *_Continue_* after Cloudera Manager successfully runs the *_Enable Kerberos_* command.

image:image75.png[image75,width=704,height=298]

[arabic, start=34]
. Installation wizard will run the first command to start cluster roles and services. Click *_Continue_*.

image:image96.png[image96,width=693,height=265]

[arabic, start=35]
. If you still face any issue while making the services up or during the installation or start of any Control Plane services please refer to troubleshooting PvC Base Cluster part at the end of this document. Though, some of the major issues during installation, their cause and their resolution is listed as below:

*Zookeeper SASL error:

**Solution:**resolved by regenerate key tab

++==========================================================================================================++

*Kafka error:

*Solution:

Update clusterid and broker id from Role logs in meta.properties

rm -vf /var/local/kafka/data/meta.properties

rm -vf /tmp/kafka-logs/

https://gautambangalore.medium.com/resolved-error-fatal-error-during-kafkaserver-startup-37f638c2c00c[[.underline]#https://gautambangalore.medium.com/resolved-error-fatal-error-during-kafkaserver-startup-37f638c2c00c#]

ansible datanodes -m shell -a "sed -i 's#aHRUaTqoQzOpGf8qA5bXlQ#rxsV4DwNRvWtIcl5ejG-IA#g'

++==========================================================================================================++

*Service NodeManager failed in state INITED

org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed NodeManager login
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:511)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:974)
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:1054)
Caused by: org.apache.hadoop.security.KerberosAuthException: failure to login: for principal: yarn/pvcbase-worker3.cldrsetup.local@CLDRSETUP.LOCAL from keytab yarn.keytab javax.security.auth.login.LoginException: Client not found in Kerberos database (6) - CLIENT_NOT_FOUND

*Solution:
Regenerate kerberos creds from administration>security for yarn

++==========================================================================================================++

*YARN issue

*Solution:

https://community.cloudera.com/t5/Support-Questions/Error-CM-Server-guid-updated-CDH-5-9-0/m-p/47221[[.underline]#https://community.cloudera.com/t5/Support-Questions/Error-CM-Server-guid-updated-CDH-5-9-0/m-p/47221#]

https://community.cloudera.com/t5/Support-Questions/CDH-6-1-Installation-Issues-Unable-to-obtain-CM-release/td-p/88238[[.underline]#https://community.cloudera.com/t5/Support-Questions/CDH-6-1-Installation-Issues-Unable-to-obtain-CM-release/td-p/88238#]

Fixed it by deleting /var/lib/cloudera-scm-agent/cm_guid on each node.

++==========================================================================================================++

*Chrony issue on during ipaserver/ipaclient installation

*Solution:
Stop chrnoyd and remove chrony from all hosts, then install ipa-server and then ipa-client. It will work.

++==========================================================================================================++

*Rman db error

Exception while executing ddl scripts.
org.postgresql.util.PSQLException: ERROR: relation "rman_usergrouphistory_seq" already exists
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)

*Solution:
drop and recreate db solved issues.

++==========================================================================================================++

*Ozone Error:
Found SOLR_SERVICE: ''

*solution:
ozone install error
include solr in service dependency and restart services

++==========================================================================================================++

*Issue: IPASERVER failed to resolve DNS ipaservices not working

*Solution:
Port 53 was not open on AWS SecGrp: ipaserver was not working on aws due to it,
updated secgrp added 53 rule for dns fixed issue.

++==========================================================================================================++

*Ranger Error:
Repo cm_kafka already exists ->

*Solution:
delete cm_kafka from ranger

++==========================================================================================================++

*CM Not able to login

2024-05-14 04:41:02,375 INFO CommandPusher-1:com.cloudera.server.cmf.CommandPusherThread: Acquired lease lock on DbCommand:1546336335

2024-05-14 04:41:02,378 INFO CommandPusher-1:com.cloudera.cmf.service.AbstractOneOffHostCommand: Unsuccessful 'RepMgrTestDatabaseConnection'

2024-05-14 04:41:02,379 INFO CommandPusher-1:com.cloudera.cmf.service.AbstractDbConnectionTestCommand: Command exited with code: 1

2024-05-14 04:41:02,379 INFO CommandPusher-1:com.cloudera.cmf.service.AbstractDbConnectionTestCommand: + MGMT_JAVA_OPTS='-Djava.net.preferIPv4Stack=true '

+ exec /usr/lib/jvm/java-17-openjdk-17.0.11.0.9-2.el9.x86_64/bin/java -Djava.net.preferIPv4Stack=true -Djava.security.egd=file:///dev/urandom -cp '/run/cloudera-scm-agent/process/1546336334-MGMT.REPORTSMANAGER-test-db-connection:/usr/share/java/mysql-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/opt/cloudera/cm/lib/*' com.cloudera.enterprise.dbutil.DbCommandExecutor db.properties

Exception in thread "main" java.lang.NoClassDefFoundError: com/ongres/scram/common/stringprep/StringPreparation
at org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:759)
at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:161)
at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:213)
at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:51)
at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:225)
at org.postgresql.Driver.makeConnection(Driver.java:465)
at org.postgresql.Driver.connect(Driver.java:264)
at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)
at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:229)
at com.cloudera.enterprise.dbutil.DbCommandExecutor.testDbConnection(DbCommandExecutor.java:265)
at com.cloudera.enterprise.dbutil.DbCommandExecutor.main(DbCommandExecutor.java:140)
Caused by: java.lang.ClassNotFoundException: com.ongres.scram.common.stringprep.StringPreparation
at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
... 11 more

2024-05-14 04:41:02,379 ERROR CommandPusher-1:com.cloudera.cmf.model.DbCommand: Command 1546336335(RepMgrTestDatabaseConnection) has completed. finalstate:FINISHED, success:false, msg:Unexpected error. Unable to verify database connection.
Caused by: java.lang.ClassNotFoundException: com.ongres.scram.common.stringprep.StringPreparation
at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
... 11 more

2024-05-14 04:50:38,519 ERROR CommandPusher-1:com.cloudera.cmf.model.DbCommand: Command 1546336734(OozieTestDatabaseConnection) has completed. finalstate:FINISHED, success:false, msg:Unexpected error. Unable to verify database connection.

2024-05-14 04:50:38,519 INFO CommandPusher-1:com.cloudera.cmf.command.components.CommandStorage: Invoked delete temp files for command:DbCommand{id=1546336734, name=OozieTestDatabaseConnection, host=pvcbase-master.cldrsetup.local} at dir:/var/lib/cloudera-scm-server/temp/commands/1546336734

*Solution:

Caused after change in hostssl parameter for Postgres (suspected)

Delete scm db and recreated db and restart scm server fixed the issue. This lead to reinstall entire base and ecs clusters as metadata deleted from scm db

++==========================================================================================================++

*Other:

*Configure Ozone with other data services before env creation, else it will lead to CDE installation error
*Configure thrift server role in hbase for hue
*Knox and Atlas works with local Linux Users and Password credentials i.e. PAM

*For accessing the WebUI and fixing issues for web based authentication is not working for some of the services including Knox, Atlas, HDFS (Namenode UI), Yarn (History server UI) etc.Disable Kerberos Authentication for WebUI under each service configuration section which are showing 403 or 401 error.

*In case of Cleanup and re-installation (end-to-end), make sure cleanup steps are performed properly and no control plane services left user and groups created in /etc/passwd and /etc/group on all nodes of the cluster include cldr-mngr.

[arabic, start=36]
. Next, all the Cloudera services would get started and their prerequisite operations would also be run. These processes will run in a combination of serial and parallel processes. Wait for them to complete. Once completed, you will see a Green tick next to all the steps, as shown in the screenshot above.
. Once completed above step, click *_Continue_*. You will see a summary page like below.

image:image193.png[A screenshot of a computer Description automatically generated,width=504,height=367]

[arabic, start=38]
. Click *_Finish_* on the Summary page.
. This will navigate to the main page where you can see all the services installed.

image:image125.png[image125,width=666,height=379]

[arabic, start=40]
. All the services should be in Healthy state. If there are any instances in Bad Health, troubleshoot the same and fix it.
. This completes the *_CDP Private Cloud Base cluster_* setup.

[arabic, start=35]
. You might need to adjust configuration parameters of the cluster after successful first run command execution. Apply the changes and restart the cluster.

[arabic, start=42]
. We will perform the adjustment of the configuration parameters of the cluster to fix some of the issues with the services on the base cluster, in the next steps below.
. Optionally, we can also perform the prerequisites and compatibility tests for Hardware using the script provided below by Cloudera.

https://github.com/cloudera-labs/toolkits/tree/main/data_services-toolkit/DS_Pre-Install_Check[[.underline]#https://github.com/cloudera-labs/toolkits/tree/main/data_services-toolkit/DS_Pre-Install_Check#]

++**************************************************************************************************************++

=== Additional requirements and details for Private Cloud Base Cluster services:

*Note: Common Data Lake Services’ URLs:

[.underline]#CM-UI:#

*HTTP:* http://cldr-mngr.cldrsetup.local:7180/cmf/

*HTTPS:* https://cldr-mngr.cldrsetup.local:7183/cmf/

[.underline]#HDFS:#

*HDFS-NAMENODE UI:* https://pvcbase-master.cldrsetup.local:9871/dfshealth.html[[.underline]#https://pvcbase-master.cldrsetup.local:9871/dfshealth.html#]

[.underline]#YARN:#

*HDFS-YARN JobHistory UI:* https://pvcbase-master.cldrsetup.local:19890/jobhistory[[.underline]#https://pvcbase-master.cldrsetup.local:19890/jobhistory#]

*YARN RM UI:* https://pvcbase-master.cldrsetup.local:8090/ui2/#/cluster-overview[[.underline]#https://pvcbase-master.cldrsetup.local:8090/ui2/#/cluster-overview#]

{empty}[.underline]#Ranger*:# https://pvcbase-master.cldrsetup.local:6182/index.html#/policymanager/resource[[.underline]#https://pvcbase-master.cldrsetup.local:6182/index.html#/policymanager/resource#]

{empty}[.underline]#Atlas*:# https://pvcbase-master.cldrsetup.local:31443/login.jsp[[.underline]#https://pvcbase-master.cldrsetup.local:31443/login.jsp#]

*_(Login works with PAM-Linux Server Local User Credentials)_

{empty}[.underline]#Knox*:# https://pvcbase-master.cldrsetup.local:8443/gateway/knoxsso/knoxauth/login.html?originalUrl=https://pvcbase-master.cldrsetup.local:8443/gateway/homepage/home/?profile=token[[.underline]#https://pvcbase-master.cldrsetup.local:8443/gateway/knoxsso/knoxauth/login.html?originalUrl=https://pvcbase-master.cldrsetup.local:8443/gateway/homepage/home/?profile=token#]

*_(Login works with PAM-Linux Server Local User Credentials)_

{empty}[.underline]#HiveServer2 UI*:# https://pvcbase-master.cldrsetup.local:10002/[[.underline]#https://pvcbase-master.cldrsetup.local:10002/#]

[.underline]#HUE#: https://pvcbase-master.cldrsetup.local:8889/hue/editor/?type=hive[[.underline]#https://pvcbase-master.cldrsetup.local:8889/hue/editor/?type=hive#]

[.underline]#HBASE#: https://pvcbase-master.cldrsetup.local:16010/master-status[[.underline]#https://pvcbase-master.cldrsetup.local:16010/master-status#]

[.underline]#Ozone#:

*Ozone Recon:* https://pvcbase-master.cldrsetup.local:9889/#/Overview[[.underline]#https://pvcbase-master.cldrsetup.local:9889/#/Overview#]

*Ozone SCM:* https://pvcbase-master.cldrsetup.local:9877/#!/[[.underline]#https://pvcbase-master.cldrsetup.local:9877/#!/#]

*Ozone Manager*: https://pvcbase-master.cldrsetup.local:9889/#/Overview[[.underline]#https://pvcbase-master.cldrsetup.local:#]

*S3 Gateway:* https://pvcbase-master.cldrsetup.local:9889/#/Overview[[.underline]#https://pvcbase-master.cldrsetup.local:#]

*Gateway:* https://pvcbase-master.cldrsetup.local:9889/#/Overview[[.underline]#https://pvcbase-workter1.cldrsetup.local:#]

*OzoneDataNode:* https://pvcbase-master.cldrsetup.local:9889/#/Overview[[.underline]#https://pvcbase-worker1.cldrsetup.local:#]

[.underline]#Spark JobHistory Server:#

*Spark2:* https://pvcbase-master.cldrsetup.local:18488/[[.underline]#https://pvcbase-master.cldrsetup.local:18488/#]

*Spark3:* https://pvcbase-master.cldrsetup.local:18489/[[.underline]#https://pvcbase-master.cldrsetup.local:18489/#]

[.underline]#Impala*:#

*Impala Catalog:* https://pvcbase-master.cldrsetup.local:25020/[[.underline]#https://pvcbase-master.cldrsetup.local:25020/#]

*Impala Statestore:* https://pvcbase-master.cldrsetup.local:25010/[[.underline]#https://pvcbase-master.cldrsetup.local:25010/#]

##### [.underline]#Job History Server:# https://pvcbase-master.cldrsetup.local:9991/[[.underline]#https://pvcbase-master.cldrsetup.local:9991/#]

[arabic]
. *Port requirements for different services on PvC Base Cluster/Data Services (ECS) Cluster:

Please whitelist the below ports or make sure , firewall is disabled in the internal network. (Not required in on-premise Private datacenter based network)

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-ports-used-by-runtime.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-ports-used-by-runtime.html#]

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-ports-third-party-components.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-ports-third-party-components.html#]

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-ports-used-by-cm.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-ports-used-by-cm.html#]

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-service-dependencies.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-service-dependencies.html#]

https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation-ecs/topics/cdppvc-installation-ecs-steps.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation-ecs/topics/cdppvc-installation-ecs-steps.html#]

[arabic, start=2]
. *Disable Kerberized Web-UI for YARN, Spark, HBase, HDFS, etc. from configuration:

While accessing Web UIs from web browsers, for HDFS-Namenode, YARN services, HiveServer2, Impala Services, etc. If gets 401 unauthorized error: We need to enable non-kerberized webUI for those services, for this we need to:

* Login to CM-UI> Go to individual services> Go to the Configuration section for individual services> Search for *_Kerberos_*.
* Disable the option, by unchecking the checkbox, for Kerberos authentication for WebUI. Save changes. Restart the Stale Services to update the backend configuration files.

*Example Screenshots for HDFS. You can follow the same for other services, which requires WebUI access.

image:image54.png[image54,width=689,height=392]

image:image110.png[image110,width=687,height=388]

*1. For Impala, Hive, Hive on Tez edit value for

Ranger Plugin URL Auth Filesystem Schemes - file:,wasb:,adl:

*2. Disable [line-through]#Enable# Kerberos Authentication for HTTP Web-Consoles -* HBase (Service-Wide), YARN, Spark2, Spark3, HiveServer2, Impala, HDFS, etc. [line-through]#Click on Generate missing credentials for Kerberos.#

*3. For TLS/SSL enabled HDFS configuration you might see a warning as _“_*_DataNode configuration is valid, but not recommended. There are two recommended configurations:_

_(1) DataNode Transceiver Port and Secure DataNode Web UI Port (TLS/SSL) both >= 1024, DataNode Data Transfer Protection set, Hadoop TLS/SSL enabled;_

_(2) DataNode Transceiver Port and DataNode HTTP Web UI Port both < 1024, DataNode Data Transfer Protection not set, Hadoop TLS/SSL disabled.*”*_

DataNode Transceiver Port (dfs_datanode_port)- 9866

DataNode HTTP Web UI Port (dfs.datanode.http.address) - 9864

DataNode Data Transfer Protection (dfs.data.transfer.protection) – Authentication

*4. Install Ranger Plugin for services, add service dependency* *i.e. Hive etc. and restart the cluster.

Ensure that the Ranger Solr and Ranger HDFS plugins are enabled. See https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-additional-steps-ranger.html[[.underline]#Additional Steps for Apache Ranger#] for more details on Configuration Steps.

*5. Make Sure HDFS, Ozone services are Installed and Running Successfully* (critical services – if not working properly, then no other service will work properly)

1.HDFS 2.Zookeeper 3. …….

*6. Atlas and Knox work with PAM authentication* i.e. Local (Non-LDAP) users created on the base-master node where your Atlas server is running, unless Atlas is explicitly configured (integrated) to use LDAP. So you may need to create a local user on the base-master node, if it does not already exist.

*7. Atlas is having dependencies on some additional services i.e. HBase, SOLR and Kafka

[arabic, start=3]
. Optionally, Update the /etc/hosts file on your working machine/JumpHost where you are trying to access your CM-UI to work with the URLs smoothly:

Open *_C:\Windows\System32\drivers\etc_* (on Windows) or *_/etc/hosts_* (on MAC/Linux), with sudo privileges.

*ksahu@Kuldeeps-MacBook-Air ~ % sudo vi /etc/hosts

##

# Host Database

#

# localhost is used to configure the loopback interface

# when the system is booting. Do not change this entry.

##

127.0.0.1 localhost

255.255.255.255 broadcasthost

::1 localhost

52.221.202.246 pvcecs-master.cldrsetup.local

52.221.202.246 hue-kd-hive-vw1.apps.cldrsetup.local

52.221.202.246 cml-task-bo6klv.kuldeep-cml.apps.cldrsetup.local

52.221.202.246 kuldeep-cml.apps.cldrsetup.local

18.139.222.78 pvcbase-master.cldrsetup.local

13.251.65.11 cldr-mngr.cldrsetup.local

# ECS Links

52.221.202.246 console-cdp.apps.cldrsetup.local prometheus-cp.apps.cldrsetup.local infra-prometheus.apps.cldrsetup.local validation-cdp.apps.cldrsetup.local kube-dashboard.apps.cldrsetup.local longhorn.apps.cldrsetup.local fluent-console-cdp.apps.cldrsetup.local vault.localhost.localdomain

# PvC Base Cluster Nodes

18.139.222.78 pvcbase-master.cldrsetup.local pvcbase-master

13.215.202.164 pvcbase-worker1.cldrsetup.local pvcbase-worker1

172.31.23.0 pvcbase-worker2.cldrsetup.local pvcbase-worker2

18.141.13.157 pvcbase-worker3.cldrsetup.local pvcbase-worker3

# PvC Data Services Cluster Nodes

172.31.30.239 pvcecs-master.cldrsetup.local pvcecs-master

172.31.22.43 pvcecs-worker1.cldrsetup.local pvcecs-worker1

172.31.30.249 pvcecs-worker2.cldrsetup.local pvcecs-worker2

172.31.26.24 pvcecs-worker3.cldrsetup.local pvcecs-worker3

172.31.24.198 pvcecs-worker4.cldrsetup.local pvcecs-worker4

172.31.24.53 pvcecs-worker5.cldrsetup.local pvcecs-worker5

[arabic, start=4]
. Optionally, Update the /etc/hosts file on your working machine/JumpHost (The /etc/hosts entries required for ECS Data Services)

https://docs.cloudera.com/management-console/1.5.4/private-cloud-security-overview/mc-private-cloud-security-overview.pdf[[.underline]#https://docs.cloudera.com/management-console/1.5.4/private-cloud-security-overview/mc-private-cloud-security-overview.pdf#]

*Embedded Container Service (ECS) :

• console-cdp.apps.*APPDOMAIN

• prometheus-cp.apps.*APPDOMAIN

• infra-prometheus.apps.*APPDOMAIN

• validation-cdp.apps.*APPDOMAIN

• kube-dashboard.apps.*APPDOMAIN

• longhorn.apps.*APPDOMAIN

• fluent-console-cdp.apps.*APPDOMAIN

*Entries required by CDW

Let *APPDOMAIN* be the base app domain for the ECS cluster. For example, if your console URL is "console-cdp.apps.cldrsetup.local", then the APPDOMAIN is "cldrsetup.local". Let *VWHNAME* be the name of the CDW Virtual Warehouse. This must match the name the user provides when creating a new Virtual Warehouse (VW).

*Endpoints of Hive VW:

• hue-*VWHNAME*.apps.*APPDOMAIN

• hs2-*VWHNAME*.apps.*APPDOMAIN

*Endpoints of Impala VW:

• hue-*VWHNAME*.apps.*APPDOMAIN

• coordinator-*VWHNAME*.apps.*APPDOMAIN

• admissiond-web-*VWHNAME*.apps.*APPDOMAIN

• catalogd-web-*VWHNAME*.apps.*APPDOMAIN

• coordinator-web-*VWHNAME*.apps.*APPDOMAIN

• statestored-web-*VWHNAME*.apps.*APPDOMAIN

• impala-proxy-*VWHNAME*.apps.*APPDOMAIN

• impala-autoscaler-web-*VWHNAME*.apps.*APPDOMAIN

*Endpoints of Viz:

• viz-VWHNAME.apps.APPDOMAIN

++**************************************************************************************************************++

=== Configure Ranger with SSL/TLS enabled PostgreSQL Database

Login to Cloudera Manager Web Console. Go to *_Ranger > Configuration._

*Note*: Make sure that:

* The database and database user for Ranger service are created in the required postgreSQL.
* A database server certificate is issued by a trusted certificate authority.
* The server host name matches the host name in the database server certificate.

From CDPDC-7.1.5 onwards, Ranger service requires postgres JDBC driver *_version >= 42.2.5_*. The Ranger code also constructs the JDBC connection string to have *_sslmode=verify-full_*, if Ranger Database SSL configurations are set in case of postgresql database type.

*For more details:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-enable-ssl-tls-ranger-postgres-db.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-enable-ssl-tls-ranger-postgres-db.html#]

Copy the database server certificate to *_/var/lib/ranger/_* path, or use any custom path.

[root@pvcbase-master ~]# *cp -rv /root/.postgresql/root.crt /var/lib/ranger/root.crt

* *In Review Config, search for SSL and update the following configurations:

[width="100%",cols="36%,64%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Ranger DB SSL Enabled:

(ranger.db.ssl.enabled)
____

a|
____
true (Checked)
____

a|
____
Ranger DB SSL Required:

(ranger.db.ssl.required)
____

a|
____
true (Checked)
____

a|
____
Ranger DB SSL Verify Server Certificate:

(ranger.db.ssl.verifyServerCertificate)
____

a|
____
true (Checked)
____

a|
____
Ranger DB Auth Type:

(ranger.db.ssl.auth.type)
____

a|
____
1-way
____

a|
____
Ranger Admin Database SSL Certificate File:

(ranger.db.ssl.certificateFile)
____

a|
____
<path-to-db-server-certificate>: *_/var/lib/ranger/root.crt_* or custom path

*_/var/lib/cloudera-scm-server/.postgresql/root.crt_
____

a|
____
Ranger Database JDBC URL Override:
____

a|
____
jdbc:postgresql://<db_host>:<db_port>/<db_name>?sslmode=verify-full&

sslrootcert=<server_certificate_path>

*_jdbc:postgresql://cldr-mngr.cldrsetup.local:5432/ranger?ssl=true&sslmode=verify-full&sslrootcert=/var/lib/ranger/root.crt_
____

a|
____
Set Load Balancer Address (Optional)
____

a|
____
http://<ranger_host>:6080 http://pvcbase-master.cldrsetup.local:6080[[.underline]#http://pvcbase-master.cldrsetup.local:6080#]

https://<ranger_host>:6182 https://pvcbase-master.cldrsetup.local:6182[[.underline]#https://pvcbase-master.cldrsetup.local:6182#]
____

|===

* After updating the configurations, click on *_Save Changes._
* Ranger Service restart is required for rangeradmin after updating Ranger configuration.
* Click on *_Actions -> Restart_* under Ranger Service.
* Run below command on pvcbase-master node to check the Ranger logs, during restart.

[root@pvcbase-master ~]# tail -f /var/log/ranger/admin/catalina.out

image:image72.png[image72,width=691,height=446]

++**************************************************************************************************************++

=== Configure Hive metastore with SSL/TLS enabled PostgreSQL Database (Mandatory Step for CDW)

In the Cloudera Manager Web console; go to *_Hive > Configuration > Hive Metastore Database JDBC URL Override_*.

Copy the database server certificate to *_/var/lib/hive/_* path, or use any custom path.

[root@pvcbase-master ~]# *cp -rv /root/.postgresql/root.crt /var/lib/hive/root.crt

*Edit value as:* jdbc:postgresql://<db_host>:<db_port>/<db_name>?sslmode=verify-full&sslrootcert=<server_certificate_path>

*_jdbc:postgresql://cldr-mngr.cldrsetup.local:5432/hive?ssl=true&sslmode=verify-full&sslrootcert=/var/lib/hive/root.crt_

image:image88.png[image88,width=666,height=72]

[arabic, start=36]
. Restart required for Hive Metastore Server and HiveServer2 after updating Hive configuration.
. Click on *_Actions -> Restart_* under *_Hive_* and *_Hive-on-Tez_* Services.

++**************************************************************************************************************++

=== Scale the Cluster (Optional– Skip this step)

The role assignment recommendation above is for clusters with at least 64 servers and in High Availability. For smaller clusters running without High Availability the recommendation is to dedicate one server for Name Node and a second server for secondary name node and YARN Resource Manager. For larger clusters larger than 16 nodes the recommendation is to dedicate one server each for name node, YARN Resource Manager and one more for running both Name Node (High Availability) and Resource Manager (High Availability) as in the table (no Secondary Name Node when in High Availability).

[arabic, start=38]
. For production clusters, it is recommended to set up Name Node and Resource manager in High Availability mode.

This implies that there will be at least 3 master nodes, running the Name Node, YARN Resource manager, the failover counterpart being designated to run on another node and a third node that would have similar capacity as the other two nodes.

All the three nodes will also need to run zookeeper and quorum journal node services. It is also recommended to have a minimum of 8 Data Nodes in a cluster. Please refer to the next section for details on how to enable HA.

=== Enable High Availability (Optional– Skip this step)

[arabic, start=39]
. Setting up High Availability is done after the Cloudera Installation is completed. https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/managing-clusters/topics/cm-high-availability.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/managing-clusters/topics/cm-high-availability.html#]

*Configure Browsers for Kerberos Authentication

[arabic, start=40]
. To enable specific web browsers to use SPNEGO to negotiate Kerberos authentication, please visit: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-how-to-guides/topics/cm-security-browser-access-kerberos-protected-url.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-how-to-guides/topics/cm-security-browser-access-kerberos-protected-url.html#]

=== CDP Private Cloud Base checklist

https://supportmatrix.cloudera.com/[[.underline]#Cloudera support matrix#] lists the supported software for the CDP Private Cloud Base cluster and the CDP Private Cloud Data Services containerized cluster.

*Please review CDP Private Cloud Base Checklist:

https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation/topics/cdppvc-installation-pvcbase-checklist.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation/topics/cdppvc-installation-pvcbase-checklist.html#]

image:image154.png[A screenshot of a computer Description automatically generated,width=446,height=265]

++**************************************************************************************************************++

=== Configure Ranger authentication for LDAP (Optional– Skip this Step)

Follow steps below to configure Ranger for LDAP authentication.

[arabic]
. In Cloudera Manager, select *_Ranger_*, then click the *_Configuration_* tab.
. To display the authentication settings, type "*_authentication_*" in the Search box. Scroll down to see all of the *_LDAP_* settings.
. Select LDAP for "Admin Authentication Method".

image:image187.png[image187,width=555,height=331]

[arabic, start=4]
. Configure the following settings for LDAP authentication as shown below, the details depends on your configuration, based on existing LDAP/AD setup:

[arabic, start=8]
. *User LDAP Integration

[width="100%",cols="39%,61%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Admin *LDAP*/AD Auth URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/ *_(Give LDAP or AD Server LDAP ADDR)_
____

a|
____
Admin *LDAP*/AD Auth *Bind User*/ Bind DN:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
Admin *LDAP*/AD Auth Bind User Password:
____

a|
____
*_<redhat123>_* (password for KDC admin, configured earlier)
____

a|
____
Admin *LDAP*/AD Auth User DN Pattern:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
Admin *LDAP*/AD Auth User Search Filter:
____

a|
____
*For AD:* (&(objectClass=user)(sAMAccountName={0}))

*For LDAP*: (&(objectClass=person)(uid={0}))
____

a|
____
Admin *LDAP*/AD Auth Group Search Base:
____

a|
____
cn=groups,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
Admin *LDAP*/AD Auth Group Search Filter:
____

a|
____
*For AD:* (&(objectClass=group)(member={0}))
____

*For LDAP*: (&(objectClass=posixGroup)(memberUid={0}))

a|
____
Admin *LDAP*/AD Auth Group Role Attribute:
____

a|
____
cn
____

a|
____
Admin *LDAP*/AD Auth Base DN:
____

a|
____
dc=cldrsetup,dc=local
____

a|
____
Admin *LDAP*/AD Auth Referral:
____

a|
____
follow
____

a|
____
Admin AD Auth Domain Name: (For AD Setup)
____

a|
____
cldrsetup.local
____

|===

*(Search under Configuration for- _ranger.ldap_)

image:image197.png[image197,width=623,height=367]

*Additional parameters required for AD Based integration: (Search under Configuration for- _ranger.ldap.ad_)

image:image58.png[image58,width=624,height=330]

[arabic, start=5]
. Edit Usersync configuration. Example values set are shown in the screenshot below:

image:image139.png[image139,width=693,height=525]

image:image1.png[image1,width=444,height=501]

image:image57.png[image57,width=450,height=348]

[arabic, start=9]
. *UserSync LDAP Integration

[width="100%",cols="41%,59%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Source for Syncing User and Groups:
____

a|
____
org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder
____

a|
____
Ranger Usersync Unix Backend:
____

a|
____
nss
____

a|
____
Usersync LDAP/AD URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/
____

a|
____
Usersync Bind User:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
Usersync Bind User Password:
____

a|
____
*_<vmware123>_* (password for KDC admin, configured earlier)
____

a|
____
Usersync User Search Base:
____

a|
____
cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
Usersync User Search Scope:
____

a|
____
sub
____

a|
____
Usersync User Object Class:
____

a|
____
person
____

a|
____
Usersync User Search Filter:
____

a|
____
uid=
____

a|
____
Usersync User Name Attribute:
____

a|
____
uid
____

a|
____
Usersync Referral:
____

a|
____
follow
____

a|
____
Usersync Username Case Conversion:
____

a|
____
none
____

a|
____
Usersync Groupname Case Conversion:
____

a|
____
none
____

a|
____
Usersync Enable User Search:
____

a|
____
Ranger Usersync Default Group
____

a|
____
Usersync Group Search Base:
____

a|
____
cn=groups,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
Usersync Group Search Scope:
____

a|
____
sub
____

a|
____
Usersync Group Object Class:
____

a|
____
ipausergroup
____

a|
____
Usersync Group Name Attribute:
____

a|
____
cn
____

a|
____
Usersync Group Member Attribute:
____

a|
____
member
____

|===

[arabic, start=6]
. Click on save changes.
. Restart Ranger service.
. Login to Ranger Admin WebUI with ldap authentication

For more details: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-ranger-authentication-unix-ldap-ad/topics/security-ranger-authentication-ldap-settings.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/security-ranger-authentication-unix-ldap-ad/topics/security-ranger-authentication-ldap-settings.html#]

++**************************************************************************************************************++

=== Configure Hue for LDAP Authentication (Optional– Skip this Step)

Configuring Hue for Lightweight Directory Access Protocol (LDAP) enables you to import users and groups from a directory service, synchronize group membership manually or automatically at login, and authenticate with an LDAP server. Hue supports Microsoft Active Directory (AD) and open standard LDAP such as OpenLDAP and Forgerock OpenDJ Directory Services.

[arabic]
. Login to *_Cloudera Manager_*. Go to *_Cluster > Hue > Configuration_*.
. Change value for Authentication Backend – desktop.auth.backend.LdapBeckend,desktop.auth.backend.AllowFirstUserDjangoBackend

image:image76.png[image76,width=616,height=59]

[arabic, start=3]
. Edit value for LDAP configuration. Example values set are shown in the screenshot below:

image:image137.png[A screenshot of a computer Description automatically generated,width=553,height=568]

image:image158.png[A screenshot of a computer Description automatically generated,width=449,height=482]

*Table: LDAP Integration

[width="100%",cols="41%,59%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
LDAP URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/
____

a|
____
LDAP Server CA Certificate (Optional):
____

a|
____
/root/cacert.p12
____

a|
____
Enable LDAP TLS (Hue):
____

a|
____
True (Checked)
____

a|
____
LDAP Search Base:
____

a|
____
dc=cldrsetup,dc=local
____

a|
____
LDAP Bind User Distinguished Name:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Bind Password:
____

a|
____
*_<vmware123>_* (password for KDC admin, configured earlier)
____

a|
____
LDAP Username for Test LDAP Config:
____

a|
____
admin
____

a|
____
LDAP Group Name for Test LDAP Config:
____

a|
____
users
____

a|
____
LDAP User filter:
____

a|
____
(&(uid={0})(objectClass=person))
____

a|
____
LDAP Group filter:
____

a|
____
(&(member={1})(objectClass=posixgroup))
____

a|
____
LDAP Group Name Attribute:
____

a|
____
cn
____

a|
____
LDAP Group Membership Attribute:
____

a|
____
member
____

|===

[arabic, start=4]
. Click on save changes
. Restart HUE service.
. Click on Actions next to Hue. Click on Test LDAP Configuration.

image:image183.png[A screenshot of a computer Description automatically generated,width=251,height=292]

[arabic, start=7]
. Click on Test LDAP Configuration.

image:image9.png[A screenshot of a computer error Description automatically generated,width=344,height=163]

[arabic, start=8]
. Click on Finish.

image:image195.png[A screenshot of a computer Description automatically generated,width=407,height=169]

For more details: https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/securing-hue/topics/hue-authenticate-users-with-ldap.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/securing-hue/topics/hue-authenticate-users-with-ldap.html#]

++**************************************************************************************************************++

=== Configure Atlas for LDAP authentication (Optional– Skip this Step)

Follow steps below to configure Atlas authentication for LDAP. (*_If LDAP is not integrated for Atlas, we need to use local OS users present on the node where Atlas server is installed i.e. base-master)_

[arabic]
. Login to Cloudera Manager WebUI. Go to Cluster > Atlas > Configuration.
. Edit LDAP configuration. Sample configuration is shown in the screenshot below:

image:image119.png[A screenshot of a computer Description automatically generated,width=592,height=733]

image:image36.png[image36,width=484,height=360]

[arabic, start=10]
. *Atlas LDAP Integration

[width="100%",cols="46%,54%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Enable LDAP Authentication (Atlas):
____

a|
____
True (Checked)
____

a|
____
LDAP Server URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/
____

a|
____
User DN Pattern:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Group Search filter:
____

a|
____
(&(member={1})(objectClass=posixgroup))
____

a|
____
LDAP Group Search Base:
____

a|
____
cn=groups,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Group-Role Attribute:
____

a|
____
cn
____

a|
____
LDAP DN:
____

a|
____
dc=cldrsetup,dc=local
____

a|
____
LDAP Bind DN Username:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Bind DN Password:
____

a|
____
*_<vmware123>_* (password for KDC admin, configured earlier)
____

a|
____
LDAP Referral:
____

a|
____
follow
____

a|
____
LDAP User filter:
____

a|
____
(&(uid={0})(objectClass=person))
____

a|
____
LDAP Authentication Type:
____

a|
____
LDAP
____

a|
____
AD Referral: *(Only for AD Setup)
____

a|
____
follow
____

a|
____
AD User Search Filter: *(Only for AD Setup)
____

a|
____
(sAMAccountName={0})
____

a|
____
AD User Default Role: *(Only for AD Setup)
____

a|
____
ROLE_USER
____

|===

[arabic, start=3]
. Click on save changes.
. Restart Atlas service.

*For more details:* https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/atlas-securing/topics/atlas-configure-ldap-authentication.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/atlas-securing/topics/atlas-configure-ldap-authentication.html#]

++**************************************************************************************************************++

=== Configure Hive for LDAP Authentication (Optional– Skip this Step)

image:image28.png[image28,width=566,height=509]

[arabic, start=11]
. *LDAP Integration-Hive

[width="100%",cols="41%,59%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
LDAP Username:
____

a|
____
admin
____

a|
____
LDAP Password:
____

a|
____
*_<vmware123>_* (password for KDC admin, configured earlier)
____

a|
____
Enable LDAP Authentication for HiveS2:
____

a|
____
True (Checked)
____

a|
____
LDAP URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/
____

a|
____
LDAP Base DN:
____

a|
____
dc=cldrsetup,dc=local
____

a|
____
Enable LDAP Authentication for HMS:
____

a|
____
True (Checked)
____

a|
____
LDAP URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/
____

a|
____
LDAP Base DN:
____

a|
____
dc=cldrsetup,dc=local
____

|===

++**************************************************************************************************************++

=== Configure HDFS properties to optimize log collection (Optional– Skip this Step)

CDP uses “out_webhdfs” Fluentd output plugin to write records into HDFS, in the form of log files, which are then used by different Data Services to generate diagnostic bundles. Over time, these log files can grow in size. To optimize the size of logs that are captured and stored on HDFS, you must update certain HDFS configurations in the hdfs-site.xml file using Cloudera Manager.

[arabic]
. Login to *_Cloudera Manager WebUI_*.
. Go to *_Cluster_* > *_HDFS Service_* > *_Configuration._
. Enable *_WebHDFS._

image:image68.png[A screenshot of a computer Description automatically generated,width=443,height=197]

[arabic, start=4]
. Edit value for HDFS Service Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml as shown in the screenshot below:

image:image172.png[A screenshot of a computer Description automatically generated,width=440,height=297]

[arabic, start=5]
. Click Save Changes.
. Restart the HDFS service.
. Restart CDP Private Cloud Base cluster.

++**************************************************************************************************************++

=== CDP Private Cloud (PvC) Data Services (DS) Installation

CDP Private Cloud Data Services lets you deploy and use the Cloudera Data Warehouse (CDW), Cloudera Machine Learning (CML), and Cloudera Data Engineering (CDE) Data Services.

This section summarizes Cloudera Private Cloud Data Science v1.5.4 installation through Embedded Container Service on Cloudera Private Cloud Base 7.1.9.

A CDP Private Cloud Data Services deployment includes an Environment, a Data Lake, the Management Console, and Data Services (Data Warehouse, Machine Learning, Data Engineering). Other tools and utilities include Replication Manager, Data Recovery Service, CDP CLI, and monitoring using Grafana.

To deploy CDP Private Cloud Data Services you need a CDP Private Cloud Base cluster, along with container-based clusters that run the Data Services. You can either use a dedicated *_RedHat OpenShift container cluster (OCP)_* or deploy an *_Embedded Container Service (ECS)_* container cluster.

The Private Cloud deployment process involves configuring Management Console, registering an environment by providing details of the Data Lake configured on the Base cluster, and then creating the workloads.

Platform Managers and Administrators can rapidly provision and deploy the data services through the Management Console, and easily scale them up or down as required.

CDP Private Cloud Base provides the following components and services that are used by CDP Private Cloud Data Services:

* SDX Data Lake cluster for security, metadata, and governance
* HDFS and Ozone for storage
* Powerful and open-source Cloudera Runtime services such as Ranger, Atlas, Hive Metastore (HMS), etc.
* Networking infrastructure that supports network traffic between storage and compute environments.

=== Embedded Container Service (ECS) checklist

Use the checklist for Embedded Container Service (ECS) for CDP Private Cloud Data Services: https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ds-checklist.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ds-checklist.html#]

*CDP Private Cloud Data Services software requirements

[arabic]
. You must have a minimum of one agent node for ECS.
. Enable TLS on the Cloudera Manager cluster for communication with components and services.
. Set up Kerberos on these clusters.
. Ensure that all of the hosts in the ECS cluster have more than *_300 GiB_* of free space in the each *_/var/lib_* and *_/docker_* directories, on each host in a Private Cloud Containerized Cluster, at the time of installation. The default docker service uses *_/docker_* folder. Whether you wish to retain *_/docker_* or override *_/docker_* with any other folder, you *_must have a minimum of 300 GiB free space_*. The hosts in a Private Cloud Containerized Cluster that have GPUs are required to have nVidia Drivers and nvidia-container-runtime installed.

image:image63.png[image63,width=660,height=97]

[arabic, start=5]
. *_Python 3.8<_* is required for Cloudera Manager version 7.11.3.0 and higher versions. Cloudera Manager agents will not start unless Python 3.8 is installed on the cluster nodes.
. The cluster generates multiple hosts and host-based routing is used in the cluster in order to route it to the right service. You must decide on a domain for the services which Cloudera Manager by default points to one of the host names on the cluster. However, during the installation, you should check the default domain and override the default domain (only if necessary) with what you plan to use as the domain. The default domain must have a https://docs.cloudera.com/management-console/1.5.4/private-cloud-security-overview/topics/mc-private-cloud-security-no-wildcard-tls.html[*_[.underline]#wildcard DNS entry#_*]. For example, “*_*.apps.cldrsetup.local_*”.
. It is recommended that you leave IPv6 enabled at the OS level on all ECS nodes.
. Take care of enough disk space is available on each host in ECS cluster.
. On each of the ECS hosts, create three partitions of the attached 2T EBS volume (non-root) and mount those partitions as volumes *_/lhdata /cdwdata /docker_*. (*_/docker_* is used for docker cache, *_/lhdata_* for LongHorn NFS Volume storage, and *_/cdwdata_* for local storage and CDW)
. ECS requires JDK, krb5-workstation, krb5-libs, NTP, iptables packages to be installed on all hosts.
. Enable cgroup v1 in Red Hat Enterprise Linux 9. *_(optional, skip this step)_

# In RHEL 9 cgroup-v2 is enabled by default, follow steps below to enable cgroup v1:

# Check if the cgroup-v2 is mounted currently as default. +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "mount | grep cgroup" +
 +
# Add the kernel command line parameter systemd.unified_cgroup_hierarchy=0 & systemd.legacy_systemd_cgroup_controller.

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a 'grubby --update-kernel=/boot/vmlinuz-$(uname -r) --args="systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller"' +
 +
# Reboot the system for changes to take effect.

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "systemctl reboot"

# Verify the changes after reboot:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "cat /proc/cmdline"

BOOT_IMAGE=(hd0,gpt2)/vmlinuz-5.14.0-162.23.1.el9_1.x86_64 root=/dev/mapper/rhel-root ro crashkernel=1G-4G:192M,4G-64G:256M,64G-:512M resume=/dev/mapper/rhel-swap rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller

# Mount shows legacy cgroup-v1 mounted now.

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "mount | grep cgroup" +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "ll /sys/fs/cgroup"

[arabic, start=12]
. For CML, you must install *_nfs-utils_* in order to mount longhorn-nfs provisioned mounts. The *_nfs-utils_* package is required on every node of the ECS cluster. Run this command *_“dnf install nfs-utils”_* to install *_nfs-utils_*.

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "dnf install -y nfs-utils iscsi-initiator-utils"

[arabic, start=13]
. Check *_iptables_* is installed and working fine.

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "iptables -L"

[arabic, start=14]
. If *_iptables_* is not working as expected or giving command not found error then remove the package on all ecs nodes and re-install.

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "dnf remove -y iptables && dnf install -y iptables && iptables -L"

[arabic, start=15]
. If *_iptables_* is not present on ecs cluster nodes, then Install *_iptables_*, and verify if it is working as expected.

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "dnf install -y iptables && iptables -L"

[arabic, start=16]
. For nodes with NVIDIA GPU *_(if applicable)_*, ensure that the GPU hosts have nVidia Drivers and *_nvidia-container-runtime_* installed. You must confirm that drivers are properly loaded on the host by executing the command nvidia-smi. You must also install the nvidia-container-toolkit package. *_(skip this step)_
. You must install *_nvidia-container-toolkit_* *_(if applicable)_*. (*_nvidia-container-runtime_* is migrated to *_nvidia-container-toolkit_*, see Migration Notice.) The steps for this are shown in the https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-yum-or-dnf[[.underline]#NVIDIA Installation Guide#]. If using Red Hat Enterprise Linux (RHEL), use dnf to install the package. See Installing the https://docs.nvidia.com/datacenter/cloud-native/edge/latest/nvidia-gpu-with-device-edge.html#installing-the-nvidia-container-toolkit[[.underline]#NVIDIA Container Toolkit#]. *_(skip this step)_

##### Verify linux version:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "uname -m && cat /etc/*release"

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "uname -a"

* +
##### Verify GCC installation and version

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "gcc --version"

##### Verify nodes with NVIDIA GPU installed:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "lspci -nnv | grep -i nvidia"

##### Set subscription to RHEL9.x

[root@ipaserver ~]# ansible all -m shell -a "subscription-manager release --set=9.4"

[root@ipaserver ~]# ansible all -m shell -a "subscription-manager release --show" +
[root@ipaserver ~]# ansible all -m shell -a "sudo dnf clean all"

[root@ipaserver ~]# ansible all -m shell -a "sudo rm -rvf /var/cache/dnf" +
 +
##### _(Optional if not completed prior)_ Enable optional repos - On RHEL 9 Linux only

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms"

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms"

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "subscription-manager repos --enable=codeready-builder-for-rhel-9-x86_64-rpms"

##### Install kernel headers and development packages for the currently running kernel* +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r)"

##### Remove outdated Signing Key:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo rpm --erase gpg-pubkey-7fa2af80*"

##### Download and Install NVIDIA CUDA Toolkit [This exercise documented with CUDA 12.3.2 for RHEL 9 rpm(local) installation]

[root@ipaserver ~]# wget https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda-repo-rhel9-12-2-local-12.2.2_535.104.05-1.x86_64.rpm[[.underline]#https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda-repo-rhel9-12-2-local-12.2.2_535.104.05-1.x86_64.rpm#] +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m copy -a "src=/root/cuda-repo-rhel9-12-2-local-12.2.2_535.104.05-1.x86_64.rpm dest=/root/cuda-repo-rhel9-12-2-local-12.2.2_535.104.05-1.x86_64.rpm" +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo rpm --install cuda-repo-rhel9-12-2-local-12.2.2_535.104.05-1.x86_64.rpm"

##### From the NVIDIA Driver Downloads page, download NVIDIA Driver https://www.nvidia.com/download/index.aspx[[.underline]#https://www.nvidia.com/download/index.aspx#] as per the GPU, OS and NVIDIA CUDA version.* +
[root@ipaserver ~]# wget https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo[[.underline]#https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo#]

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m copy -a "src=/root/nvidia-driver-local-repo-rhel9-535.161.07-1.0-1.x86_64.rpm dest=/root/nvidia-driver-local-repo-rhel9-535.161.07-1.0-1.x86_64.rpm" +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo rpm --install nvidia-driver-local-repo-rhel9-535.161.07-1.0-1.x86_64.rpm” +
 +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo dnf clean all"

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo dnf -y module install nvidia-driver:latest-dkms" +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo dnf -y install cuda"

##### Enable nvidia-persistenced services:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo systemctl enable nvidia-persistenced.service"

* +
##### Reboot the machine:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo reboot"

##### After the machine boots, verify that the NVIDIA drivers are installed properly:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo nvidia-smi"

* +
##### Installing with dnf

##### Configure the production repository:* +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m copy -a "src=/root/nvidia-container-toolkit.repo dest=/etc/yum.repos.d/nvidia-container-toolkit.repo" +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo dnf config-manager --enable nvidia-container-toolkit-experimental" +
[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m shell -a "sudo dnf install -y nvidia-container-toolkit"

*If you are installing ECS on RHEL 8 or RHEL 9: _(Not Required for Cloud Based Servers, perform only on Datacenter/Bare-metal environments for RKE Based ECS clusters)_

##### Run the following command to check to see if the nm-cloud-setup.service and nm-cloud-setup.timer services are enabled:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m copy -a "systemctl status nm-cloud-setup.service nm-cloud-setup.timer"

##### If the nm-cloud-setup.service and nm-cloud-setup.timer services are enabled, disable them by running the following command on each host you added:

[root@ipaserver ~]# ansible ecsmasternodes,ecsnodes -m copy -a "systemctl disable nm-cloud-setup.service nm-cloud-setup.timer && systemctl stop nm-cloud-setup.service nm-cloud-setup.timer"

##### For more information, see Known issues and limitations.

If you disabled the nm-cloud-setup.service and nm-cloud-setup.timer services, reboot the added hosts.

[arabic, start=41]
. For more information, see: https://docs.rke2.io/known_issues/#networkmanager[Known issues and limitations].

[arabic]
. Prepare CDP Private Cloud Base for the Private Cloud Data Services installation: https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ecs-prepare-cdp-private-cloud-base.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ecs-prepare-cdp-private-cloud-base.html#]
. Use this checklist to ensure that your CDP Private Cloud Base is configured and ready for installing CDP Private Cloud Data Services: https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-pvcbase-checklist.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-pvcbase-checklist.html#]
. Use this checklist to ensure that your Embedded Container Service (ECS) is configured and ready for installing CDP Private Cloud Data Services: https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ds-checklist.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ds-checklist.html#]

[width="100%",cols="27%,73%",options="header",]
|===
|*Checklist Item* |*Details
|*Docs Links* a|
Docs - https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-pvcbase-checklist.html[[.underline]#ECS Checklist#]

Docs - https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-requirements-ecs.html[[.underline]#ECS System Requirements#]

https://supportmatrix.cloudera.com/[[.underline]#Support Matrix#]

|*ClouderaManager version* |7.11.3 - CHF6 (minimum)
|*LDAP Configuration* |CM is configured for LDAP, and you have a copy of the CA trust chain that signed your LDAP server's cert (i.e. root cert & intermediates if any).
|*LDAP Bind User Account* |You have the account and password for LDAP Bind user account.
|*Base Cluster & CM Agents TLS* a|
TLS: AutoTLS? Yes/No

TLS: AutoTLS, signed with customer CA? Yes/No

TLS: Manual, signed with customer CA? Yes/No

TLS: Manual self signed (not recommended) xxx

|*Kerberos Admin Account* |CM is setup with Kerberos admin account (cloudera-scm account & password).
|*ECS Installer Prerequisites Checker* |IMPORTANT! CM ECS installer now has an Enforcing prerequisites checker, install will fail if your infra does not meet requirements outlined in docs. https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ecs-steps.html[[.underline]#See item 14&#44; in doc#].
|*Runtime Version* |7.1.9 (CHF6+) is installed & configured. See https://supportmatrix.cloudera.com/[[.underline]#support matrix#]
|*New Changes with 1.5.4* |
|*Minimum Required Components* |Zookeeper, HDFS, Ozone, HBase, Hive Metastore, Kafka, Solr-Infra, Ranger, Atlas, YARN (optional for CDW, but required for Spark pushdown in CDE, CML).
|*Base Components TLS* |All Base components configured with TLS.
|*Base Components Kerberos* |All Base components configured with Kerberos.
|*CDW Base Cluster HMS* a|
CDW - Base HMS, mTLS can be used instead of user/password.

If using CDW, the Base Cluster HMS is configured to allow TLS connections (set to allow TLS, but not forced to require it).

|*Wildcard DNS Record* a|
*CRITICAL*: A wildcard subdomain (called app_domain in cm) has been created in DNS. For AD this usually means a subdomain folder + an "A record" with name = "_" pointing to the ECS host and a CNAME record. The "A record" does not need a reverse PTR._

_*AD Example:* If corp domain is "company.com", then create a new subdomain folder called "ecs-dev", then within that create AD another called "apps", then create a "A record" inside that folder, its name is "_", its IP will be the IP of the host you will install ECS Server onto.

Verify with dig utility e.g., $ dig foobar.apps.cdppvc.com.

Look for "ANSWER" section in the result.

|*DNS Resolver* |DNS needs to be the primary resolver, do not use entries in /etc/hosts (except for localhost).
|*ECS Nodes File System* |RHEL 8 & RHEL 9 - disk partition that backs "/var" should be xfs file system (using the default ftype = 1), this is usually the root partition /.
|*RHEL/Centos 7.x Support* |*New in 1.5.4:* RH/Centos 7.x no longer supported. If running RHEL 7.x, you must upgrade to a higher version before installing CDP Private Cloud Data Services.
|*Hostname Configuration* |Hostname must = FQDN, use hostnamectl set-hostname <fqdn>.
|*/etc/resolv.conf Entries* |/etc/resolv.conf must NOT have more than 3 "nameserver" entries.
|*MTU Size* |Ensure that net interfaces are configured to a maximum transmission unit size (MTU) not smaller than 1450, (typical MTU is 1500), required by the Calico CNI plugin.
|*Storage Devices* |Validate that Storage devices meet Sys Requirements (see docs) and are mounted using a consistent naming convention. Devices are expected to be SCSI block devices (NFS mounts do not work). Devices for CDW Cache and Block should be separate mounts. Use a Logical Volume Manager if you have multiple devices per type.
|*Disk Mounts* |All disk mounts should be xfs(type1), including root partition. The partition backing /var/lib *must have at least 300GB free* after install of CM agent. If 300GB free is not available, then must symlink "/var/lib/docker" & "/var/lib/rancher" from another partition (must be xfs).
|*Required Mounts* a|
3 mounts required per host, name them any you like, DO NOT USE symbolic links.

*See*: https://longhorn.io/docs/1.5.4/volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node[[.underline]#https://longhorn.io/docs/1.5.4/volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node#]

{empty}1) A xfs mount for Docker storage (used for install staging, CML model building, and embedded repo storage) at least 300GB

{empty}2) A xfs mount for CDW Cache, this is a physical NVMe device, sizing dependent on use case, consumed in 600GB chunks per executor & coordinator

{empty}3) A xfs mount for Longhorn Block storage which provides k8s Persistent Volumes (PVs), min 1 TB per node, vendor recommends this mount should be a Logical Volume (from LVM2) (for future expansion needs)

|*Passwordless Sudo User* |You have credentials for a passwordless sudo user for each host. If you don't have these credentials, then you will have to install CMAgents manually (not using wizard) and configure them for TLS manually.
|*CM Agents Configuration* |Nodes are configured with requirements to run CM Agents.
|*iptables Configuration* |Do NOT disable iptables, but ensure iptables is set to defaults. Any customer-set chains/rules must be un-set. (firewalld not supported, nftables not yet certified). Check if /etc/sysconfig/iptables exists and inform the customer to remove it. Ensure no system management software like puppet, ansible, etc., will recreate it on reboot. ECS will attempt to install iptables-services if it's not present.
|*RHEL8 - iptables* |Install iptables like: # yum --setopt=tsflags=noscripts install -y iptables. See docs: https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.0/installation-ecs/topics/cdppvc-installation-ecs-steps.html[[.underline]#ECS Installation Steps#].
|*RHEL8 - Net Manager* |Services nm-cloud-setup.service and nm-cloud-setup.timer must be stopped (if they exist) and disabled. This is seen on AWS RHEL images, unlikely to see this on-prem. (*Risky, as we found instances can get corrupt, due to network connectivity may lost from AWS side)
|*RHEL8 - VMware* |Certain versions of VMware virtual network interface (vmxnet3) can cause what seems like intra-pod packet drops. This is due to the Calico https://github.com/projectcalico/calico/issues/4727[[.underline]#Issue#]. Workaround: See case https://csh.cloudera.com/ccs/index.html#/case/946323/case-escalations/a152H0000045PkiQAE[[.underline]#Cloudera Support#].
|*RHEL9 - iptables* |Check iptables configuration as described for RHEL8.
|*Time Service* |Host has chronyd or NTP running.
|*DNS Resolving* |Each ECS host is forward and reverse DNS resolvable.
|*DNS Forward and Reverse* |Each ECS host can forward and reverse DNS resolve each base host.
|*SELinux and System Settings* |SELinux off or set to Permissive, vm.swappiness = 1, Transparent Huge Pages Off (1.3.4+ support SELinux, see docs).
|*NFS Requirements* |Linux package "nfs-utils" must be installed for CML & CDE.
|*GPU Requirements & Config* |See docs for GPU requirements & config for CML.
|*3rd Party Software* a|
Certain 3rd party network monitors/firewalls may seriously interfere with ECS cluster traffic.

Illumio... Illumio runs an agent on each host, uses iptables and can block traffic if it is running in Primary Firewall Mode. That mode must be set to = off. See https://docs.illumio.com/core/21.1/Content/Guides/kubernetes-and-openshift/configuration/firewall-coexistence-on-pods.htm[[.underline]#Illumio Docs#].

VMware NSX-T can create blocking firewall rules. See https://jira.cloudera.com/browse/CDPVC-686[[.underline]#NSX-T Jira#].

|*Air Gapped Customers* |If a customer is Air Gapped, they must download all the ECS bits and stage them behind an HTTP server. Make sure they do this right away as this is 150GB of content (circa 1.5.0).
|*License Key* |In every case, Customer (or Cloudera employee) MUST have a valid license key that includes an entitlement for PrivateCloud!
|*Ingress Certificates* |It is usually better to have a customer CA signed ECS Ingress domain cert, but not required as ECS will sign using an RKE CA cert (the RKE CA will have to be pre-trusted on a user's machine, or the browser will show "insecure" TLS connection). This ECS generated CA is not managed by CM's auto-TLS, so for JDBC/impala-shell/beeline/other connections, you must add this to a client truststore manually. The ECS generated CA will have a CN=rke2-server-ca@xxxxxxxxxx and is located at /var/lib/rancher/rke2/server/tls/server-ca.crt.
|*Customer CA Signed Certs* a|
For customer CA signed certs: This cert must include 2 SubjectAltNames. If wildcard format is not allowed, let the installer use the RKE CA. Example:

DNS.1 = *.apps.cdppvc.com

DNS.2 = apps.cdppvc.com

|*New in 1.5:* |
|*No Wildcard TLS* |There is an option to deploy when wildcard SAN entries are not allowed. See doc for method & limits: https://docs.cloudera.com/management-console/1.5.4/private-cloud-security-overview/topics/mc-private-cloud-security-no-wildcard-tls.html[[.underline]#No wildcard TLS#].
|*CML Workspace Certificates* a|
If installing in a network domain that requires strict host checking, HSTS, you will need to use TLS for CML Workspaces. Fun fact: "cloudera.com" requires HSTS, which is usually not seen with customers but is emerging.

This cert must include SubjectAltNames.

DNS.1 = *.xxxx-xxxx.apps.cdppvc.com (xxxx-xxxx = the CML workspace ID)

|*CDE Virtual Cluster Certificates* a|
Same as CML for HSTS. CDE includes a utility to generate RKE signed certs for each virtual cluster. If you make your own cert for a CDE virtual cluster, this cert must include SubjectAltNames.

DNS.1 = *.xxxx-xxxx.apps.cdppvc.com (xxxx-xxxx = the CDE virtual cluster ID)

|*CDW HMS Configuration* |CDW can be configured to allow non-TLS connections for HMS db. See https://docs.cloudera.com/data-warehouse/1.5.1/securing/topics/dw-private-disable-ssl-requirement-hms-database.html[[.underline]#CDW Documentation#].
|*FreeIPA Support* a|
FreeIPA is now supported. "LDAP Group Search Filter" in Mgmt Console must include *(!(cn=admins))*, or group sync will break

e.g. *(&(member={0})(objectclass=posixgroup)(!(cn=admins)))*.

Must alter the /etc/krb5.conf - comment out includedir directives.

In CM7.10.1, CM generated "local to auth" krb5 rules include embedded "\Q" and "\E" chars, this is legal for base but causes a parsing problem for CDW. Contact for workaround (Should be patched in 1.5.1 - CHF1).

|*Fixed in 1.5.0:* |
|*/etc/resolv.conf Entries* |/etc/resolv.conf must NOT have more than 2 "search" entries, causes a side-effect in Impala Statestore pod. Per https://jira.cloudera.com/browse/BLESC-6074[[.underline]#Jira#].
|*External DS Metadata DB* |*New*: External DS metadata db is deprecated. This will be a problem for "legacy" customers as we have no migrate from external to embedded db.
|*Container Repos* |Able to use customer-owned container repos is now supported e.g., Artifactory, Nexus, Harbor, Docker dist.
|*Iceberg Tables* |Support for Iceberg Tables.
|*CDSW to CML Migration* |Tech Preview: CDSW to CML migration. See release notes.
|*New in 1.4.1:* |
|*/var/lib Storage Requirements* |/var/lib must have 100GB.
|*Docker Storage Requirements* |Docker storage must have 200GB.
|*vCores Requirements* |Min 16 vcores required (on Masters too? Yes).
|*iptables-save* |iptables-save to /var/lib/ecs/iptables.save (we don't need to flush IPtables upon reinstall anymore).
|*Oracle HMS* |Oracle HMS is now GA. This can get complicated as many Oracle customers use TLS + Client cert auth, which we don't support. We support TLS + user password auth only. Contact for workaround.
|*Data Viz* |Data Viz is GA.
|*Ozone for CDW* |Ozone for CDW is Tech Preview.
|*Impala Custom Pod Sizes* |Impala Custom pod sizes now GA. See https://docs.cloudera.com/data-warehouse/1.4.1/administration/topics/dw-private-cloud-create-custom-pod-configs-impala-vw.html[[.underline]#Impala Pod Configs#].
|*CDE Resource Limits* |CDE Resource Limits - Tech Preview.
|*/etc/resolv.conf Entries (2)* |/etc/resolv.conf must NOT have more than 2 "search" entries, causes a side-effect in Impala Statestore pod. Per https://jira.cloudera.com/browse/BLESC-6074[[.underline]#Jira#].
|*New In 1.3.4:* |
|*Cloudera Manager* |Cloudera Manager now prevents ECS Server hosts from running workloads. ECS Servers are masters only. See https://docs.cloudera.com/management-console/1.3.4/private-cloud-release-notes/topics/mc-private-cloud-whats-new-07.html[[.underline]#Cloudera Manager Release Notes#].
|*ECS Hosts Workloads* |ECS hosts can now be configured to reserve hosts for workloads that require GPU drivers. See https://docs.cloudera.com/machine-learning/1.3.4/private-cloud-requirements/topics/ml-gpu-node-setup.html[[.underline]#GPU Node Setup#].
|*SELinux Support* |SELinux is now supported for ECS clusters. See https://docs.cloudera.com/cdp-private-cloud-data-services/1.3.4/managing-ecs/topics/mc-ecs-selinux.html[[.underline]#SELinux Documentation#].
|===

++**************************************************************************************************************++



=== Installing CDP Private Cloud Data Services using ECS

Follow the steps here to install CDP Private Cloud Data Services with the *_Embedded Container Service (ECS)_*.

Follow the steps outlined below to add hosts to be part of the Cloudera Private Cloud Data Services cluster and the install ECS (embedded container service) through either internet or air gapped method.

https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation-ecs/topics/cdppvc-installation-ecs-steps.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/latest/installation-ecs/topics/cdppvc-installation-ecs-steps.html#]

https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/index.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/index.html#]

[arabic, start=42]
. We will be installing CDP Private Cloud Data Services via the internet method.
. For more details on dedicating ECS node for specific workload type please visit: https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/managing-ecs/topics/cm-managing-ecs-dedicating-nodes-for-workloads.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/managing-ecs/topics/cm-managing-ecs-dedicating-nodes-for-workloads.html#]
. If you do not have entitlements to access https://archive.cloudera.com/p/cdp-pvc-ds/latest/[[.underline]#https://archive.cloudera.com/p/cdp-pvc-ds/latest/#], contact your Cloudera account team to get the necessary entitlements.

##### Latest ECS Supported Version Of Cloudera-Manager is _[.underline]#7.11.3 CHF11#_

++**************************************************************************************************************++



=== Installing ECS Cluster

Follow the steps in this topic to install CDP Private Cloud Data Services with the Embedded Container Service (ECS).

*Important:

RHEL 7.x support on ECS has been dropped in CDP Private Cloud Data Services 1.5.4 and higher versions. If you are running RHEL 7.x, you must upgrade to a higher version before installing CDP Private Cloud Data Services.

[arabic]
. In the *_Cloudera Manager WebUI_* console, go to the *_Data Services_* page by clicking on the *_Data Services_* link on the Pane located at the Left Hand side of the browser window. Alternatively, you can also click *(+) _Add > Add Cluster_* at the top right in Cloudera Manager, then select *_Private Cloud Containerized Cluster_* as the cluster type.

image:image41.png[image41,width=432,height=265]

[arabic, start=2]
. The *_Add Private Cloud Containerized Cluster_* page appears. Click *_Continue_* on the page.

image:image13.png[image13,width=564,height=373]

*Note*: Alternatively, you can also click *_(+) Add > Add Cluster_* at the *_top right_* in *_Cloudera Manager_*, then select *_Private Cloud Containerized Cluster_* as the *_cluster type_*, then click *_Continue_*. +
image:image74.png[image74,width=230,height=109] +
image:image132.png[image132,width=611,height=397]

*Step. Getting started

[arabic, start=3]
. On the getting started page of the installation wizard, select either *_Internet_* or *_Air Gapped_* as the Install Method. *_(We are going here with Internet Installation method only)_
. If you select the *_Internet Install_* Method option on the *_Getting Started_* page, images are copied over the internet from the Cloudera repository. For this deployment, we will select the *_Internet_* as the install method. *_Select Repository._* (To use a custom repository link provided to you by Cloudera, click *_Custom Repository, instructions for this are mentioned in later steps_*):

image:image59.png[image59,width=624,height=380]

*Note:* Verify if the version shown below is the same as the version that we are willing to install i.e. *_1.5.4-h5_* for current setup.

image:image194.png[image194,width=624,height=109]

If not (as shown in below screenshot), then we need to add-up an additional custom URL and configure that to use our specific version.

image:image40.png[image40,width=476,height=59]

image:image8.png[image8,width=624,height=114]

Add the additional URL by *(+)* and click on *_Save Changes._

image:image192.png[image192,width=551,height=167]

[arabic, start=5]
. When you select the *_Air Gapped_* install option, extra steps are displayed. Follow these steps on the *_cldr-mngr_* node (our bits server), to download and mirror the Cloudera archive URL using a local HTTP server:
. (For installing via a local mirror with an http server. You will need to set up a full mirror of Cloudera's repositories via a temporary HTTP server within the perimeter network of all hosts.): *(Skip this step, as we will choose Internet method in next steps to Install)

image:image189.png[image189,width=461,height=250]

[root@cldr-mngr ~]# mkdir -p /var/www/html/cloudera-repos/cdp-pvc-ds/ +
[root@cldr-mngr ~]# cd /var/www/html/cloudera-repos/cdp-pvc-ds/

##### Download everything under https://archive.cloudera.com/p/cdp-pvc-ds/latest[[.underline]#https://archive.cloudera.com/p/cdp-pvc-ds/latest#]/ to your local httpserver, e.g. http://your_local_repo/cdp-pvc-ds/latest[[.underline]#http://your_local_repo/cdp-pvc-ds/latest#]/ using the below command

[root@cldr-mngr cdp-pvc-ds]# wget -l 0 --recursive --no-parent -e robots=off -nH --cut-dirs=2 --reject="index.html*" -t 10 https://_<username>:<password>_@archive.cloudera.com/p/cdp-pvc-ds/latest/

[root@cldr-mngr cdp-pvc-ds]#

[root@cldr-mngr cdp-pvc-ds]# ls -lt 1.5.4-h4/

total 116300

-rw-r--r-- 1 root root 284820 Mar 15 10:13 manifest.json

-rw-r--r-- 1 root root 118747085 Mar 15 10:13 cdp-private-1.5.4-h4-b27.tgz

drwxr-xr-x 2 root root 4096 Mar 15 10:13 parcels

drwxr-xr-x 2 root root 4096 Mar 15 10:12 manifests

drwxr-xr-x 2 root root 32768 Mar 15 10:12 images

[root@cldr-mngr ~]#

##### Modify the manifest.json file inside the downloaded directory. Change "http_url": "..." to "http_url": "http://your_local_repo/cloudera-repos/cdp-pvc-ds/1.5.4-h2/[[.underline]#http://your_local_repo/cloudera-repos/cdp-pvc-ds/1.5.4-h4/#]"

[root@cldr-mngr ~]# vi manifest.json* +
"http_url": "http://*192.168.1.38*/cloudera-repos/cdp-pvc-ds/1.5.4-h4/"

[root@cldr-mngr ~]#

[arabic, start=7]
. Click *_Custom Repository_*. Add *http://your_local_repo/cdp-pvc-ds/1.5.4-h2[_[.underline]#http://your_local_repo/cloudera-repos/cdp-pvc-ds/1.5.4-h4#_]/* as a custom repository. Click on *_Save Changes_*. *(Skip this step, as we have chosen Internet method steps to Install)

image:image27.png[image27,width=675,height=123]

[arabic, start=8]
. Click the *_Select Repository_* drop-down and select *http://your_local_repo/cloudera-repos/cdp-pvc-ds/1.5.4-h2/[_[.underline]#http://your_local_repo/cloudera-repos/cdp-pvc-ds/1.5.4-h4/#_] (Skip this step, as we will chosen Internet method steps to Install)

image:image149.png[image149,width=679,height=439]

[arabic, start=9]
. Click *_Continue_*.

*Step. Cluster Basics

[arabic, start=10]
. On the *_Cluster Basics- Add Private Cloud Containerized Cluster_* page, enter the *_Cluster Name_* for the *_Private Cloud Data Services (ECS) cluster_* that you want to create in the *_Cluster Name_* field. From the *_Base Cluster_* drop-down list, select the *_CDP Private Cloud Base Cluster_* (which is created earlier i.e. *_PvCBaseCluster1_*), that has the storage and SDX services that you want this new Private Cloud Data Services instance to connect with. Click *_Continue_*.

image:image159.png[image159,width=668,height=431]

*Step. Specify Hosts

[arabic, start=11]
. On the *_Specify Hosts_* page, hosts that have already been added to Cloudera Manager are listed on the *_Currently Managed Hosts_* tab. If the intended ECS nodes are not added previously (in case of a fresh setup) and you can’t see them listed under *_Currently Managed Hosts_* tab, Click on *_New Hosts_* tab.
* *Note*: To specify the hosts that are part of the cluster, enter Fully Qualified Domain Names (FQDNs)/hostnames or IP addresses in the Hostname field, or provide a list of search patterns/ IP address range of available matching ECS hosts. Host names must be in lowercase. If you use uppercase letters in any host name, the cluster services will not start after enabling Kerberos.
+
*Note*: You can Provide host pattern pvcbase-master, pvcbase-worker[1-5] or pvcbase-worker[1-5].cldrsetup.local etc separated with a new line and Click on *_Search_*. Cloudera Manager will "discover" the hosts based on matching the pattern provided by you to add in the cluster. Verify that all desired nodes have been found and *_selected for installation_*. Verify host entries, *_deselect_* any that you do not want to install services on. Select and/or deselect one or more of these hosts to add to the ECS cluster and click *_Continue_*.

pvcecs-master.cldrsetup.local

pvcecs-worker[1-10].cldrsetup.local

*Note:* Click the pattern link under the Hostname box to display more information about allowed *FQDN* patterns.

image:image138.png[image138,width=669,height=356]

*Step. Select JDK

[arabic, start=12]
. On the *_Select JDK_* page, select any one from the below options:

* *Manually manage JDK (Select this option)* (manual installation of JDK11/17 with CDH 7.1.9+).
* Install a Cloudera-provided version of OpenJDK
* Install a system-provided version of OpenJDK

image:image53.png[image53,width=676,height=383]

*Step. Enter Login Credentials

[arabic, start=13]
. On the *_Enter Login Credentials_* page, ‘*_All hosts accept the same password’_* is selected by default. Enter the user name in the SSH Username box, and type in and confirm the password. You can also select the *_All hosts accept the same private key_* option and provide the Private Key generated in previous steps and passphrase (If applicable).

Enter the values for the parameters as shown below. (*We will be using the private key approach*, you can use password option as well, both options should considerably work)

[width="100%",cols="39%,61%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Enable TLS for
____

a|
____
All existing and future clusters
____

a|
____
SSH username
____

a|
____
*_root_
____

a|
____
Authentication method
____

a|
____
*_All hosts accept same private key_* / All hosts accept same password
____

a|
____
*Private Key (If using Key approach)
____

a|
____
Choose the private key created and downloaded in earlier section
____

a|
____
Password (If using Password approach)
____

a|
____
Enter VM’s root users’ password
____

a|
____
Confirm Password
____

a|
____
Enter VM’s root users’ password (again)
____

|===

image:image95.png[image95,width=606,height=338]

[loweralpha]
. *Screenshot for using the Password based authentication method.

image:image122.png[image122,width=604,height=455]

[loweralpha, start=2]
. *Screenshot for using the Private Key based authentication method.

*Step. Install Agents

[arabic, start=14]
. The *_Install Agents_* page appears and displays a progress indicator showing the agent packages getting installed. Click on *_Continue_* after successful agent installation on hosts to be added in Cloudera Manager.

image:image156.png[image156,width=670,height=327]

image:image160.png[image160,width=665,height=392]

*Step. Assign Roles

[arabic, start=15]
. Next on the *_Assign Roles_* page, ensure that the roles assignment for your new *_Private Cloud Containerized cluster_* is as follows. You can customize the role assignment for your cluster. But, Cloudera does not recommend altering assignments unless you have specific requirements such as having selected a specific host for a specific role.

*Note:* Single node ECS installation is supported, but is only intended to enable CDSW to CML migration. If you are installing ECS on a single node, only the Docker and ECS Server roles are assigned. The ECS Agent role is not required for single node installation.

[arabic, start=45]
. With 1 mgmt node and 12 worker node for CDP Data Services ECS cluster we select host role assignment as:

[width="100%",cols="39%,61%",options="header",]
|===
a|
____
*Role
____

a|
____
*ECS Host
____

a|
____
Docker Server
____

a|
____
All ECS Hosts (i.e., ECS master and worker nodes)

pvcecs-master, pvcecs-worker[1-11]
____

a|
____
ECS Server
____

a|
____
ECS Master Nodes only (pvcecs-master)
____

a|
____
ECS Agent
____

a|
____
ECS Worker Nodes only (pvcecs-worker[1-11])
____

|===

image:image30.png[image30,width=647,height=467]

[arabic, start=16]
. Click *Continue*.

*Step. Configure Docker Repository

[arabic, start=17]
. On the *_Configure Docker Repository_* page, select *_Cloudera default Docker Repository_* or ( *_Use an Embedded Docker Repository_* option. Then select Default in the below section. There are several options for configuring a Docker Repository. For more information about these options, see https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-docker-access.html[[.underline]#Docker repository access#]. )

The following ports must be opened and allowed no matter which Docker repository option you choose.

* Ports required for Cloudera Manager/Cloudera Manager agent (port 5000 is required for CML):

[width="100%",cols="44%,56%",options="header",]
|===
a|
____
*Protocol
____

a|
____
*Port
____

a|
____
TCP
____

a|
____
7180-7192
____

a|
____
TCP
____

|19001
a|
____
TCP
____

a|
____
5000
____

a|
____
TCP
____

a|
____
9000
____

|===

* Inbound rules for ECS Server nodes (Kubernetes/RKE2):

[width="100%",cols="44%,56%",options="header",]
|===
a|
____
*Protocol
____

a|
____
*Port
____

a|
____
TCP
____

a|
____
9345
____

a|
____
TCP
____

|6443
a|
____
UDP
____

a|
____
8472
____

a|
____
TCP
____

a|
____
10250
____

a|
____
TCP
____

a|
____
2379
____

a|
____
TCP
____

a|
____
2380
____

a|
____
TCP
____

a|
____
30000-32767
____

|===

* Inbound Rules for the ECS Agent (Kubernetes/RKE2):

[width="100%",cols="44%,56%",options="header",]
|===
a|
____
*Protocol
____

a|
____
*Port
____

a|
____
UDP
____

a|
____
4789
____

|===

[.underline]#Embedded Docker Repository:#

Proceed with default selection to deploy all of the default Docker images to the repository, or select *_Select the Optional Images_* to choose which images to deploy. If you will be deploying Cloudera Machine Learning (CML), toggle the *_Cloudera Machine Learning_* switch on to copy the images for CML.

image:image210.png[image210,width=655,height=562]

[loweralpha]
. *Screenshot for Use an embedded Docker Repository Option

[.underline]#Cloudera default Docker Repository:# (We will setup this way)

This option requires that cluster hosts have access to the internet and you have selected Internet as the install method.

image:image146.png[image146,width=677,height=654]

[loweralpha, start=2]
. *Screenshot for Use Cloudera’s Docker Repository Option

[.underline]#Custom Docker Repository:#

This option requires that you set up a Docker Repository in your environment and that all cluster hosts have connectivity to the repository.

*Note*: If you are installing ECS on a single node, you should select the Use a Custom Docker Repository option. Single node ECS installation is supported, but is only intended to enable CDSW to CML migration.

image:image204.png[image204,width=567,height=569]

[loweralpha, start=3]
. *Screenshot for Use Cloudera’s Docker Repository Option

You must enter the following options:

[width="100%",cols="44%,56%",options="header",]
|===
a|
____
*Option
____

a|
____
*Value
____

a|
____
Custom Docker Repository:
____

a|
____
Enter the URL for your Docker Repository
____

a|
____
Docker Username:
____

a|
____
Enter the username for the Docker Repository
____

a|
____
Docker Password:
____

a|
____
Enter the password for the Docker Repository
____

|===

*Important:* Do not use the $ character for this password.

*Docker Certificate* – Click the *_Choose File_* button to upload a TLS certificate to secure communications with the Docker Repository.

Click the *_Generate the copy-docker script_* button to generate and download a script that copies the Docker images from Cloudera, or (for air-gapped installation) from a local http mirror in your network.

Run the script from a machine that is running Docker locally and has access to the Docker images using the following commands:

docker login [***URL for Docker Repository***] -u [***username of user with write access***]

bash copy-docker.txt

The copying operation may take 4 - 5 hours.

[arabic, start=46]
. Embedded Repository can be a single point of failure. If the node that runs the Docker Repository fails or becomes unavailable, some cluster functionalities might become unavailable. Moving the Docker Repository to another node is a complex process and will require engaging Cloudera Professional Services.
. Cloudera Repository option is best suited for proof-of-concept, non-production deployments or deployments that do not have security requirements that disallow internet access. This option requires that cluster hosts have access to the internet, and an installation method selected as Internet.

*Step. Configure Data Services

[arabic, start=18]
. On the *_Configure Data Services_* page, modify configuration as appropriate and modify the storage related parameters. Edit Application domain to match “app.example.com”. For example in this solution we configure AD Domain Services with “CLDRSETUP.LOCAL” as domain name. Created a wildcard entry “*.apps.cldrsetup.local”. Click *_Continue_*.

[width="100%",cols="39%,61%",options="header",]
|===
a|
____
*Role
____

a|
____
*ECS Host
____

a|
____
Data Storage Directory:
____

a|
____
/docker
____

a|
____
ECS (Service Wide):
____

a|
____
/lhdata
____

a|
____
Application Domain:
____

a|
____
cldrsetup.local
____

a|
____
Local Path Storage Directory:
____

a|
____
/cdwdata
____

|===

On the *_Configure Data Services_* page, you can modify configuration settings such as the data storage directory, number of replicas, and so on. If you want to specify a custom certificate, place the certificate and the private key in a specific location on the Cloudera Manager server host and specify the paths in the input boxes labeled as Ingress Controller TLS/SSL Server Certificate/Private Key File below. This certificate will be copied to the Control Plane during the installation process.

*Note:* The "Ingress Controller TLS/SSL Server Certificate File (PEM Format)" must only contain -----BEGIN CERTIFICATE----- through -----END CERTIFICATE----- (inclusive) for the server and CA certs. It cannot include any preamble text and, and must not include a private key.

The "Ingress Controller TLS/SSL Server Private Key File (PEM Format)" must only contain the unencrypted key, and only the header through the footer, with no preamble text.

Both of these files must be readable by the "cloudera-scm" account.

For information on the required entries that must be present in DNS and TLS certificates when not using wildcards, refer to 'No Wildcard DNS/TLS Setup'

image:image124.png[image124,width=651,height=653]

[arabic, start=48]
. Please review the range of cluster IP and service IP as part of the ECS installation. It might conflict existing network configuration. Please adjust the range of IPs to be configured. Consult with the network team to avoid potential conflict.

image:image56.png[A white background with black lines Description automatically generated with medium confidence,width=604,height=87]

[arabic, start=19]
. Click *_Continue_*.

*Step. Configure Databases

[arabic, start=20]
. On the *_Configure Databases_* page, edit size for the *_Embedded Database Disk Space_*. Click *_Continue_*.

image:image148.png[image148,width=681,height=560]

*Step. Install Parcels

[arabic, start=21]
. On the *_Install Parcels_* page, the selected parcel is downloaded to the Cloudera Manager server host, distributed, unpacked, and activated on the ECS cluster hosts. Click *_Continue_*.

image:image171.png[image171,width=666,height=402]

*Step. Check Prerequisites

[arabic, start=22]
. If the hosts do not meet the prerequisites, the *_Check Prerequisites_* page displays the applicable issues. Correct the issues, then click *_Run Again_*. After all of the issues have been resolved, click *_Continue_*. Prerequisites checks are included in the new ECS release version 1.5.4.

image:image63.png[image63,width=624,height=89]

*The following prerequisites are checked:

[width="100%",cols="30%,70%",options="header",]
|===
a|
____
*Host Prerequisite Inspection
____

a|
____
*Validation
____

a|
____
StorageInspection:
____

a|
____
Checks for a minimum of 300GiB space in the /var/lib and docker data directories. Checks if /var/lib/longhorn or its parent directories are symlinked. If they are, this inspection will fail.
____

a|
____
CPUInspection:
____

a|
____
Checks to make sure the hosts have 16 virtual cores.
____

a|
____
PortsInspection:
____

a|
____
Checks for the availability of ports 443 and 80.
____

a|
____
EcsHostDnsInspection:
____

a|
____
Checks to make sure there are less than 3 nameserver entries in the /etc/resolv.conf file, and checks the connections to the Cloudera Manager cluster and the CDP console. It also checks to see if vault.localhost.localdomain’s ping can be resolved. If not, it is likely that the host /etc/nsswitch.conf file is misconfigured.

If this inspection fails:
____

* Check the /etc/resolv.conf and /etc/nsswitch.conf files and ensure that /etc/resolv.conf does not contain 3 or more nameservers, and that /etc/nsswitch.conf does not contain myhostname under the hosts field.
* Check to see if the connections were resolved correctly. If connection to the CDP console fails, check to see if your DNS wildcard is configured properly.

a|
____
VersionInspection:
____

a|
____
Checks that Java is installed and consistent among all ECS hosts.
____

a|
____
IPTablesInspection:
____

a|
____
Checks that if the iptables command exists, rules are cleared. If the iptables command does not exist, iptables gets installed during FirstRun so this inspection passes.

If iptables are installed and the rules are not cleared, this inspection will fail.
____

a|
____
EcsCleanUpHostInspection:
____

a|
____
Checks to make sure that the /var/lib/rancher and docker data directories do not contain any files.
____

|===

image:image52.png[image52,width=598,height=407]

*Note:* If the prerequisite check fails due to any reason or you need to skip this step, try the workaround by right-clicking on the screen, select *_Inspect_*, go to the *_Console_* tab, and run the below command:

*document.querySelector('.btn.next').removeAttribute('disabled');

This will enable the *Continue* button.

image:image177.png[image177,width=569,height=422]

*Step. Inspect Cluster

[arabic, start=23]
. On the *_Inspect Cluster_* page, you can *_Inspect your Network Performance and Hosts_*. Click on the *_Show Inspector Results_*. If the inspect tool displays any issues, you can fix those issues and click on *_Run Again_* to rerun the inspect tool. After all of the issues have been resolved, click on *_Continue_*.

*Note*: These inspections are more comprehensive host and network tests that you can *_optionally run_*. To *_skip these tests_*, select the *_I understand the risks of not running the inspections or the detected issues, let me continue with cluster setup_* checkbox.

[arabic, start=49]
. Safe to ignore unrelated errors in the host inspector result. For example, the hosts in a Private Cloud Containerized Cluster that have GPUs are required to have NVidia Drivers and NVidia-container-runtime installed. The following hosts do not satisfy this requirement: pvcecs-worker[1-12].cldrsetup.local +
Since all hosts part of the ECS installation might not have NVIDIA GPU installed and NVidia driver and NVidia container-runtime is not installed on non-GPU node(s). It is safe to ignore the warning and click on the checkbox to continue with ECS installation.

image:image84.png[image84,width=665,height=610]

*Step. Install Data Services

[arabic, start=24]
. Login to the pvcecs-master node and create kubeconfig file by copying the rke2.yaml file on ecs master node, in order to be able to run the kubectl commands from master node:

[root@pvcecs-master ~]# rm -rvf ~/.kube && mkdir -p ~/.kube && cp /etc/rancher/rke2/rke2.yaml ~/.kube/config

##### The kubectl binary path may vary, do whereis kubectl or use find or locate commands to find the exact path of the kubectl binary on the node.

[root@pvcecs-master ~]# export PATH=/var/lib/rancher/rke2/data/v1.26.10-rke2r1-e58e49f33617/bin/:$PATH

[root@pvcecs-master ~]# kubectl get pods

No resources found in the default namespace.

[root@pvcecs-master ~]# kubectl get all

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE

service/kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 13h

[root@pvcecs-master ~]#

##### Create alias on node, to run the kubectl command

[root@pvcecs-master ~]# echo "export PATH=/var/lib/rancher/rke2/data/v1.26.10-rke2r1-e58e49f33617/bin/:$PATH" >> ~/.bashrc && source ~/.bashrc

[root@pvcecs-master ~]# echo "alias k=kubectl" >> ~/.bashrc && source ~/.bashrc

#### You can copy the config file and set the above ~/.bashrc paths on any of ECS nodes to run kubectl commands from there.

[root@pvcecs-master ~]#

*Note:* Run # kubectl get pods -A to review all pods and their status as either running or completed.

[arabic, start=25]
. After the *RKE2 installation step is completed*, *keep the remaining steps running but immediately log in* to *_pvcecs-master_* node and *_update the config for coredns to point to private DNS_* (ipaserver). The coredns config should look like similar to below one:

##### Test the nslookup and dig from ubuntu image if it is able to resolve the IP from Private DNS

[root@pvcecs-master ~]# kubectl run -it ubuntu1 --image=ubuntu bash

##### If not able to resolve the IP from Private DNS, we need to point coredns to PrivateDNS/ FreeIPA server

[root@pvcecs-master ~]# kubectl run -it ubuntu1 --image=ubuntu bash

[root@pvcecs-master ~]# kubectl get configmap -n kube-system | grep coredns

[root@pvcecs-master ~]# kubectl edit configmap rke2-coredns-rke2-coredns -n kube-system

# Please edit the object below. Lines beginning with a '#' will be ignored,

# and an empty file will abort the edit. If an error occurs while saving this file will be

# reopened with the relevant failures.

#

apiVersion: v1

data:

Corefile: |

.:53 {

errors

health {

lameduck 5s

}

ready

kubernetes cluster.local cluster.local in-addr.arpa ip6.arpa {

pods insecure

fallthrough in-addr.arpa ip6.arpa

ttl 30

}

prometheus :9153

forward . *_172.31.24.240_* {

max_concurrent 1000

}

cache 30

loop

reload

loadbalance

}

kind: ConfigMap

metadata:

annotations:

meta.helm.sh/release-name: rke2-coredns

meta.helm.sh/release-namespace: kube-system

creationTimestamp: "2024-05-17T07:12:19Z"

labels:

app.kubernetes.io/instance: rke2-coredns

app.kubernetes.io/managed-by: Helm

app.kubernetes.io/name: rke2-coredns

helm.sh/chart: rke2-coredns-1.24.006

k8s-app: kube-dns

kubernetes.io/cluster-service: "true"

kubernetes.io/name: CoreDNS

name: rke2-coredns-rke2-coredns

namespace: kube-system

resourceVersion: "39715"

uid: dcb8f600-e107-4c1b-9db7-701060fe063a

[root@pvcecs-master ~]# kubectl rollout restart deployment rke2-coredns-rke2-coredns -n kube-system

##### Verify the status that deployment of coredns is updated with configmap changes

[root@pvcecs-master ~]# k get pod -A |grep dns

##### Test the nslookup and dig from ubuntu image if it is actually able to resolve the IP from Private DNS

[root@pvcecs-master ~]# k run -it ubuntu1 --image=ubuntu bash

[arabic, start=26]
. *_Install Data Services_* step will run a set of first run commands and report status on Data Services installation. The installation progress is displayed on the *_Install Data Services_* page. This step will take *_nearly an hour to complete_*. When the installation is complete, click *_Continue_*.

[arabic, start=50]
. Installing Data Services can take several hours. The copying operation for Docker repository may take 4 - 5 hours.

image:image71.png[image71,width=598,height=508]

[arabic, start=27]
. If you still face any issue while making the services up or during the installation or start of any ECS services please refer to troubleshooting PvC Data Services Cluster part at the end of this document. Though, some of the major issues during installation, their cause and their resolution is listed as below:

*ksahu@Kuldeeps-MacBook-Air ~ %

*Name resolution not working inside pod for console-cdp leading cli pod to fail and env not getting created

*Cluster name CM url not able to connect from ECS cluster

*Solution:

Update coredns pod by edit and redeploy and mention private dns server ip in config

make sure wildcard dns is set up properly.

*After issue is fixed and env get created for ECS: it will show like below:

_Fri May 17 01:23:55 AM PDT 2024_

_Running on: pvcecs-master.cldrsetup.local (172.31.30.239)_

_Fetching session token..._

_Creating environment cldrsetup_

_secret "cm.args" deleted_

_secret/cm.args created_

_job.batch "cli" deleted_

_job.batch/cli created_

_job.batch/cli condition met_

_/opt/app-root/lib64/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'console-cdp.apps.cldrsetup.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings_

_warnings.warn(_

_{_

_"environment": {_

_"environmentName": "cldrsetup",_

_"crn": "crn:altus:environments:us-west-1:d750ff5a-791d-46c8-a95a-f1efaf6185ef:environment:cldrsetup/ad47b514-a135-47dc-a8dd-8d11f33145f7",_

_"cloudPlatform": "standard"_

_}_

_}_

_Waiting for environment cldrsetup to be available_

_The http response code is 200_

_..._

_The http response code is 200_

_..._

_The http response code is 200_

_..._

_The http response code is 200_

_..._

_The http response code is 200_

_Environment was successfully created_

++==========================================================================================================++

*Failed to reconcile with temporary etcd:

*Solution:

bootstrap data already found and encrypted with different token rke error

rm -vf /var/lib/rancher/k3s/server/node-token

++==========================================================================================================++

*Update k8s cordons and rollout

*Solution:

https://www.reddit.com/r/k3s/comments/p3rdap/lost_bootstrap_data_already_found_and_encrypted/[[.underline]#https://www.reddit.com/r/k3s/comments/p3rdap/lost_bootstrap_data_already_found_and_encrypted/#]

[root@pvcecs-master bin]# rm -vf /var/lib/rancher/k3s/server/node-token

rm -rvf /var/lib/rancher/ /etc/rancher

++==========================================================================================================++

*X509 error-

*Solution:

do cleanup for ECS properly

++==========================================================================================================++

*Unable to connect 6443

*Solution:

mkdir -p ~/.kube && cp /etc/rancher/rke2/rke2.yaml ~/.kube/config

++==========================================================================================================++

*Aws_key_id error in cde cluster creation

*Solution:

Configure ozone with other data services before env creation

https://community.cloudera.com/t5/Support-Questions/Unable-to-create-Data-Engineering-Cluster-in-CDP-Private/m-p/377969[[.underline]#https://community.cloudera.com/t5/Support-Questions/Unable-to-create-Data-Engineering-Cluster-in-CDP-Private/m-p/377969#]

++==========================================================================================================++

*Other:

https://repost.aws/knowledge-center/create-lv-on-ebs-partition[[.underline]#https://repost.aws/knowledge-center/create-lv-on-ebs-partition#]

++==========================================================================================================++

*UNRESOLVED: CDE Error

[root@pvcecs-master ~]# k get po -A|grep dex | grep -v -E 'Run|Comp'

dex-base-j95kjx9f cdp-cde-embedded-db-0 0/1 Pending 0 8m54s

dex-base-j95kjx9f dex-base-data-connectors-746fdcddc8-jvbsx 0/1 CrashLoopBackOff 6 (3m22s ago) 8m54s

dex-base-j95kjx9f dex-base-management-api-6c5d48fd96-dgrnt 0/1 CrashLoopBackOff 6 (2m21s ago) 8m54s

[root@pvcecs-master ~]# kubectl logs -n dex-base-j95kjx9f dex-base-data-connectors-746fdcddc8-jvbsx

Defaulted container "data-connectors" out of: data-connectors, k8tz (init)

{"level":"INFO","timestamp":"2024-05-27T01:37:23.834-0700","caller":"cmd/data-connectors.go:92","message":"Configuration file used: /etc/dc/data-connectors.yaml"}

{"level":"INFO","timestamp":"2024-05-27T01:37:23.834-0700","caller":"cmd/start.go:112","message":"Data store init","store type":"sql"}

2024/05/27 01:37:23 /grid/0/jenkins/workspace/workspace/App_builds/SOURCES/data-connectors/pkg/store/sqlstore/utils.go:79

[error] failed to initialize database, got error dial tcp 10.43.139.205:3306: connect: connection refused

{"level":"FATAL","timestamp":"2024-05-27T01:37:23.840-0700","caller":"cmd/start.go:115","message":"store initialisation failure","error":"dcObject persistent store init failure: DB session creation error: error creating DB session for 'mysql', error: dial tcp 10.43.139.205:3306: connect: connection refused","stacktrace":"github.infra.cloudera.com/CDH/data-connectors/pkg/cmd.startServer\n\t/grid/0/jenkins/workspace/workspace/App_builds/SOURCES/data-connectors/pkg/cmd/start.go:115\ngithub.infra.cloudera.com/CDH/data-connectors/pkg/cmd.glob..func3\n\t/grid/0/jenkins/workspace/workspace/App_builds/SOURCES/data-connectors/pkg/cmd/start.go:68\ngithub.com/spf13/cobra.(*Command).execute\n\t/grid/0/jenkins/.asdf/installs/golang/1.17.6/packages/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:856\ngithub.com/spf13/cobra.(*Command).ExecuteC\n\t/grid/0/jenkins/.asdf/installs/golang/1.17.6/packages/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:960\ngithub.com/spf13/cobra.(*Command).Execute\n\t/grid/0/jenkins/.asdf/installs/golang/1.17.6/packages/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:897\ngithub.infra.cloudera.com/CDH/data-connectors/pkg/cmd.Execute\n\t/grid/0/jenkins/workspace/workspace/App_builds/SOURCES/data-connectors/pkg/cmd/data-connectors.go:105\nmain.main\n\t/grid/0/jenkins/workspace/workspace/App_builds/SOURCES/data-connectors/cmd/data-connectors/main.go:32\nruntime.main\n\t/grid/0/jenkins/.asdf/installs/golang/1.17.6/go/src/runtime/proc.go:255"}

[root@pvcecs-master ~]# kubectl logs -n dex-base-j95kjx9f dex-base-management-api-6c5d48fd96-dgrnt

Defaulted container "dex-base-management-api" out of: dex-base-management-api, k8tz (init)

No Envoy proxy, skip waiting for its readiness

Skip fetching DB certificates

Running main binary: /dex/bin/runtime-management-server

2024/05/27 01:38:24 INFO config.go:61 Loaded config from: /etc/dex/conf/dex.yaml

2024/05/27 01:38:24 INFO db.go:204 Opening database connection, driver: mysql

2024/05/27 01:38:24 FATAL db.go:226 dbConn.BeginTx failed: dial tcp 10.43.139.205:3306: connect: connection refused

[root@pvcecs-master ~]# kubectl logs -n dex-base-j95kjx9f cdp-cde-embedded-db-0

Defaulted container "cdp-cde-embedded-db" out of: cdp-cde-embedded-db, k8tz (init)

[root@pvcecs-master ~]#

[arabic, start=28]
. When the installation is complete, the *_Summary page_* appears. Click *_Launch CDP Private Cloud_*. You can also click *_Finish_* and then *_access the Private Cloud Data Services_* instance from *_Cloudera Manager_*. Click *_Data Services_*, then click *_Open Private Cloud Data Services_* for the applicable Data Services cluster.

image:image6.png[image6,width=635,height=543]

++**************************************************************************************************************++

=== Additional Steps for ECS Cluster Setup: _(Optional, Skip this step)_

*Note:* If nvgfd-gpu-feature-discovery-xxxx pods remain in crashlookbackoff please apply patch to fix the issue.

[root@pvcecs-master ~]# kubectl patch clusterrolebinding gpu-feature-discovery -p '{"subjects":[{"kind":"ServiceAccount","name":"gpu-feature-discovery","namespace":"kube-system"}]}'

image:image26.png[A screenshot of a computer Description automatically generated,width=572,height=274]

To reserve a GPU node in Cloudera Private Cloud Data Services ECS cluster, assign a taint to the node. Set the node taint “nvidia.com/gpu: true:NoSchedule” For more details on setting up GPU node:

https://docs.cloudera.com/machine-learning/1.5.4/private-cloud-requirements/topics/ml-gpu-node-setup.html[[.underline]#https://docs.cloudera.com/machine-learning/1.5.4/private-cloud-requirements/topics/ml-gpu-node-setup.html#]

[arabic]
. To *_set up a GPU node_* for ECS, go to *_Hosts > Configuration_*.

image:image200.png[A screenshot of a computer Description automatically generated,width=655,height=284]

[arabic, start=2]
. Edit value for Data Services: Restrict workload types (node_taint) by clicking on Add Host Overrides.image:image60.png[A screenshot of a computer Description automatically generated,width=561,height=204]
. Add Host Overrides for the ECS nodes as per the requirement. For example, we selected two of the four nodes as Dedicated GPU Nodes.

image:image46.png[A screenshot of a computer Description automatically generated,width=580,height=188]

[arabic, start=4]
. Click on *_Add_* and click *_Save Changes_*.

++**************************************************************************************************************++



=== Dedicating ECS nodes for specific workloads _(Optional, Skip this step)_

You use Cloudera Manager to dedicate Embedded Container Service (ECS) cluster nodes for specific workloads. You can dedicate GPU nodes for CML workloads, and NVME nodes for CDW workloads.

*Dedicating ECS nodes when creating a new cluster

[arabic]
. Check the ECS installation requirements.
. Add the new hosts to *_Cloudera Manager._
. In Cloudera Manager, click *_Hosts > All Hosts_*, then select one or more of the *_new ECS hosts_*.
. Click the *_Configuration_* tab, then use the *_Search box_* to locate the *_node_taint_* configuration property.
. *_Select Dedicated GPU Node_* to dedicate the node for CML workloads, or select *_Dedicated NVME node_* to dedicate the node for *_CDW_* workloads. When either of these options are selected, no other workload pods will be allowed to run on the dedicated node.
. Click *_Save Changes_*.
. *_Repeat the previous steps_* to add the *_other ECS hosts_* to *_Cloudera Manager_* and *_assign_* workload types.
. Follow the ECS installation procedure. When you reach the *_Specify Hosts_* page in the installation wizard, the hosts you added to *_Cloudera Manager_* appear. Select the hosts, click *_Continue_*, then proceed through the rest of the installation wizard.
. After the installation is complete, the applicable workloads will only run on the specified dedicated nodes.

image:image100.png[image100,width=493,height=297]

*Dedicating ECS nodes in an existing cluster

[arabic]
. Open the *_Cloudera Manager Admin Console_*.
. On the *_Home page_*, click the *_ECS Cluster_*.
. Click *_Hosts_*, select one or more of the *_ECS hosts_*, then click the *_Configuration_* tab.
. Click the *_Configuration_* tab, then use the *_Search box_* to locate the *_node_taint_* configuration property.
. Select *_Dedicated GPU Node_* to dedicate the node for *_CML_* workloads, or select *_Dedicated NVME node_* to dedicate the node for *_CDW_* workloads. When either of these options are selected, no other workload pods will be allowed to run on the dedicated node.
. Click *_Save Changes_*.
. *_Repeat the previous steps_* to assign workload types to the other *_ECS_* hosts.
. On the *_ECS Cluster_* landing page, click *_Actions > Refresh Cluster._
. After the Refresh is complete, click *_Actions > Rolling Restart_*.

image:image100.png[image100,width=487,height=294]

++**************************************************************************************************************++



=== Accessing CDP Private Cloud

[arabic]
. From the *_Cloudera Manager_* screen, click on *_Data Services(New)_* in the left pane.

image:image25.png[A screenshot of a cloud data service Description automatically generated,width=492,height=240]

[arabic, start=2]
. On the *_CDP Private Cloud Containerized services_* page, click on the *_Open CDP Private Cloud Data Services_* button. This will open the *_CDP PvC authentication_* page.

image:image42.png[open data svs,width=361,height=196]

[arabic, start=3]
. On the PvC DS Authentication Page, if you have *_LDAP account_* credentials, then enter its username and password and then click on *_Login_*. Else, you can click on *_Login as Local Administrator_*, and enter the default credentials. *_(admin/admin)_

image:image15.png[data svcs auth,width=322,height=221]

[arabic, start=4]
. *_Login_* to *_CDP Private Cloud Data Services_* as *_local administrator_*: *_(admin/admin)_

image:image69.png[A screenshot of a computer Description automatically generated,width=289,height=167]

[arabic, start=5]
. After authenticating successfully, you will land at the *_CDP console/Data Services_* page. From this page, you can navigate to different data services and the management services. Click on the *_Management Console_*.

image:image90.png[A screenshot of a computer Description automatically generated,width=360,height=309]

[arabic, start=6]
. On the *_Welcome to CDP Private Cloud_* page, click *_Reset Password_* to change the *_Local Administrator Account password_*. *(OR)* On the *_Management Console_* page, navigate to *_Administration > Authentication_*, and then click *_Reset Password_* to change the *_Local Administrator Account password_*.

image:image7.png[image7,width=577,height=226]

image:image11.png[image11,width=358,height=185]

[arabic, start=7]
. *_Set up external authentication_* using the URL of the LDAP server and a CA certificate (If using prior existing LDAP server and not proceeding with FreeIPA setup) of your secure LDAP *_(e.g. ldap://<ipa_or_ldap_server_fqdn>:389/)_*. Learn more about https://docs.cloudera.com/management-console/latest/private-cloud-administration/topics/mc-private-cloud-security-ldap.html[[.underline]#LDAP user authentication for CDP Private Cloud.#] Enter values for ldap authentication, as mentioned in the below table.

[arabic, start=12]
. *LDAP Integration

[width="100%",cols="43%,57%",options="header",]
|===
a|
____
*Component
____

a|
____
*Value
____

a|
____
Authentication Backend Order:
____

a|
____
Database then EXTERNAL
____

a|
____
Authorization Backend Order:
____

a|
____
Database and EXTERNAL
____

a|
____
External Authentication Type:
____

a|
____
LDAP
____

a|
____
LDAP URL:
____

a|
____
ldap://ipaserver.cldrsetup.local:389/
____

a|
____
LDAP Bind User Distinguished Name:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Bind Password:
____

a|
____
*_<vmware123>_* (password for KDC admin, configured earlier)
____

a|
____
Active Directory Domain: *(For AD Based LDAP)
____

a|
____
<AD DOMAIN>
____

a|
____
LDAP User Search filter:
____

a|
____
(&(uid={0})(objectClass=person))
____

a|
____
LDAP User Search Base:
____

a|
____
cn=users,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP Group Search filter:
____

a|
____
(&(member={1})(objectClass=posixgroup))
____

a|
____
LDAP Group Search Base:
____

a|
____
cn=groups,cn=accounts,dc=cldrsetup,dc=local
____

a|
____
LDAP DistName Pattern:
____

a|
____
uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local
____

|===

image:image92.png[image92,width=676,height=533]

[arabic, start=8]
. Follow the instructions on the *_Welcome to CDP Private Cloud page_* to complete this step.
. Click *_Test Connection_* to ensure that you are able to connect to the *_configured LDAP server_*.

image:image64.png[image64,width=624,height=106]

[arabic, start=10]
. The *_User Management_* tab allows users to add or update roles on existing users. *_Groups_* tab allows users to sync user groups from the active directory to access *_CDP Data Services_*.

image:image157.png[A screenshot of a computer Description automatically generated,width=685,height=196]

image:image209.png[image209,width=688,height=335]

For more details on Cloudera Private Cloud Management console please visit:

https://docs.cloudera.com/management-console/1.5.4/index.html[[.underline]#https://docs.cloudera.com/management-console/1.5.4/index.html#]

image:image142.png[image142,width=684,height=337]

[arabic, start=11]
. After successfully configuring and testing the setup for *_LDAP integration_*, page will auto-redirect for *_Register Environment_* Page, if not, navigate to the https://docs.cloudera.com/management-console/latest/private-cloud-environments/topics/mc-private-cloud-environments.html[[.underline]#Environments#] page, and select *_Register Environment_* where you will provide the Cloudera Manager details and credentials to register the *_PvC Base Cluster DataLake/Control Plane environment_* to DS. Click *_Choose Cluster,_* select the PvC Base cluster from the populated list and click on *_Register_*.

image:image130.png[image130,width=686,height=265]

image:image168.png[image168,width=648,height=319]

image:image31.png[image31,width=658,height=146]

image:image19.png[A screenshot of a computer Description automatically generated,width=664,height=341]

[arabic, start=12]
. To come to this section further, from the *_Cloudera Manager_* screen, click on *_Data Services(New)_* in the left pane and then click on *_Open Private Cloud Data Services_* to launch your *_CDP Private Cloud Data Services instance_*. Log in using the default username and password *_admin_*.

* Click *_Launch CDP_* to launch your CDP Private Cloud.
* Log in using the default username and password admin.
* In the *_Welcome to CDP Private Cloud_* page, click *_Change Password_* to change the Local Administrator Account password.
* Set up external authentication using the URL of the LDAP server and a CA certificate of your secure LDAP. Follow the instructions on the *_Welcome to CDP Private Cloud_* page to complete this step.
* Click *_Test Connection_* to ensure that you are able to connect to the configured LDAP server.
* https://docs.cloudera.com/management-console/1.5.4/private-cloud-environments/topics/mc-private-cloud-environment-register-ui.html[Register a CDP Private Cloud environment]
* https://docs.cloudera.com/data-warehouse/1.5.4/private-cloud-getting-started/topics/dw-private-cloud-create-virtual-warehouse-openshift-overview.html[Create your first Virtual Warehouse in the CDW Data Service]
* https://docs.cloudera.com/machine-learning/1.5.4/workspaces-privatecloud/topics/ml-pvc-provision-ml-workspace.html[Provision an ML Workspace in the CML Data Service]
* https://docs.cloudera.com/data-engineering/1.5.4/enable-data-engineering/topics/cde-private-cloud-add-cde-service.html[Add a CDE service in the CDE Data Service]

++**************************************************************************************************************++



=== CDP Private Cloud Machine Learning (CML)

Please review https://docs.cloudera.com/machine-learning/1.5.3/index.html[[.underline]#CDP Private Cloud Machine Learning#] for more details.

Please review requirements page for ECS and get started with CML on Private Cloud: https://docs.cloudera.com/machine-learning/1.5.4/private-cloud-requirements/topics/ml-pvc-intro.html[[.underline]#https://docs.cloudera.com/machine-learning/1.5.4/private-cloud-requirements/topics/ml-pvc-intro.html#]

For more details on CML workspace and how to steps, visit: https://docs.cloudera.com/machine-learning/cloud/workspaces/topics/ml-provision-workspaces.html[[.underline]#https://docs.cloudera.com/machine-learning/cloud/workspaces/topics/ml-provision-workspaces.html#]

https://docs.cloudera.com/machine-learning/1.5.4/workspaces-privatecloud/topics/ml-pvc-provision-ml-workspace.html[[.underline]#https://docs.cloudera.com/machine-learning/1.5.4/workspaces-privatecloud/topics/ml-pvc-provision-ml-workspace.html#]

=== ML Workspace Creation:

To get started with CML follow steps below:

[arabic]
. On the *_Cloudera Private Cloud Data Services console_*, click on Cloudera *_Machine Learning_*.

image:image129.png[A screenshot of a computer Description automatically generated,width=407,height=329]

[arabic, start=2]
. First time login requires provision of a workspace. Since this will be the first time you open CML, there will be no CML workspace. You will see the screen below. Click on *_Provision Workspace_* on the same page.

image:image145.png[A screenshot of a computer Description automatically generated,width=648,height=208]

[arabic, start=3]
. Provide input required to provision machine learning workspace. Click on provision workspace.

* Enter the configuration values for the workspace as described below.
** *Workspace name*: A suitable name for the workspace.
** *Environment*: Select the default environment from the drop down.
** *Namespace*: This will be the kubernetes namespace under which the pods would be spinned up. By default, it is set to cml. You can change it if you wish to.
** *NFS server*: Select *_Internal_*.
** If you choose *_External NFS Server,_* perform on *_all ECS nodes_*. *(Skip this, as we are not using it in current setup)

#### *nfs://172.31.30.239:/lhdata/nfs_storage/kuldeep-test-cml-w1

[root@pvcecs-master ~]# mkdir -p /lhdata/nfs_storage/kuldeep-test-cml-w1

[root@pvcecs-master ~]# chown 8536:8536 /lhdata/nfs_storage/kuldeep-test-cml-w1

* Under Production Learning, the below parameters need to be updated.
** *Enable Governance*: This provided advanced lineage and governance features. For simple demos or POCs, you may choose to disable it.
** *Enable Model Metrics*: Keep it enabled. It provides you with the metrics.
** *Enable TLS*: You can keep it disabled.
** *Enable Monitoring*: This helps in monitoring the resource usage for the provisioned workspace. Enable it.
** *CML Static Subdomain*: Enter any short name for this parameter that helps in monitoring the resource usage for the provisioned workspace.

image:image43.png[A screenshot of a computer Description automatically generated,width=443,height=467]

image:image37.png[A screenshot of a computer Description automatically generated,width=434,height=235]

*_Note: Click on i icon to get more information on the field._

[arabic, start=4]
. When provisioning of workspace is completed the status reports as *_Ready_*. Once it is created, it appears on the CML Workspaces page as shown below.

image:image115.png[A screenshot of a computer Description automatically generated,width=600,height=302]

[arabic, start=5]
. Click on Manage Access.

image:image99.png[A screenshot of a cloud provider Description automatically generated,width=283,height=211]

[arabic, start=6]
. In the search field search for a user or group to be able to access Machine Learning workspace.

image:image207.png[A screenshot of a computer Description automatically generated,width=640,height=250]

[arabic, start=7]
. Update Resource role for user or group selected to manage access to workspace provisioned in Cloudera Machine Learning.

image:image14.png[A screenshot of a computer Description automatically generated,width=644,height=288]

[arabic, start=8]
. Click on the workspace name created.

image:image16.png[A screenshot of a computer Description automatically generated,width=671,height=205]

[arabic, start=9]
. CML workspace *_WebUI_* overview.

image:image5.png[A screenshot of a computer Description automatically generated,width=642,height=329]

[arabic, start=10]
. Click on Projects tab, expand View Resource Usage Details to review available resources.

image:image143.png[A screenshot of a computer Description automatically generated,width=642,height=266]

[arabic, start=11]
. For more details and how to review projects section in ML workspace: https://docs.cloudera.com/machine-learning/cloud/projects/index.html[[.underline]#https://docs.cloudera.com/machine-learning/cloud/projects/index.html#]

=== Creation of Project in CML Workspace:

[arabic, start=12]
. At the middle right, you will find the New Project button. Click on it. New Project page appears. Enter the details as described below. Enter project name and select type of initial setup.

* *Project Name*: Enter a suitable name for your project.
* *Project Description*: Enter a description for the project.
* *Project Visibility*: Keep it Public for any demos or PoC’s. If you are creating this in a multi-tenant environment, choose Private.

The Initial Setup Section for the New Project has five options as described below. Choose any of these based on your requirement.

* *Blank*: Choose this if you want to start from scratch.
* *Template*: Template projects contain example code that can help you get started with Cloudera Machine Learning. They are available in R, Python, PySpark, and Scala. Using a template project is not required, but it helps you start using Cloudera Machine Learning right away.
* *AMPs*: Applied ML Prototypes provide components to create a complete project. They may include jobs, models and experiments.
* *Local Files*: Choose this if you have all the necessary files in a folder or in a zip.
* *Git*: Choose this if all the resources are stored in a github project.

image:image205.png[A screenshot of a chatbot Description automatically generated,width=493,height=485]

[arabic, start=13]
. Select *_Runtime setup_*. For initial exploration, select Basic and keep the kernel to *_Python3.9_* (enable checkbox to add GPU enabled Runtime variant, if applicable — *We are not using GPU in our current setup*).

image:image182.png[project runtime setup,width=510,height=368]

image:image106.png[A screenshot of a computer program Description automatically generated,width=526,height=394]

[arabic, start=14]
. Click on *_Create Project_*. After some time, a new project will be created and will be available on the Projects page.
. Click on the sessions tab and enter details for the new session. You will see a warning like below:

image:image123.png[A screenshot of a computer Description automatically generated,width=566,height=402]

[arabic, start=16]
. Before starting any new session in the recently created Project, you must complete the hadoop authentication part as the cluster setup is kerberized.

So, to be able to access data from Hadoop clusters *_go to user > user settings > Hadoop authentication._

Open a *_new tab in the same browser_* window, by duplicating the existing tab. Go to the *_CML home page_* and click on *_User Settings_* in the left pane. Click on *_Hadoop Authentication_*.

image:image147.png[hadoop authentication,width=485,height=333]

* Enter *_Principal_* e.g. link:about:blank[[.underline]#username@DOMAIN.LOCAL#] i.e. *_admin@CLDRSETUP.LOCAL_
* Under the *_Credentials_* and password (i.e. *_vmware123_*) or keytab details of the LDAP user and click on Authenticate.
* Once the authentication is successful, proceed to the next step. You will see the output similar to below screenshot, after the successful authentication and integration to Kerberized Hadoop Cluster.

image:image109.png[A screenshot of a computer Description automatically generated,width=411,height=267]

[arabic, start=17]
. Now, to explore the CML IDEs, click on the newly created workspace. It will open the Projects screen of CML. Go to the *_Project page_* and click on the newly created project and then click on the *_New Session_* button on the top right. Explore CML by running the jobs with different IDEs like Jupyterlab and Workbench.

image:image62.png[click on workspc,width=504,height=83]

image:image178.png[new created project,width=365,height=211]

[arabic, start=18]
. Go to *_Site Administration_* to edit *_Resource profile_* and *_GPU per session/ Job_*.

image:image47.png[A screenshot of a computer Description automatically generated,width=616,height=341]

[arabic, start=19]
. Go to the *_AMPs_* tab to get started with pre-built models.

image:image101.png[A screenshot of a social media page Description automatically generated,width=628,height=208]

[arabic, start=20]
. Select *_AMP_* and click on *_Configure Project_*.
. After editing the *_Runtime field_* for the new project, click on *_Launch Project._

image:image176.png[A screenshot of a computer Description automatically generated,width=611,height=428]

[arabic, start=22]
. *_Intelligent QA Chatbot with NiFi, Pinecone, and Llama2 – AMP_* project overview.

image:image136.png[A screenshot of a computer Description automatically generated,width=612,height=343]

[arabic, start=23]
. Create a new session by *_Start A New Session_* with desired resources, editor, kernel, and number of GPUs.

image:image94.png[A screenshot of a computer Description automatically generated,width=636,height=469]

[arabic, start=24]
. *_Jupyter notebook_* session in *_CML_*.

image:image186.png[A screenshot of a computer Description automatically generated,width=674,height=278]

[arabic, start=25]
. Open *_AMP_* created project. To access *_WebUI_* click on *_Open_*.

image:image181.png[A screenshot of a computer Description automatically generated,width=679,height=385]

[arabic, start=26]
. WebUI for *_Llama2 based chatbot_* with pre-trained data is now available. *_Change settings_* on the WebUI or use them out of the box. For example, we questioned “*_what is VMware Platform?_*”.

image:image33.png[A screenshot of a chatbot Description automatically generated,width=688,height=301]

[arabic, start=27]
. *_Access HDFS data_* from the *_jupyter notebook session_* in CML.

image:image131.png[A screenshot of a computer Description automatically generated,width=681,height=304]

*Note*: Deploying and documentation of every aspect of *_CML workspace, project, and user management_* is not covered here. Please refer to the related *_Cloudera documentation_* on *_Cloudera Machine Learning How to_* *_section_* for more details:

https://docs.cloudera.com/machine-learning/cloud/product/topics/ml-product-overview.html[[.underline]#https://docs.cloudera.com/machine-learning/cloud/product/topics/ml-product-overview.html#]

++**************************************************************************************************************++

=== [.underline]#CDP Private Cloud Data Warehouse (CDW)#

https://docs.cloudera.com/data-warehouse/1.5.4/private-cloud-getting-started/topics/dw-private-cloud-create-virtual-warehouse-openshift-overview.html[[.underline]#https://docs.cloudera.com/data-warehouse/1.5.4/private-cloud-getting-started/topics/dw-private-cloud-create-virtual-warehouse-openshift-overview.html#]

=== Enable CDW environment and creation of Database Catalog

* Open *_CDP Data Services_* page.
* Click on *_Data Warehouse_*.

image:image198.png[select CDW,width=285,height=235]

* On the *_Overview_* page, click on the *_Activate_* icon as shown below.

image:image93.png[activate cdw,width=324,height=199]

* On the *_Activate Environment_* page, enter the *_LDAP username and password_*. Enable *_Low resource mode_* and click on *_Activate_*.

image:image161.png[low res mode,width=498,height=157]

=== Create Virtual Warehouse

* Once the *_environment_* is *_activated_*, a *_default Database Catalog_* gets created automatically.

image:image78.png[default dbc,width=379,height=214]

* Once the *_database catalog_* is created, click on *_+_* icon next to *_Virtual Warehouses_*.

image:image102.png[crt vwh,width=426,height=126]

* A *_New Virtual Warehouse_* tab appears on the same page.
* Enter the *_name_* for the **_new virtual warehouse(_**VW).
* Choose the *_type_* of VW, i.e. *_Hive_* or *_Impala_*.
* Choose the *_default Database catalog_* that appears in the dropdown.
* Choose *_Size_* as *_xsmall-2 Executors_*.
* *_AutoSuspend_*: If you want the VW to keep running all the time, you can *_Disable_* it.
* Keep the remaining parameters *_default_* and click on *_Create_*.

image:image118.png[crt wh,width=370,height=531]

* A new *_Virtual Warehouse_* will be created. You can use *_Hue_* to submit queries to the underlying engine of the Virtual Warehouse.

image:image164.png[vw hue,width=512,height=176]

++**************************************************************************************************************++

=== [.underline]#CDP Private Cloud Data Engineering (CDE)#

https://docs.cloudera.com/data-engineering/1.5.4/enable-data-engineering/topics/cde-private-cloud-add-cde-service.html[[.underline]#https://docs.cloudera.com/data-engineering/1.5.4/enable-data-engineering/topics/cde-private-cloud-add-cde-service.html#]

=== CDP Base cluster requirements:

The *_Cloudera Data Engineering (CDE)_* service requires proper configuration of *_Ozone_* service in the Base cluster. Ensure that *_Ozone_* is running properly otherwise you will end up with issues while enabling CDE.

=== Enabling CDE Service:

* From the *_CDP console page_*, click on *_Data Engineering_*.

image:image73.png[open cde,width=305,height=268]

* This will open the *_CDE home page_*. Since this will be the first time you will be opening CDE, you will not see any virtual clusters. Click on *_Administration_* in the left pane.

image:image49.png[cde home,width=550,height=186]

* Click on *_+_* icon as shown below which will allow you to *_enable CDE service_* post which you can *_create Virtual Clusters_*.

image:image135.png[enable cde,width=99,height=60]

* On the *_Enable a Service_* page, enter the *_values_* as shown below and then click on *_Enable_*.

image:image29.png[enable cde config,width=297,height=381]

Please note that the cpu and memory config chosen here are *_minimum values_*. You can choose to increase it.

* This will take *_approximately 30 mins_* after which you will be able to see a *_CDE service_* on the *_CDE_* home page.

image:image211.png[running cde svc,width=407,height=240]

(_default *CDE* is the name given as an example. You will see as per the value you entered in the previous step_.)

* The *_CDE Home page_* displays the status of the *_CDE_* service initialization. You can view logs for the service by clicking on the service *_vertical ellipsis (three dots) menu_*, and then clicking *_View Logs_*.

https://docs.cloudera.com/management-console/1.5.4/private-cloud-environments/topics/mc-private-cloud-environment-register-ui.html[[.underline]#https://docs.cloudera.com/management-console/1.5.4/private-cloud-environments/topics/mc-private-cloud-environment-register-ui.html#]

If you are unable to see the service, then the chances are that the default virtual cluster would not have been created properly. In this case, click on the *_View Services_* button and then you will be able to see the *_CDE_* service *_enabled_*.

image:image20.png[cde svc,width=571,height=185]

=== Create Virtual Cluster:

When you *_enable CDE service_*, by default a new *_Virtual cluster with Spark2.4_* will be created. If you have not enabled this option earlier, then you need to create a virtual cluster again.

* On the *_CDE Home page_*, click on the *_+_* icon next to *_Virtual clusters_* as shown below.

image:image166.png[crt vc,width=624,height=46]

* On the *_Create a Virtual Cluster_* page, enter the below *_values_* and click on *_Create_*.
* *_Cluster Name:_* Cluster Name should adhere to the below conditions.
** Begin with a letter
** Be between 3 and 30 characters (inclusive)
** Contain only alphanumeric characters and hyphens
* *_Service_*: Select the CDE service created earlier.
* *_Spark_* *_Version_*: Select the Spark version as per your requirement. If you need both *_Spark2.4_* and *_Spark3.7_*, you can create two virtual clusters provided you have sufficient resources.

image:image165.png[crt vc choose spark,width=433,height=235]

This will take approximately 20 minutes.

* You can check the logs of the cluster creation by clicking on the *_pencil_* icon and selecting the *_Logs_* section on the cluster page as shown below.

image:image141.png[vc clust details,width=483,height=197]

image:image91.png[vc clust logs,width=488,height=168]

=== [.underline]#Initializing Virtual Cluster#

Every time a *_new virtual cluster_* is created, there are a few *_manual steps_* that must be performed.

* Log in to the *_ECS master_* and run the next set of *_commands_* as per the instructions.
* Run the below command to create a temporary directory and navigate to the same.

[root@pvcecs-master ~]# mkdir -p /tmp/cde-latest && cd /tmp/cde-latest

* Download the script https://docs.cloudera.com/data-engineering/1.5.0/cdp-cde-utils.sh[[.underline]#cdp-cde-utils#] using wget.

[root@pvcecs-master ~]# wget https://docs.cloudera.com/data-engineering/1.5.4/cdp-cde-utils.sh

* Add execute permission to this script.

[root@pvcecs-master ~]# chmod +x /tmp/cde-latest/cdp-cde-utils.sh

* Identify the *_virtual cluster endpoint_*:

On the *_CDE homepage_*, select the *_CDE_* *_service_* in which the *_virtual_* *_cluster_* is created. Click on the *_pencil_* icon on the virtual cluster to be configured.

image:image206.png[cde to vc details,width=512,height=108]

* Click JOBS API URL to copy the URL to your clipboard.

image:image24.png[vc jobs api,width=624,height=157]

* Paste the URL into a text editor to identify the endpoint host. For example, if the URL is similar to the following:

*http://dfdj6kgx.cde-2cdxw5x5.ecs-demo.example.com/dex/api/v1

Then the endpoint will then be as shown below.

*dfdj6kgx.cde-2cdxw5x5.ecs-demo.example.com

* Once you get the endpoint of the virtual cluster, *_login to the ECS master_* and navigate to *_/tmp/cde-latest_* directory where the *_cdp-cde-utils.sh_* script is present.

[root@pvcecs-master ~]# cd /tmp/cde-latest

* Generate a self-signed certificate with the below command. Replace the endpoint_host with the endpoint of your virtual cluster that you got from the previous step.

[root@pvcecs-master ~]# ./cdp-cde-utils.sh init-virtual-cluster -h <endpoint_host> -a

For the example host we used above, this command will be as below.

[root@pvcecs-master ~]# ./cdp-cde-utils.sh init-virtual-cluster -h dfdj6kgx.cde-2cdxw5x5.ecs-demo.example.com -a

* These steps must be performed for each virtual cluster you create.

=== [.underline]#Configuring LDAP Users on CDE#

This step is required to submit the jobs to CDE from the LDAP users.

* Log in to the ECS master host and navigate to the directory /tmp/cde-latest.

[root@pvcecs-master ~]# cd /tmp/cde-latest

* Install krb5-workstation package using dnf.

[root@pvcecs-master ~]# dnf install krb5-workstation krb5-libs -y

* Create a file named *_<username>.principal_* containing the user principal. As an example, we will consider admin as the username. Here *_CLDRSETUP.LOCAL_* is the realm provided during IPA setup. You need to replace it with the realm you configured.

[root@pvcecs-master ~]# cat>> admin.principal

*cdpuser@CLDRSETUP.LOCAL

* Generate a keytab named *_<username>.keytab_* for the user using *_ktutil_*:

[root@pvcecs-master ~]# cat>> admin.keytab

[root@pvcecs-master ~]# sudo ktutil

ktutil: addent -password -p admin@CLDRSETUP.LOCAL -k 1 -e aes256-cts

Password for admin@CLDRSETUP.LOCAL:

ktutil: addent -password -p admin@CLDRSETUP.LOCAL -k 2 -e aes128-cts

Password for admin@CLDRSETUP.LOCAL:

ktutil: wkt admin.keytab

ktutil: q

* Validate the keytab using *_klist_*. This command should use the principals created with two encryptions provided above, namely aes256-cts and aes128-cts.

[root@pvcecs-master ~]# klist -ekt admin.keytab

* Validate the *_keytab_* using *_kinit_*. This command should get executed successfully.

[root@pvcecs-master ~]# kinit -kt admin.keytab admin@CLDRSETUP.LOCAL

* Make sure that the *_keytab_* is valid before continuing. If the *_kinit_* command fails, the user will not be able to run jobs in the *_virtual cluster_*. After verifying that the *_kinit_* command succeeds, you can *_destroy_* the Kerberos ticket by running *_kdestroy_*.
* Use the *_cdp-cde-utils.sh_* script to copy the user *_keytab_* to the virtual cluster hosts.

[root@pvcecs-master ~]# ./cdp-cde-utils.sh init-user-in-virtual-cluster -h <endpoint_host> -u <user> -p <principal_file> -k <keytab_file>

For the above example, the command would be below.

[root@pvcecs-master ~]# ./cdp-cde-utils.sh init-user-in-virtual-cluster -h dfdj6kgx.cde-2cdxw5x5.ecs-demo.example.com -u cdpuser -p cdpuser.principal -k cdpuser.keytab

* Repeat these steps for all users that need to submit jobs to the virtual cluster.

++**************************************************************************************************************++

=== Appendix

This appendix contains the following:

=== Appendix A – References Used in Guide 

*Cloudera Private Cloud Base Getting Started Guide:

https://docs.cloudera.com/cdp-private-cloud/latest/index.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud/latest/index.html#]

*Cloudera Private Cloud Data Services Getting Started Guide:

https://docs.cloudera.com/cdp-private-cloud-data-services/latest/index.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/latest/index.html#]

*CDP Private Cloud Machine Learning Overview:

https://docs.cloudera.com/machine-learning/1.5.4/index.html[[.underline]#https://docs.cloudera.com/machine-learning/1.5.4/index.html#]

*CDP Private Cloud Data Engineering Overview:

https://docs.cloudera.com/data-engineering/1.5.4/index.html[[.underline]#https://docs.cloudera.com/data-engineering/1.5.4/index.html#]

*CDP Private Cloud Data Warehouse Overview:

https://docs.cloudera.com/data-warehouse/1.5.4/index.html[[.underline]#https://docs.cloudera.com/data-warehouse/1.5.4/index.html#]

=== Appendix B – Glossary of Terms

This glossary addresses some terms used in this document, for the purposes of aiding understanding. This is not a complete list of all multi cloud terminology. Some VMware product links are supplied here also, where considered useful for the purposes of clarity, but this is by no means intended to be a complete list of all applicable VMware products.

[width="100%",cols="30%,70%",options="header",]
|===
a|
____
*Ansible
____

a|
____
An infrastructure automation tool, used to implement processes for instantiating and configuring IT service components, such as VMs on an IaaS platform. Supports the consistent execution of processes defined in YAML “playbooks” at scale, across multiple targets. Because the Ansible artifacts (playbooks) are text-based, they can be stored in a Source Code Management (SCM) system, such as GitHub. This allows for software development like processes to be applied to infrastructure automation, such as, Infrastructure-as-code (see IaC below).

https://www.ansible.com[[.underline]#https://www.ansible.com#]
____

a|
____
*AWS (Amazon Web Services)
____

a|
____
Provider of IaaS and PaaS.

https://aws.amazon.com[[.underline]#https://aws.amazon.com#]
____

a|
____
*Azure
____

a|
____
Microsoft IaaS and PaaS.

https://azure.microsoft.com/en-gb/[[.underline]#https://azure.microsoft.com/en-gb/#]
____

a|
____
*Containers (Docker)
____

a|
____
A (Docker) container is a means to create a package of code for an application and its dependencies, such that the application can run on different platforms which support the Docker environment. In the context of aaS, microservices are typically packaged within Linux containers orchestrated by Kubernetes (K8s).

https://www.docker.com[[.underline]#https://www.docker.com#]

https://www.cisco.com/c/en/us/products/cloud-systems-management/containerplatform/index.html[[.underline]#https://www.cisco.com/c/en/us/products/cloud-systems-management/containerplatform/index.html#]
____

a|
____
*DevOps
____

a|
____
The underlying principle of DevOps is that the application development and operations teams should work closely together, ideally within the context of a toolchain that automates the stages of development, test, deployment, monitoring, and issue handling. DevOps is closely aligned with IaC, continuous integration and deployment (CI/CD), and Agile software development practices.

https://en.wikipedia.org/wiki/DevOps[[.underline]#https://en.wikipedia.org/wiki/DevOps#]

https://en.wikipedia.org/wiki/CI/CD[[.underline]#https://en.wikipedia.org/wiki/CI/CD#]
____

a|
____
*IaaS (Infrastructure as-a-Service)
____

a|
____
Infrastructure components provided aaS, located in data centers operated by a provider, typically accessed over the public Internet. IaaS provides a base platform for the deployment of workloads, typically with containers and Kubernetes (K8s).
____

a|
____
*IaC (Infrastructure as-Code)
____

a|
____
Given the ability to automate aaS via APIs, the implementation of the automation is typically via Python code, Ansible playbooks, and similar. These automation artifacts are programming code that define how the services are consumed. As such, they can be subject to the same code management and software development regimes as any other body of code. This means that infrastructure automation can be subject to all of the quality and consistency benefits, CI/CD, traceability, automated testing, compliance checking, and so on, that could be applied to any coding project.

https://en.wikipedia.org/wiki/Infrastructure_as_code[[.underline]#https://en.wikipedia.org/wiki/Infrastructure_as_code#]
____

a|
____
*IAM (Identity and Access Management)
____

a|
____
IAM is the means to control access to IT resources so that only those explicitly authorized to access given resources can do so. IAM is an essential foundation to a secure multi cloud environment.

[.underline]#https://en.wikipedia.org/wiki/Identity_management#
____

a|
____
*GCP

*(Google Cloud Platform)
____

a|
____
Google IaaS and PaaS.

https://cloud.google.com/gcp[[.underline]#https://cloud.google.com/gcp#]
____

a|
____
*Kubernetes

*(K8s)
____

a|
____
Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.

https://kubernetes.io[[.underline]#https://kubernetes.io#]
____

a|
____
*Microservices
____

a|
____
A microservices architecture is characterized by processes implementing fine-grained services, typically exposed via REST APIs and which can be composed into systems. The processes are often container-based, and the instantiation of the services is often managed with Kubernetes. Microservices managed in this way are intrinsically well suited for deployment into IaaS environments, and as such, are the basis of a cloud native architecture.

https://en.wikipedia.org/wiki/Microservices[[.underline]#https://en.wikipedia.org/wiki/Microservices#]
____

a|
____
*PaaS

*(Platform-as-a-Service)
____

a|
____
PaaS is a layer of value-add services, typically for application development, deployment, monitoring, and general lifecycle management. The use of IaC with IaaS and PaaS is very closely associated with DevOps practices.
____

a|
____
*Private on-premises data center
____

a|
____
A data center infrastructure housed within an environment owned by a given enterprise is distinguished from other forms of data center, with the implication that the private data center is more secure, given that access is restricted to those authorized by the enterprise. Thus, circumstances can arise where very sensitive IT assets are only deployed in a private data center, in contrast to using public IaaS. For many intents and purposes, the underlying technology can be identical, allowing for hybrid deployments where some IT assets are privately deployed but also accessible to other assets in public IaaS. IAM, VPNs, firewalls, and similar are key technologies needed to underpin the security of such an arrangement.
____

a|
____
*REST API
____

a|
____
Representational State Transfer (REST) APIs is a generic term for APIs accessed over HTTP(S), typically transporting data encoded in JSON or XML. REST APIs have the advantage that they support distributed systems, communicating over HTTP, which is a well-understood protocol from a security management perspective. REST APIs are another element of a cloud-native applications architecture, alongside microservices.

https://en.wikipedia.org/wiki/Representational_state_transfer[[.underline]#https://en.wikipedia.org/wiki/Representational_state_transfer#]
____

a|
____
*SaaS

*(Software-as-a-Service)
____

a|
____
End-user applications provided “aaS” over the public Internet, with the underlying software systems and infrastructure owned and managed by the provider.
____

a|
____
*SAML

*(Security Assertion Markup Language)
____

a|
____
Used in the context of Single-Sign-On (SSO) for exchanging authentication and authorization data between an identity provider, typically an IAM system, and a service provider (some form of SaaS). The SAML protocol exchanges XML documents that contain security assertions used by the aaS for access control decisions.

https://en.wikipedia.org/wiki/Security_Assertion_Markup_Language[[.underline]#https://en.wikipedia.org/wiki/Security_Assertion_Markup_Language#]
____

a|
____
*Terraform
____

a|
____
An open-source IaC software tool for cloud services, based on declarative configuration files. https://www.terraform.io[[.underline]#https://www.terraform.io#]
____

|===

=== Appendix C – Glossary of Acronyms

*ACL*—Access-Control List

*AD*—Microsoft Active Directory

*API*—Application Programming Interface

*CDP* – Cloudera Data Platform

*CDP* *Private Cloud* – Cloudera Data Platform Private Cloud

*CDP* *Private Cloud* *DS* – Cloudera Data Platform Private Cloud Data Services

*CDW* – Cloudera Data Warehouse

*CML* – Cloudera Machine Learning

*CDE* – Cloudera Data Engineering

*CPU*—Central Processing Unit

*DC*—Data Center

*DHCP*—Dynamic Host Configuration Protocol

*DNS*—Domain Name System

*HA*—High-Availability

*ICMP*— Internet Control Message Protocol

*LAN*—Local Area Network

*MAC*—Media Access Control Address (OSI Layer 2 Address)

*MTU*—Maximum Transmission Unit

*NAT*—Network Address Translation

*OSI*—Open Systems Interconnection model

*RHEL* – Red Hat Enterprise Linux

*Syslog*—System Logging Protocol

*TCP*—Transmission Control Protocol (OSI Layer 4)

*UDP*—User Datagram Protocol (OSI Layer 4)

*URL*—Uniform Resource Locator

*VM*—Virtual Machine

*VPN*—Virtual Private Network

Cloudera Data Platform Private Cloud latest release note, go to: [.underline]#https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/release-guide/topics/cdpdc-release-notes-links.html#

Cloudera Data Platform Private Cloud Base Requirements and Supported Versions, go to: [.underline]#https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/release-guide/topics/cdpdc-requirements-supported-versions.html#

Cloudera Data Platform Private Cloud Data Services installation on Embedded Container Service requirements and supported versions, go to: [.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/index.html#

++**************************************************************************************************************++

=== FreeIPA Reference

https://www.devopszones.com/2020/03/how-to-add-freeipa-user-in-cli-and-web.html[[.underline]#https://www.devopszones.com/2020/03/how-to-add-freeipa-user-in-cli-and-web.html#]

=== Add users on FreeIPA

* Log in to the IPA server and run kinit with admin and enter the password: *_kinit admin_

* Run the below command to create a user. Enter the password in the password prompt.

*_ipa user-add --password <User Name>_

[root@pvcbasemaster ~]# ipa

ipa: ERROR: Ticket expired

[root@pvcbasemaster ~]# kinit admin && klist -e

Password for admin@CLDRSETUP.LOCAL:

[root@pvcbasemaster ~]# ipa user-add --password kdsahu

First name: Kuldeep

Last name: Sahu

User login [kdsahu]: kdsahu

Password:

Enter Password again to verify:

-------------------

Added user "kdsahu"

-------------------

User login: kdsahu

First name: Kuldeep

Last name: Sahu

Full name: Kuldeep Sahu

Display name: Kuldeep Sahu

Initials: KS

Home directory: /home/kdsahu

GECOS: Kuldeep Sahu

Login shell: /bin/sh

Principal name: kdsahu@CLDRSETUP.LOCAL

Principal alias: kdsahu@CLDRSETUP.LOCAL

User password expiration: 20240321113054Z

Email address: kdsahu@cldrsetup.local

UID: 971200008

GID: 971200008

Password: True

Member of groups: ipausers

Kerberos keys available: True

[root@pvcbasemaster ~]# su - kdsahu

Last login: Thu Mar 21 01:00:40 PDT 2024 on pts/0

++**************************************************************************************************************++-----------------------------

[.underline]#Free-IPA Command Reference:#

*kinit admin

*ipa dnsrecord-add 16.172.in-addr.arpa. 231.31 --ptr-rec console-cdp.apps.pvcecsmaster.cldrsetup.local.

*ipa dnsrecord-del 16.172.in-addr.arpa. 231.31 --ptr-rec console-cdp.apps.pvcecsmaster.cldrsetup.local

*ipa dnsrecord-find 16.172.in-addr.arpa.

*ipa dnsrecord-add cldrsetup.local *.apps

*ipa dnszone-list

*ipa user-del cmadmin

*ipa user-show

*ipa status | start | stop

*ipactl status | stop | start | restart

++**************************************************************************************************************++-----------------------------

[kuldeep@pvcbasemaster ~]$ kinit kdsahu

Password for kdsahu@CLDRSETUP.LOCAL:

Password expired. You must change it now.

Enter new password:

Enter it again:

[kuldeep@pvcbasemaster ~]$

++**************************************************************************************************************++-----------------------------

[root@ipaserver ~]# ldapsearch -H ldap://ipaserver.cldrsetup.local:389 -D "uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local" -w 'vmware123' -b "cn=users,cn=accounts,dc=cldrsetup,dc=local" '(&(uid=admin))' | grep -v "#"

dn: uid=admin,cn=users,cn=accounts,dc=cldrsetup,dc=local

objectClass: top

objectClass: person

objectClass: posixaccount

objectClass: krbprincipalaux

objectClass: krbticketpolicyaux

objectClass: inetuser

objectClass: ipaobject

objectClass: ipasshuser

objectClass: ipaSshGroupOfPubKeys

uid: admin

krbPrincipalName: admin@CLDRSETUP.LOCAL

cn: Administrator

sn: Administrator

uidNumber: 971200000

gidNumber: 971200000

homeDirectory: /home/admin

loginShell: /bin/bash

gecos: Administrator

ipaUniqueID: e42d6b54-e094-11ee-9c71-0050568db389

memberOf: cn=admins,cn=groups,cn=accounts,dc=cldrsetup,dc=local

memberOf: cn=Replication Administrators,cn=privileges,cn=pbac,dc=cldrsetup,dc=c

om

memberOf: cn=Add Replication Agreements,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=Read Replication Agreements,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=Modify DNA Range,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=Read LDBM Database Configuration,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=Host Enrollment,cn=privileges,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=System: Add krbPrincipalName to a Host,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=System: Enroll a Host,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=System: Manage Host Enrollment Password,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=System: Manage Host Keytab,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=System: Manage Host Principals,cn=permissions,cn=pbac,dc=cldrsetup,dc=local

memberOf: cn=trust admins,cn=groups,cn=accounts,dc=cldrsetup,dc=local

krbLastPwdChange: 20240312172405Z

krbPasswordExpiration: 20240610172405Z

krbExtraData:: AAK1j/Blcm9vdC9hZG1pbkBDRFBQVkNEUy5DT00A

krbLoginFailedCount: 0

krbLastFailedAuth: 20240325064805Z

search: 2

result: 0 Success

[root@ipaserver ~]#

++**************************************************************************************************************++-----------------------------

[root@pvcecsmaster ~]# ipa dnsrecord-find 16.172.in-addr.arpa.

Record name: @

NS record: ipaserver.cldrsetup.local.

Record name: 226.31

PTR record: ipaserver.cldrsetup.local.

Record name: 227.31

PTR record: pvcbase-master.cldrsetup.local.

Record name: 228.31

PTR record: pvcbase-worker1.cldrsetup.local.

Record name: 231.31

PTR record: pvcecs-master.cldrsetup.local.

Record name: 232.31

PTR record: pvcecs-worker1.cldrsetup.local.

-----------------------------

Number of entries returned 12

-----------------------------

++**************************************************************************************************************++

=== Perform the PvC Base Cluster Validation:

https://training-team.gitbook.io/setting-up-cloudera-data-platform-cdp/hive-validation[[.underline]#https://training-team.gitbook.io/setting-up-cloudera-data-platform-cdp/hive-validation#]

https://www.quora.com/How-do-you-load-data-into-a-Hive-external-table[[.underline]#https://www.quora.com/How-do-you-load-data-into-a-Hive-external-table#]

https://stackoverflow.com/questions/17425492/hive-insert-query-like-sql[[.underline]#https://stackoverflow.com/questions/17425492/hive-insert-query-like-sql#]

https://github.com/mionisation/BI_BigData_2_HiveDatasetAnalysis/blob/master/createMovieLensTables.hql[[.underline]#https://github.com/mionisation/BI_BigData_2_HiveDatasetAnalysis/blob/master/createMovieLensTables.hql#]

https://grouplens.org/datasets/movielens/20m/[[.underline]#https://grouplens.org/datasets/movielens/20m/#]

##### Validation:

[root@pvcbase-master ~]# dnf install -y wget unzip

[root@pvcbase-master ~]# wget https://files.grouplens.org/datasets/movielens/ml-20m.zip

[root@pvcbase-master ~]# unzip ml-20m.zip

[root@pvcbase-master ~]# cd ml-20m

[root@pvcbase-master ml-20m]# sed -i 1d 

[root@pvcbase-master ml-20m]#

[root@pvcbase-master ml-20m]# hdfs dfs -ls /

24/03/21 04:32:28 WARN ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

ls: DestHost:destPort pvcbasemaster.cldrsetup.local:8020 , LocalHost:localPort pvcbasemaster.cldrsetup.local/172.16.31.227:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

[root@pvcbase-master ml-20m]#

[root@pvcbase-master ml-20m]# find / -name hdfs.keytab

[root@pvcbase-master ml-20m]#

[root@pvcbase-master ml-20m]# klist -kt /run/cloudera-scm-agent/process/1546343796-hdfs-NAMENODE/hdfs.keytab +
kinit -kt /run/cloudera-scm-agent/process/1546343796-hdfs-NAMENODE/hdfs.keytab

[root@pvcbase-master ml-20m]#

[root@pvcbase-master ml-20m]# hdfs/pvcbasemaster.cldrsetup.local@CLDRSETUP.LOCAL

[root@pvcbase-master ml-20m]# klist -kt /run/cloudera-scm-agent/process/1546343796-hdfs-NAMENODE/hdfs.keytab

Keytab name: FILE:/run/cloudera-scm-agent/process/1546343796-hdfs-NAMENODE/hdfs.keytab

KVNO Timestamp Principal

---- ------------------- ------------------------------------------------------

3 03/17/2024 12:33:40 HTTP/pvcbasemaster.cldrsetup.local@CLDRSETUP.LOCAL

3 03/17/2024 12:33:40 hdfs/pvcbasemaster.cldrsetup.local@CLDRSETUP.LOCAL

[root@pvcbase-master ml-20m]# klist

Ticket cache: FILE:/tmp/krb5cc_0

Default principal: hdfs/pvcbasemaster.cldrsetup.local@CLDRSETUP.LOCAL

Valid starting Expires Service principal

03/21/2024 22:50:18 03/22/2024 22:50:18 krbtgt/CLDRSETUP.LOCAL@CLDRSETUP.LOCAL

renew until 03/28/2024 22:50:18

[root@pvcbase-master ml-20m]# hdfs dfs -mkdir /tmp/movielens

[root@pvcbase-master ml-20m]# hdfs dfs -put * /tmp/movielens/

[root@pvcbase-master ml-20m]# hdfs dfs -chown -R hive:supergroup /tmp/movielens

[root@pvcbase-master ml-20m]# hdfs dfs -ls /tmp/movielens/

[root@pvcbase-master ml-20m]# hive

CREATE DATABASE movielens;

use movielens;

CREATE TABLE IF NOT EXISTS ratings ( userId int, movieId int, rating double, ts bigint)

COMMENT "Movie Ratings"

ROW FORMAT DELIMITED

FIELDS TERMINATED BY '\054'

LINES TERMINATED BY '\n'

STORED AS TEXTFILE;

LOAD DATA INPATH '/tmp/movielens/movies.csv' overwrite INTO TABLE movies;

LOAD DATA INPATH '/tmp/movielens/tags.csv' overwrite INTO TABLE tags;

LOAD DATA INPATH '/tmp/movielens/ratings.csv' overwrite INTO TABLE ratings;

LOAD DATA INPATH '/tmp/movielens/genome-tags.csv' overwrite INTO TABLE genome_tags;

LOAD DATA INPATH '/tmp/movielens/genome-scores.csv' overwrite INTO TABLE genome_scores;

*Run the queries from HUE for create db, create table.

*Upload data from Hive cli.

*Run select query to fetch operations from HUE.

++**************************************************************************************************************++

[.underline]#OZONE Validation:#

[root@pvcbase-master ~]# ozone sh bucket list ozone11

24/05/26 07:32:42 WARN ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

24/05/26 07:32:42 WARN ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

24/05/26 07:32:42 WARN ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

24/05/26 07:32:42 ERROR client.OzoneClientFactory: Couldn't create RpcClient protocol exception:

... 42 more

org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

[root@pvcbase-master ~]# klist -e

Ticket cache: FILE:/tmp/krb5cc_0

Default principal: admin@CLDRSETUP.LOCAL

Valid starting Expires Service principal

05/15/2024 21:24:12 05/16/2024 20:35:44 krbtgt/CLDRSETUP.LOCAL@CLDRSETUP.LOCAL

renew until 05/22/2024 21:24:09, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha384-192

[root@pvcbase-master ~]# find / -name ozone.keytab

/run/cloudera-scm-agent/process/1546347521-ozone-S3_GATEWAY/ozone.keytab

/run/cloudera-scm-agent/process/1546347511-ozone-OZONE_RECON/ozone.keytab

/run/cloudera-scm-agent/process/1546347517-ozone-STORAGE_CONTAINER_MANAGER/ozone.keytab

/run/cloudera-scm-agent/process/1546347273-ozone-STORAGE_CONTAINER_MANAGER/ozone.keytab

/run/cloudera-scm-agent/process/1546347267-ozone-OZONE_RECON/ozone.keytab

/run/cloudera-scm-agent/process/1546347277-ozone-S3_GATEWAY/ozone.keytab

/run/cloudera-scm-agent/process/1546344328-ozone-OZONE_RECON/ozone.keytab

/run/cloudera-scm-agent/process/1546344334-ozone-STORAGE_CONTAINER_MANAGER/ozone.keytab

/run/cloudera-scm-agent/process/1546344338-ozone-S3_GATEWAY/ozone.keytab

/run/cloudera-scm-agent/process/1546344034-ozone-STORAGE_CONTAINER_MANAGER/ozone.keytab

/run/cloudera-scm-agent/process/1546344028-ozone-OZONE_RECON/ozone.keytab

/run/cloudera-scm-agent/process/1546344038-ozone-S3_GATEWAY/ozone.keytab

[root@pvcbase-master ~]# klist -kt /run/cloudera-scm-agent/process/1546344038-ozone-S3_GATEWAY/ozone.keytab

Keytab name: FILE:/run/cloudera-scm-agent/process/1546344038-ozone-S3_GATEWAY/ozone.keytab

KVNO Timestamp Principal

---- ------------------- ------------------------------------------------------

2 05/19/2024 22:11:49 HTTP/pvcbase-master.cldrsetup.local@CLDRSETUP.LOCAL

2 05/19/2024 22:11:49 s3g/pvcbase-master.cldrsetup.local@CLDRSETUP.LOCAL

[root@pvcbase-master ~]# kinit -kt /run/cloudera-scm-agent/process/1546344038-ozone-S3_GATEWAY/ozone.keytab s3g/pvcbase-master.cldrsetup.local@CLDRSETUP.LOCAL

[root@pvcbase-master ~]# klist -kt* /run/cloudera-scm-agent/process/1546344038-ozone-S3_GATEWAY/ozone.keytabKeytab name: FILE:/run/cloudera-scm-agent/process/1546344038-ozone-S3_GATEWAY/ozone.keytab

KVNO Timestamp Principal

---- ------------------- ------------------------------------------------------

2 05/19/2024 22:11:49 HTTP/pvcbase-master.cldrsetup.local@CLDRSETUP.LOCAL

2 05/19/2024 22:11:49 s3g/pvcbase-master.cldrsetup.local@CLDRSETUP.LOCAL

[root@pvcbase-master ~]# klist

Ticket cache: FILE:/tmp/krb5cc_0

Default principal: s3g/pvcbase-master.cldrsetup.local@CLDRSETUP.LOCAL

Valid starting Expires Service principal

05/26/2024 07:36:13 05/27/2024 07:11:26 krbtgt/CLDRSETUP.LOCAL@CLDRSETUP.LOCAL

renew until 06/02/2024 07:36:13

==============================================

[root@pvcbase-master ~]# ozone sh volume list

[ ]

[root@pvcbase-master ~]# ozone sh volume create ozone11

24/05/26 07:46:20 INFO rpc.RpcClient: Creating Volume: ozone11, with s3g as owner and space quota set to -1 bytes, counts quota set to -1

[root@pvcbase-master ~]# ozone sh volume list

[ {

"metadata" : { },

"name" : "*ozone11*",

"admin" : "s3g",

"owner" : "s3g",

"quotaInBytes" : -1,

"quotaInNamespace" : -1,

"usedNamespace" : 0,

"creationTime" : "2024-05-26T14:46:20.912Z",

"modificationTime" : "2024-05-26T14:46:20.912Z",

"acls" : [ {

"type" : "USER",

"name" : "s3g",

"aclScope" : "ACCESS",

"aclList" : [ "ALL" ]

} ],

"refCount" : 0

} ]

[root@pvcbase-master ~]# ozone sh bucket create ozone11/testkdbkt1

24/05/26 07:47:19 INFO rpc.RpcClient: Creating Bucket: ozone11/testkdbkt1, with server-side default bucket layout, s3g as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1

[root@pvcbase-master ~]# ozone sh bucket list ozone11

[ {

"metadata" : { },

"volumeName" : "ozone11",

"name" : "testkdbkt1",

"storageType" : "DISK",

"versioning" : false,

"usedBytes" : 0,

"usedNamespace" : 0,

"creationTime" : "2024-05-26T14:47:19.692Z",

"modificationTime" : "2024-05-26T14:47:19.692Z",

"sourcePathExist" : true,

"quotaInBytes" : -1,

"quotaInNamespace" : -1,

"bucketLayout" : "FILE_SYSTEM_OPTIMIZED",

"owner" : "s3g",

"link" : false

} ]

[root@pvcbase-master ~]#

++**************************************************************************************************************++

=== Cleanup CDP PvC Base Cluster:

https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-uninstallation.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/installation/topics/cdpdc-uninstallation.html#]

[.underline]#UnInstall and Cleanup Steps (If Installation fails and not-able to resolve the issues)#

[.underline]#Stop all Services#

[.underline]#Delete the Cluster#

On the Home page, Click the drop-down list next to the cluster you want to delete and select Delete.

[.underline]#Uninstall the Cloudera Manager Server#

[source,bash]
----

#### Cleanup DB

systemctl status cloudera-scm-server

#cd /etc/yum.repos.d

#rm -rvf cloudera-manager.repo

date

systemctl stop postgresql-14

#dnf remove -y postgresql-contrib postgresql-14-contrib postgresql-server postgresql-14-server

#userdel postgres

systemctl stop cloudera-scm-server cloudera-scm-agent cloudera-scm-server-db cloudera-manager-server-db

dnf remove -y cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server cloudera-manager-server-db

systemctl daemon-reload

#rm -rvf /var/lib/pgsql/14/data/

mv -v /var/lib/pgsql/14/data/ /var/lib/pgsql/14/data_bkp_$(date +%Y%m%d)

##### Cleanup CDP-CM, Base Master and Worker nodes

#/opt/cloudera/installer/uninstall-cloudera-manager.sh

systemctl stop cloudera-scm-server cloudera-scm-agent cloudera-scm-server-db supervisord;

dnf remove -y cloudera-manager-server cloudera-manager-server-db-2 cloudera-management-agent cloudera-management-daemon cloudera-manager-*; dnf clean all; systemctl daemon-reload;

for u in cloudera-scm* flume hadoop hdfs hbase hive httpfs hue impala llama mapred oozie solr spark sqoop sqoop2 yarn zookeeper; do sudo kill $(ps -u $u -o pid=); done

sudo umount cm_processes

##### Cleanup CDP-CM, Base Master and Worker nodes

sudo rm -rvf /usr/share/cmf /var/lib/cloudera* /var/cache/yum/cloudera* /var/log/cloudera* /var/run/cloudera* /etc/cloudera-scm-server /opt/cloudera /etc/cloudera-scm-agent /var/lib/cloudera-scm-agent/cm_guid* /tmp/.scm_prepare_node.lock

sudo rm -rvf /tmp/kafka-logs

sudo rm -rvf /var/lib/flume-ng /var/lib/hadoop* /var/lib/hue /var/lib/navigator /var/lib/oozie /var/lib/solr /var/lib/sqoop* /var/lib/zookeeper /hadoop-ozone /impala /hadoop-ozone /var/local/kafka/data/meta.properties

sudo rm -rvf /hdfs/* /dfs* /hdfs/mapred/* /hdfs/yarn/* /var/lib/had*ozon* /yarn* /etc/{*atlas*,*hadoop*,ranger,hue,impala,knox,hbase,*hive*,hbase-solr,hadoop-kms,*ozone*,*kafka*,*zeppelin*,*spark*,sqoop*,schemaregistry,*solr*,hive-hcatalog,hive-webhcat,hue,*hbase*,*kudu*,*knox*,zookeeper,*tez*,streams*} /tmp/kafka-logs/* /var/local/kafka/data/meta.properties /var/lib/cloudera-scm-agent/cm_guid

##### Only If you are doing end-to-end cleanup, including cloudera-manager and postgres DB, run on all

for user in hdfs httpfs sqoop kafka yarn hbase streamsrepmgr streamsmsgmgr livy kms atlas schemaregistry hue zookeeper accumulo phoenix mapred druid ranger zeppelin oozie kudu knox superset solr hive cruisecontrol impala rangerraz ozone tez dpprofiler flume nifi nifiregistry nifitoolkit spark flink rangerrms omid hadoop kraft; do userdel -r "$user" 2>/dev/null; done

for group in hdfs hue httpfs sqoop kafka yarn hbase streamsrepmgr streamsmsgmgr livy kms atlas schemaregistry hue zookeeper accumulo phoenix mapred druid ranger zeppelin oozie kudu knox superset solr hive cruisecontrol impala rangerraz ozone tez dpprofiler flume nifi nifiregistry nifitoolkit spark flink rangerrms omid hadoop kraft; do groupdel "$group" 2>/dev/null; done

java -version

python3 -V
----

++**************************************************************************************************************++

=== Cleanup CDP PvC Data Services-ECS Cluster:

https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ecs-uninstall-pvc.html[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/installation-ecs/topics/cdppvc-installation-ecs-uninstall-pvc.html#]

##### CLEANUP ECS
[source,bash]
----

# On each host in the cluster:

/opt/cloudera/parcels/ECS/docker/docker container stop registry

/opt/cloudera/parcels/ECS/docker/docker container rm -v registry

/opt/cloudera/parcels/ECS/docker/docker image rm registry:2

# Stop the ECS cluster in Cloudera Manager

# On each host:

cd /opt/cloudera/parcels/ECS/bin

./rke2-killall.sh # usually 2 times is sufficient

# Use umount to unmount all NFS disks.

# umount /docker /lhdata /cdwdata #not needed in our case

./rke2-uninstall.sh

systemctl daemon-reload

rm -rvf /ecs/* # assumes the default defaultDataPath and lsoDataPath

rm -rvf /var/lib/docker_server/* # deletes the auth and certs

rm -rvf /etc/docker/certs.d/ /etc/docker/* # delete the ca.crt

rm -rvf /var/lib/docker/

rm -rvf /etc/rancher /var/lib/rancher /var/log/rancher /var/lib/rancher/k3s/server/node-token

rm -rvf /run/k3s /opt/containerd /opt/cni

rm -rvf /docker/* /lhdata/* /cdwdata/

rm -rvf ~/.kube ~/.cache

systemctl daemon-reload

# Delete the ECS cluster in Cloudera Manager.

# In Cloudera Manager, navigate to CDP Private Cloud Data Services and click . Click Uninstall.

# The Delete Cluster wizard appears. Click Delete.

#Clean IPtables on each host:

echo "Reset iptables to ACCEPT all, then flush and delete all other chains";

declare -A chains=( [filter]=INPUT:FORWARD:OUTPUT [raw]=PREROUTING:OUTPUT [mangle]=PREROUTING:INPUT:FORWARD:OUTPUT:POSTROUTING [security]=INPUT:FORWARD:OUTPUT [nat]=PREROUTING:INPUT:OUTPUT:POSTROUTING );

for table in "${!chains[@]}";

do

echo "${chains[$table]}" | tr : $"\n" | while IFS= read -r;

do

sudo iptables -t "$table" -P "$REPLY" ACCEPT

done

sudo iptables -t "$table" -F

sudo iptables -t "$table" -X

done

rm -rvf /etc/rancher /var/lib/rancher /var/log/rancher

# remove agents from all hosts in CM UI

systemctl stop cloudera-scm-agent

dnf remove -y cloudera-manager-agent cloudera-manager-daemons

rm -rvf /opt/cloudera/cm-agent/

rm -rf /opt/cloudera/ /var/lib/cloudera-scm-agent

# Remove the hosts from CM-UI

*Alternatively, an experimental script is available. This script combines steps three through five. The script is available here:

https://github.com/cloudera-labs/snippets/blob/main/private-cloud/kill-2-rke.sh[[.underline]#https://github.com/cloudera-labs/snippets/blob/main/private-cloud/kill-2-rke.sh#]

# Reboot the host(s).

# Before you install ECS again, ensure that the IP tables list is empty by executing the following command:

*iptables -L

----

++**************************************************************************************************************++

=== CDP PvC Base Cluster Error Handling

*alternatives --set python /usr/bin/python2

*openssl s_client -connect cldr-mngr.cldrsetup.local:8443 < /dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > knoxssoAmbari.crt

++**************************************************************************************************************++

If you are using PostgreSQL, turn off Readline support by using the -n option. (history-passwd)

Start the Cloudera Management Service when the Reports Manager role is ready. See Starting the Cloudera Management Service.

++**************************************************************************************************************++

*Ensure that the Ranger Solr and Ranger HDFS plugins are enabled.

++**************************************************************************************************************++

*Important

Ensure you complete the following tasks before you start performing the steps to configure TLS 1.2 on the Reports Manager for communicating with the database:

On the Cloudera Manager UI, navigate to Clusters > Cloudera Management Service.

Select the Configuration tab and search for reportsmanager_db_safety_valve.

Based on your database type you must override headlamp.db.properties file with JDBC URL properties. Enter the appropriate values in the following format to override the connection to use TLS 1.2.

*PostgreSQL

com.cloudera.headlamp.orm.hibernate.connection.url=jdbc:postgresql://<DB-HOST>:<DB-PORT>/<DB_NAME>?useSSL=true&trustCertificateKeyStoreUrl=<PATH_TO_TRUSTSTORE_FILE>&trustCertificateKeyStoreType=<TRUSTSTORE_TYPE>&trustCertificateKeyStorePassword=<TRUSTSTORE_PASSWORD>

com.cloudera.headlamp.db.type=postgresql

com.cloudera.headlamp.db.host=<DB-HOST>:<DB-PORT>

com.cloudera.headlamp.db.name=<DB_NAME>

++**************************************************************************************************************++

[15/Mar/2024 04:59:00 -0700] 10184 MainThread agent ERROR Heartbeating to localhost:7182 failed.

Traceback (most recent call last):

File "/opt/cloudera/cm-agent/lib/python3.8/site-packages/cmf/agent.py", line 1588, in _send_heartbeat

transceiver = cmf.https.HTTPSTransceiver(

File "/opt/cloudera/cm-agent/lib/python3.8/site-packages/cmf/https.py", line 245, in __init__

self.conn.connect()

File "/opt/cloudera/cm-agent/lib/python3.8/site-packages/M2Crypto/httpslib.py", line 74, in connect

sock.connect((self.host, self.port))

File "/opt/cloudera/cm-agent/lib/python3.8/site-packages/M2Crypto/SSL/Connection.py", line 337, in connect

if not check(self.get_peer_cert(),

File "/opt/cloudera/cm-agent/lib/python3.8/site-packages/M2Crypto/SSL/Checker.py", line 122, in __call__

raise WrongHost(expectedHost=self.host,

M2Crypto.SSL.Checker.WrongHost: Peer certificate subjectAltName does not match host, expected localhost, got DNS:pvcbasemaster.cldrsetup.local

pvcecs[1-5].cldrsetup.local; pvcecsmaster.cldrsetup.local: IOException thrown while collecting data from host: Received fatal alert: internal_error

*Solution*:

openssl s_client -connect pvcbasemaster.cldrsetup.local:7183

[root@pvcbasemaster cloudera-scm-agent]# cat /etc/cloudera-scm-agent/config.ini|grep server

change hostname to dnsname in place of localhost and restart all agents (heartbeat issue resolved)

/opt/cloudera/cm-agent/bin/supervisorctl -c /var/run/cloudera-scm-agent/supervisor/supervisord.conf restart status_server

*grep -v -e '^[[:space:]]*$' -e '^#' /etc/cloudera-scm-agent/config.ini

++**************************************************************************************************************++

grep -v -e '^[[:space:]]*$' -e '^#' /etc/cloudera-scm-agent/config.ini2024-03-16 03:04:54,710 ERROR pool-7-thread-1:com.cloudera.server.cmf.components.CmServerStateSynchronizer: Failed during cleanup : null

*Solution*:

Set java_home by searching java in configuration on the CM console.

Install Postgres and CDP base same day all together otherwise may cause ssl issue (observation)

++**************************************************************************************************************++

*Stale service status require restart of cluster

++**************************************************************************************************************++

*Ozone client config issue while deploy krb - known issue

It appears that you might have a proxy setup for the Administration Console. Specify the proxy url as the Frontend url or disable the HTTP Referer Check option.

++**************************************************************************************************************++

##### Ranger, Atlas not running

Due to kafka issue and SOLR issue

++**************************************************************************************************************++

##### SOLR error:

Initialize SOLR and create HDFS home dir from actions and start service will fix issue

++**************************************************************************************************************++

##### Kafka error:

Kafka and SOLR depends on Ozone, (SOLR depends on Kafka as well) install this first

++**************************************************************************************************************++

##### Tez error

tez -> action -> upload tez file to hdfs

CM > Hive > Action > Create hive dir

++**************************************************************************************************************++

##### YARN queue manager error:

[root@pvcbase-master ~]# sudo mkdir /var/lib/hadoop-yarn/
[root@pvcbase-master ~]# sudo chmod +077 /var/lib/hadoop-yarn/
[root@pvcbase-master ~]# sudo chown yarn:hadoop /var/lib/hadoop-yarn/

++**************************************************************************************************************++

##### Kafka error:

*Solution:* delete /var/local/kafka/data/meta.properties

++**************************************************************************************************************++

##### Enable thrift server for hbase-hue

*set wal property codec-hbase

++**************************************************************************************************************++

##### HBASE master bad health:

The problem lies in Cloudera Management Monitor Service, not in Hbase itself. What I did is to restart Cloudera Management Monitor Service, and then restart HBase. After that everything seems to be fine.

++**************************************************************************************************************++

#### Ozone error - Could not find or is not a file

Make sure that hdfs_service is enabled in the Ozone configuration. By having this enabled, the CM agent will put the core-site.xml into the process directory and that error will be gone.

Cleanup ozone directories before redeployment.

++**************************************************************************************************************++

*HDDS error Ozone

[root@pvcbase-master ~]# systemctl restart cloudera-scm-supervisord

++**************************************************************************************************************++

*Ozone ERROR datanode fail to start:

*Solution:* Perform Proper Cleanup on namenode and datanodes.

[root@pvcbase-master ~]# rm -rvf /hdfs/*had*oz* /var/lib/had*oz* /etc/had*oz

++**************************************************************************************************************++

[source,bash]
----
[root@pvcbase-master ~]# sudo -u postgres psql -U postgres -p 5432 -h $(hostname) 
Password for user postgres: 
psql (14.11)
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

postgres=# \q

[root@pvcbasemaster data]# echo -n 'Sahu@123{admin}' | md5sum
c94251c29cd07ed2daf0b6edcf843362  -

[root@pvcbasemaster data]# sudo -u postgres psql -U postgres -p 5432 -h $(hostname) 
Password for user postgres: 
psql (14.11)
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

postgres=# \c ranger
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
You are now connected to database "ranger" as user "postgres".
ranger=# update x_portal_user set password = '9746e519adb14ec3ffbf4aff051f104d' where login_id = 'admin';
UPDATE 1
ranger=# 
ranger=# select * from x_portal_user where login_id = 'admin';
 id |        create_time         |       update_time       | added_by_id | upd_by_id | first_name | last_name | pub_scr_name | login_id |         
    password             | email | status | user_src | notes | other_attributes | sync_source | old_passwords | password_updated_time 
----+----------------------------+-------------------------+-------------+-----------+------------+-----------+--------------+----------+----------------------------------+-------+--------+----------+-------+------------------+-------------+---------------+-----------------------
  1 | 2024-03-17 12:15:27.272988 | 2024-03-17 19:15:43.854 |             |           | Admin      |           | Admin        | admin    | c94251c29cd07ed2daf0b6edcf843362 |       |      1 |        0 |       |                  |             |               | 
(1 row)
ranger=# \q
-------------------------------------------------------------------------------------------------------

[root@pvcbasemaster data]# sudo -u postgres psql -U rangeradmin -p 5432 -d ranger -h $(hostname) 
Password for user rangeradmin: 
psql (14.11)
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.
ranger=> \q
----

++**************************************************************************************************************++

[source,bash]
----
[root@pvcbasemaster data]# sudo -u postgres psql -U rangeradmin -p 5432 -d ranger -h $(hostname)

Password for user rangeradmin:
psql (14.11)

SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)

Type "help" for help.

ranger=> \q

----

++**************************************************************************************************************++

[root@pvcbase-master ~]# tail -f /var/log/ranger/admin/access_log-pvcbasemaster.cldrsetup.local-2024-03-21.log

++**************************************************************************************************************++

2024-03-19 23:59:28.861 PDT [7434] LOG: could not accept SSL connection: EOF detected

2024-03-19 23:59:28.861 PDT [7427] LOG: could not accept SSL connection: EOF detected

2024-03-19 23:59:28.861 PDT [7539] LOG: could not accept SSL connection: EOF detected

++**************************************************************************************************************++

*Caused by: java.io.FileNotFoundException: /var/lib/cloudera-scm-server/.postgresql/root.crt (Permission denied)

*Solution*:

Ranger UI error SSL issue : issue was with permission on cloudera-scm-server directory where root.crt was stored.

++**************************************************************************************************************++

*Postgres Connection Limit exceeded:

Operation error. response=VXResponse={org.apache.ranger.view.VXResponse@2ca9483cstatusCode={1} msgDesc={RangerKRBAuthenticationFilter Failed : Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.7.7.v20200504-69f2c2b80d): org.eclipse.persistence.exceptions.DatabaseException

Internal Exception: java.sql.SQLException: Connections could not be acquired from the underlying database!

Error Code: 0} messageList={null} }

javax.ws.rs.WebApplicationException

at org.apache.ranger.common.RESTErrorUtil.createRESTException(RESTErrorUtil.java:56)

++**************************************************************************************************************++

Request failed. loginId=null, logMessage=RangerKRBAuthenticationFilter Failed : Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.7.7.v20200504-69f2c2b80d): org.eclipse.persistence.exceptions.DatabaseException

Internal Exception: java.sql.SQLException: Connections could not be acquired from the underlying database!

Error Code: 0

javax.ws.rs.WebApplicationException

at org.apache.ranger.common.RESTErrorUtil.createRESTException(RESTErrorUtil.java:56)

++**************************************************************************************************************++

2024-03-19 00:55:23,767 WARN C3P0PooledConnectionPoolManager[identityToken->1bqot7nb2ns2s4q1pyen82|14b0e127]-HelperThread-#0:com.mchange.v2.resourcepool.BasicResourcePool: com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@1ab83b08 -- Acquisition Attempt Failed!!! Clearing pending acquires. While trying to acquire a needed new resource, we failed to succeed more than the maximum number of allowed acquisition attempts (5). Last acquisition attempt exception:

org.postgresql.util.PSQLException: FATAL: sorry, too many clients already

at org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:698)

++**************************************************************************************************************++

++**************************************************************************************************************++

## standard_conforming_strings=off

2024-03-19 23:16:26.279 PDT [26314] WARNING: nonstandard use of escape in a string literal at character 261

2024-03-19 23:16:26.279 PDT [26314] HINT: Use the escape string syntax for escapes, e.g., E'\r\n'.

2024-03-19 23:16:33.719 PDT [24593] FATAL: sorry, too many clients already

*Solution:

Increase max_connections to 1000 on postgresql.conf file

++**************************************************************************************************************++

2024-03-26 03:35:19,203 ERROR - [main:] ~ GraphBackedSearchIndexer.initialize() failed (GraphBackedSearchIndexer:386) org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at https://pvcbasemaster.cldrsetup.local:8995/solr: Can not find the specified config set: vertex_index

https://community.cloudera.com/t5/Support-Questions/atlas-webui-is-not-accessible/td-p/324743[[.underline]#https://community.cloudera.com/t5/Support-Questions/atlas-webui-is-not-accessible/td-p/324743#]

stop atlas> initialize atlas> start atlas

++**************************************************************************************************************++

[root@pvcbase-master ~]# klist -kt /run/cloudera-scm-agent/process/1546342867-SolrServerGracefulShutDown/solr.keytab

[root@pvcbase-master ~]# kinit -kt /run/cloudera-scm-agent/process/1546342867-SolrServerGracefulShutDown/solr.keytab solr/pvcbasemaster.cldrsetup.local@CLDRSETUP.LOCAL

[root@pvcbase-master ~]# /opt/cloudera/parcels/CDH/bin/zookeeper-client

++**************************************************************************************************************++

*Chart not showing*--> install mgmt service.

*logfile=/var/log/cloudera-scm-agent/supervisord.log

++**************************************************************************************************************++


=== CDP PvC Data Services ECS Cluster Error Handling:

https://spacelift.io/blog/kubectl-port-forward[[.underline]#https://spacelift.io/blog/kubectl-port-forward#]

https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.3/day-two-operations/cdppvc-data-services-day-two-operations.pdf[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/day-two-operations/cdppvc-data-services-day-two-operations.pdf#]

https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.1/managing-ecs/cm-manage-ecs.pdf[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.1/managing-ecs/cm-manage-ecs.pdf#]

https://docs.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_cdp_dc.pdf[[.underline]#https://docs.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_cdp_dc.pdf#]

https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.0/upgrade-ecs/cdppvc-upgrade-ecs.pdf[[.underline]#https://docs.cloudera.com/cdp-private-cloud-data-services/1.5.4/upgrade-ecs/cdppvc-upgrade-ecs.pdf#]

*Could not get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused

*Solution:* kubeconfig error

[source, bash]
----
[root@pvcecs-master ~]# journalctl -xeu kubelet
[root@pvcecs-master ~]# mkdir -p ~/.kube && cp /etc/rancher/rke2/rke2.yaml ~/.kube/config
[root@pvcecs-master ~]# mkdir -p ~/.kube && cp /etc/rancher/rke2/rke2.yaml ~/.kube/config
[root@pvcecs-master ~]# kubectl get pods
-bash: kubectl: command not found
[root@pvcecs-master ~]# export PATH=/var/lib/rancher/rke2/data/v1.26.10-rke2r1-e58e49f33617/bin/:$PATH
[root@pvcecs-master ~]# kubectl get pods
No resources found in the default namespace.
[root@pvcecs-master ~]# kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   13h
[root@pvcecs-master ~]# 
----

++**************************************************************************************************************++

##### Error in commands

kubectl -n kubernetes-dashboard get sa/admin-user -o 'jsonpath={.secrets[0].name}'

*Solution*:

kubectl -n kubernetes-dashboard get secret/admin-user-secret -o 'jsonpath={.metadata.name}' (working)

++**************************************************************************************************************++

+ /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n kubernetes-dashboard get secret -o 'go-template={{.data.token | base64decode}}'

error: error executing template "{{.data.token | base64decode}}": template: output:1:16: executing "output" at <base64decode>: invalid value; expected string

*Solution*:

kubectl get secret -n kubernetes-dashboard admin-user-secret -o 'go-template={{.data.token | base64decode}}' (working command)

++**************************************************************************************************************++

*Error: INSTALLATION FAILED: cannot re-use a name that is still in use

++ K8S_DASHBOARD_CHART_FILE_NAME=kubernetes-dashboard-5.10.0.tgz

++ /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/../installer/install/bin/linux/helm install kubernetes-dashboard /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/kubernetes-dashboard-5.10.0.tgz -n kubernetes-dashboard --create-namespace -f /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/kubernetes-dashboard-overrides.yaml

*Solution:

kubectl delete ns kubernetes-dashboard

++**************************************************************************************************************++

*Install python3.8 for ecs

*Always mount correct volumes in config, created previously

++**************************************************************************************************************++

*Bootstrap error kubectl-* remove /var/lib/rancher directory

++**************************************************************************************************************++

/var/lib/rancher/rke2/agent/logs/kubelet.log

++**************************************************************************************************************++

[root@pvcecsmaster ~]# kubectl get pods

Unable to connect to the server: tls: failed to verify certificate: x509: certificate signed by unknown authority

*Solution*:

After proper cleanup and reinstall issue will solved.

[root@pvcecs-master ~]# mkdir -p ~/.kube

[root@pvcecsmaster opt]# cp /etc/rancher/rke2/rke2.yaml ~/.kube/config

cp: overwrite ‘/root/.kube/config’? Yes

[root@pvcecsmaster opt]# /var/lib/rancher/rke2/data/v1.26.10-rke2r1-e58e49f33617/bin/kubectl get nodes

NAME STATUS ROLES AGE VERSION

pvcecsmaster.cldrsetup.local Ready control-plane,etcd,master 61s v1.26.10+rke2r1

[root@pvcecsmaster opt]# cp /tmp/ecs_util.sh /opt/cloudera/cm-agent/service/ecs/ecs_util.sh

*Solution*: cleanup was not done properly,after proper cleanup by script, it started running

++**************************************************************************************************************++

*Error: INSTALLATION FAILED: cannot re-use a name that is still in use

++ /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/../installer/install/bin/linux/helm install kubernetes-dashboard /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/kubernetes-dashboard-5.10.0.tgz -n kubernetes-dashboard --create-namespace -f /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/kubernetes-dashboard-overrides.yaml

*Solution*:

[source, bash]
----
[root@pvcecs-master ~]# k get pods -A|grep dashboard
kubernetes-dashboard kubernetes-dashboard-68876b7cb8-fl58c 1/1 Running 0 4m13s

[root@pvcecsmaster ~]# k delete pod kubernetes-dashboard
Error from server (NotFound): pods "kubernetes-dashboard" not found

[root@pvcecsmaster ~]# k delete pod kubernetes-dashboard-68876b7cb8-fl58c -n kubernetes-dashboard
pod "kubernetes-dashboard-68876b7cb8-fl58c" deleted

[root@pvcecsmaster ~]#
[root@pvcecsmaster ~]# k delete ns kubernetes-dashboard
----

++**************************************************************************************************************++

[root@pvcecs-master ~]# k logs cli-v2m7w -n cdp

HTTPSConnectionPool(host='console-cdp.apps.cldrsetup.local', port=443): Max retries exceeded with url: /api/v1/environments2/createPrivateEnvironment (Caused by NameResolutionError("<cdpcli.cdprequest.CdpHTTPSConnection object at 0x7fbb901047f0>: Failed to resolve 'console-cdp.apps.cldrsetup.local' ([Errno -2] Name or service not known)"))

*Solution:* Make sure to setup wildcard DNS in advance before proceeding with ECS Setup steps

++**************************************************************************************************************++

[source, bash]
----
k edit cm rke2-coredns-rke2-coredns  -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
      errors
      health {
          lameduck 5s
      }
      ready
      kubernetes cluster.local cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
      }
      prometheus 0.0.0.0:9153
      #forward . /etc/resolv.conf
      forward . 172.16.31.226
      cache 30
      loop
      reload
      loadbalance
    }
----

*Possible Cause and Solution:

Wildcard dns was not set up properly. Before proceeding with ECS installation, set up the wildcard DNS.

++**************************************************************************************************************++

Map docker dependency if localhost:6443 apiserver error come in ECS

++**************************************************************************************************************++

*Bootstrap error

[time="2024-03-19T11:10:29-07:00" level=fatal msg="Failed to reconcile with temporary etcd: bootstrap data already found and encrypted with different token"

*Solution:

Cleanup not done properly.

++**************************************************************************************************************++

=== Kubernetes Command Reference:

[source, bash]
----
export PATH=$PATH:/var/lib/rancher/rke2/bin
echo "alias helm='/opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/installer/install/bin/linux/helm'" >> ~/.bashrc
echo "alias docker='/opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/docker/docker'" >> ~/.bashrc
echo "alias helm='/opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/installer/install/bin/linux/helm'" >> ~/.bashrc
echo "alias kubectl="sudo -E /var/lib/rancher/rke2/data/v1.26.10-rke2r1-e58e49f33617/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml'" >> ~/.bashrc
alias k=kubectl
. ~/.bashrc

kubectl 	get|describe|delete|create 		all|pods|nodes|ns|namespaces|svc|service|pv|pvc|rb|rolebinding|sa|roles|csr|secret|hpa|netpol|statefulset|replicaset|crd		-n vault-system | -A
kubectl get namespace vault-system -o json|yaml > tmp.json
helm list -n vault-system
kubectl api-resources  --namespaced=true -o name | xargs -n 1 kubectl get -n vault-system
k delete ns vault-system --force
k get pods -A -o wide |grep dash
k get sa kubernetes-dashboard -n kubernetes-dashboard -o yaml
kubectl delete pods -n kube-system -l k8s-app=kube-dns
kubectl port-forward deployment.apps/kubernetes-dashboard 8443:https -n kubernetes-dashboard
kubectl port-forward deployment.apps/kubernetes-dashboard 8443:443 -n kubernetes-dashboard
k logs -n cdp cdp-release-dssapp-6b5b68bcfd-b9rdd
k apply -f secret.yaml
k create token default
kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get -n vault-system --kubeconfig /etc/rancher/rke2/rke2.yaml
helm list -n kubernetes-dashboard
k get|delete|describe|edit	 helmcharts.helm.cattle.io	 -n vault-system

helm list -n kubernetes-dashboard
helm list -A
helm upgrade --install kubernetes-dashboard /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/kubernetes-dashboard-5.10.0.tgz -n kubernetes-dashboard  -f /opt/cloudera/parcels/ECS-1.5.4-b99-ecs-1.5.4-b99.p0.50802651/kubernetes-dashboard/kubernetes-dashboard-overrides.yaml --dry-run | grep admin
find / -name kubernetes-dashboard*.tgz
netstat -lntup|grep 6443
openssl s_client -connect pvcbasemaster.cldrsetup.local:5432 -debug -msg
cp /etc/rancher/rke2/rke2.yaml .kube/config
envsubst
----

++**************************************************************************************************************++

=== Acknowledgements

For their support and contribution to the design, validation, and creation of this VMware Validated Design, the author would like to thank:

* VMware Systems, Inc.
* Kuldeep Sahu, Partner Solutions Engineer, Cloudera Inc.
* Venkatesh Sellappa, Director, Partner Solutions Engineering, Cloudera Inc.

++**************************************************************************************************************++

= Cloudera Data In Motion Kubernetes Operators Installation Guide
v0.1, 2025-04-10: Draft
:description: Installation instructions for Cloudera Kubernetes Operators
:toc: left
:toclevels: 2
:sectnums:
:source-highlighter: rouge
:icons: font
:imagesdir: ./images
:hide-uri-scheme:
:homepage: https://github.com/cloudera/cloudera-partners

[WARNING]
====
üìù **DRAFT** ‚Äî This document is WIP and Not Final.
====

== Overview

This document walks through the installation and setup of Cloudera's Data In Motion Kubernetes Operators on a local instance for development and testing.
The canonical documentation is https://docs.cloudera.com/?tab=kubernetes-operators[here]

== Recommended Pre-Requisites

. Ubuntu version 24.04 (Ubuntu version 22.04 and above are also supported)
. 8 vCPU
. 32 GB RAM
. 100GB storage

. Cloudera license
.. Request a https://github.com/cloudera/cloudera-partners/tree/main/PartnerResources#partner-developer-license-program[Free Partner Developer License]

== Installation

[NOTE]
====
üìù All `**yaml**` files used for deploying various resources can be found in the link:./overrides[`**overrides**`] folder.
====

=== Base Setup
. Create a directory `**k8soperators**` and place `cloudera_license` file, `overrides` directory under it.
+
[source, bash]
----
mkdir ~/k8soperators
git clone --depth 1 --branch feature/k8soperators https://github.com/cloudera/cloudera-partners.git /tmp/repo && \
    cp -r /tmp/repo/K8SOperators/overrides ~/k8soperators/ && \
    rm -rf /tmp/repo

#Similarly copy the cloudera_license file to this directory.

# Run all commands from below location.
cd ~/k8soperators/
----

.. Verify the content
+
[source, bash]
----
ls -la ~/k8soperators/
total 16
drwxrwxr-x  3 ubuntu ubuntu 4096 May  7 10:05 .
drwxr-x--- 10 ubuntu ubuntu 4096 May  7 10:05 ..
-rw-rw-r--  1 ubuntu docker 1142 May  7 08:23 cloudera_license.txt
drwxrwxr-x  2 ubuntu docker 4096 May  7 10:05 overrides
----

. Get the latest updates
+
[source, bash]
----
sudo apt-get update
----

. Install docker
.. Remove old docker instance
+
[source, bash]
----
sudo apt remove docker docker-engine docker.io containerd runc
----

.. Install dependencies
+
[source, bash]
----
sudo apt install ca-certificates curl gnupg lsb-release
----

.. Add Docker GPG Key & Repository
+
[source, bash]
----
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
----

.. Add Docker Repo
+
[source, bash]
----
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
----

.. Install 
+
[source, bash]
----
sudo apt-get update
yes | sudo apt-get install docker-ce docker-ce-cli
----

.. Manage Docker as a Non-Root User
+
[source, bash]
----
sudo usermod -aG docker $USER
----

... Refresh group changes
+
[source, bash]
----
newgrp docker
----

.. Enable Docker to start on Boot
+
[source, bash]
----
sudo systemctl enable docker
sudo systemctl start docker
----

. Install latest version of kubectl
+
[source, bash]
----
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
----

.. Make it installable and move to PATH
+
[source, bash]
----
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
----

. Install k9s utility for management
+
[source, bash]
----
curl -LO https://github.com/derailed/k9s/releases/latest/download/k9s_Linux_amd64.deb
sudo dpkg -i k9s_Linux_amd64.deb
rm k9s_Linux_amd64.deb
----

. Install helm
+
[source, bash]
----
curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
----

. Install minikube
+
[source, bash]
----
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
rm minikube-linux-amd64  # Cleanup
----

.. Start minikube
+
[source, bash]
----
minikube start --cpus 8 --memory 30000
----

.. Verify minikube status
+
[source, bash]
----
minikube status
----

. Deploy OpenLDAP in Kubernetes
+
[NOTE]
====
Make sure all the instances of `<admin_password>` in `overrides/openldap-values.yaml` are updated with a secure password of your choice.  
This same password will be used to:
- Create OpenLDAP secrets
- Access NiFi and NiFi Registry UIs
====

.. Deploy OpenLDAP after updating `openldap-values.yaml` file
+
[source, bash]
----
helm repo add helm-openldap https://jp-gouin.github.io/helm-openldap/
helm install openldap helm-openldap/openldap-stack-ha --create-namespace --namespace openldap -f overrides/openldap-values.yaml
----

. Install cert-manager (for automatic SSL/TLS certificate management in Kubernetes)
+
[source, bash]
----
helm repo add jetstack https://charts.jetstack.io --force-update
helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --set crds.enabled=true
----

. Deploy ClusterIssuer
+
[source, bash]
----
cat <<EOF > clusterissuer.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: self-signed-ca-issuer
spec:
  selfSigned: {}
EOF
kubectl apply -f clusterissuer.yaml
----

=== Deploy CFM Operator
. Create namespace
+
[source, bash]
----
kubectl create namespace cfm-operator-system
----

. Set environment variables for Cloudera username and password
+ 
[source,bash]
----
# Update the [***cloudera_username***] and [***cloudera_password***] with your cloudera license paywall credentials.
export cloudera_username="[***cloudera_username***]"
export cloudera_password="[***cloudera_password***]"
----

. Create Docker registry secret
+
[source, bash]
----
kubectl create secret docker-registry docker-pull-secret \
  --namespace cfm-operator-system \
  --docker-server container.repository.cloudera.com \
  --docker-username $cloudera_username \
  --docker-password $cloudera_password
----

. Create license secret
+
[source, bash]
----
#In below command `cloudera_license.txt` is the Cloudera license file. Make sure the license file exists in k8soperators folder.
kubectl create secret generic cfm-operator-license \
  --from-file=license.txt=./cloudera_license.txt \
  -n cfm-operator-system

----

. Download the `cfmctl` binary
+
[NOTE]
====
üìù Download `cfmctl` binary as per your OS architecture. Available binary options list [`cfmctl-darwin-amd64`, `cfmctl-darwin-arm64`, `cfmctl-linux-amd64`, `cfmctl-linux-arm64`, `cfmctl-windows-amd64`, `cfmctl-windows-arm64`]
====

. Install `cfmctl` utility
+
[source, bash]
----
curl -u "${cloudera_username}:${cloudera_password}" \
  -O https://archive.cloudera.com/p/cfm-operator/[**cfmctl_Binary**]
chmod +x [**cfmctl_Binary**]
mv [**cfmctl_Binary**] cfmctl
----

.. Example Usage: Below command installs `cfmctl-linux-amd64`
+
[source, bash]
----
curl -u "${cloudera_username}:${cloudera_password}" \
  -O https://archive.cloudera.com/p/cfm-operator/cfmctl-linux-amd64
chmod +x cfmctl-linux-amd64
mv cfmctl-linux-amd64 cfmctl
----

. Install the CFM Operator using cfmctl
+
[source, bash]
----
./cfmctl install --license [***LICENSE***] \
--image-repository "[***IMAGE REPOSITORY***]" \
--image-tag "[***OPERATOR VERSION***]" \
‚Äìvalues [***VALUES.YAML***] \
--namespace [***OPERATOR NAMESPACE***]
----

.. Example Usage:
+
[source, bash]
----
./cfmctl install --license ./cloudera_license.txt \
--image-repository container.repository.cloudera.com/cloudera/cfm-operator \
--image-tag 2.10.0-b134 \
--namespace cfm-operator-system
----

. Deploy NiFi
.. Create namespace
+
[source, bash]
----
kubectl create namespace demo-nifi
----

.. Create Docker registry secret for NiFi
+
[source, bash]
----
kubectl create secret docker-registry docker-pull-secret \
  --namespace demo-nifi \
  --docker-server container.repository.cloudera.com \
  --docker-username ${cloudera_username} \
  --docker-password ${cloudera_password}
----

.. Create OpenLDAP secret (if using LDAP)
+
[source, bash]
----
# Make sure to use the same <admin_password> mentioned in `openldap-values.yaml` while deploying openldap chart.
kubectl create secret generic openldap-creds \
  --from-literal=managerPassword=[***admin_passpord***] \
  -n demo-nifi
----

.. Apply NiFi configuration overrides
+
[source, bash]
----
kubectl apply -f overrides/nifi_overrides.yaml -n demo-nifi
----

. Deploy NiFi Registry
.. Create namespace
+
[source,bash]
----
kubectl create namespace demo-nifi-registry
----

.. Create Docker registry secret for NiFi Registry
+
[source,bash]
----
kubectl create secret docker-registry docker-pull-secret \
  --namespace demo-nifi-registry \
  --docker-server container.repository.cloudera.com \
  --docker-username ${cloudera_username} \
  --docker-password ${cloudera_password}
----

.. Create OpenLDAP secret (if using LDAP)
+
[source,bash]
----
# Make sure to use the same <admin_password> mentioned in `openldap-values.yaml` while deploying openldap chart.
kubectl create secret generic openldap-creds \
  --from-literal=managerPassword=[***admin_passpord***] \
  -n demo-nifi-registry
----

.. Apply NiFi Registry configuration
+
[source,bash]
----
kubectl apply -f overrides/nifiregistry.yaml --namespace demo-nifi-registry
----

=== Deploy CSA operator
. Create namespace
+ 
[source,bash]
----
kubectl create namespace csa-operator-system
----

. Set environment variables for Cloudera username and password
+ 
[source,bash]
----
# Update the [***cloudera_username***] and [***cloudera_password***] with your cloudera license paywall credentials.
export cloudera_username="[***cloudera_username***]"
export cloudera_password="[***cloudera_password***]"
----

. Create Docker registry secret for CSA
+ 
[source,bash]
----
kubectl create secret docker-registry docker-pull-secret \
  --namespace csa-operator-system \
  --docker-server container.repository.cloudera.com \
  --docker-username ${cloudera_username} \
  --docker-password ${cloudera_password}
----

. Log in to Cloudera registry using Helm
+ 
[source,bash]
----
echo "${cloudera_password}" | helm registry login container.repository.cloudera.com \
    --username "$cloudera_username" \
    --password-stdin
----

. Install CSA operator using Helm
+ 
[source,bash]
----
helm install csa-operator --namespace [***NAMESPACE***] \
    --set 'flink-kubernetes-operator.imagePullSecrets[0].name=[***SECRET NAME***]' \
    --set 'ssb.sse.image.imagePullSecrets[0].name=[***SECRET NAME***]' \
    --set 'ssb.sqlRunner.image.imagePullSecrets[0].name=[***SECRET NAME***]' \
    --set-file flink-kubernetes-operator.clouderaLicense.fileContent=[***PATH TO LICENSE FILE***] \
oci://container.repository.cloudera.com/cloudera-helm/csa-operator/csa-operator --version [***csa_operator_version***]
----

.. Example Usage:
+ 
[source,bash]
----
helm install csa-operator --namespace csa-operator-system \
    --set 'flink-kubernetes-operator.imagePullSecrets[0].name=docker-pull-secret' \
    --set 'ssb.sse.image.imagePullSecrets[0].name=docker-pull-secret' \
    --set 'ssb.sqlRunner.image.imagePullSecrets[0].name=docker-pull-secret' \
    --set-file flink-kubernetes-operator.clouderaLicense.fileContent=./cloudera_license.txt \
oci://container.repository.cloudera.com/cloudera-helm/csa-operator/csa-operator --version 1.2.0-b27
----

. Verify CSA operator installation
+ 
[source,bash]
----
# Make sure all the pods are in ready state before moving to the next step.
kubectl get pods -n csa-operator-system
----

. Deploy Flink
.. Deploy Flink application using session Cluster deployments
+ 
[source,bash]
----
kubectl -n csa-operator-system apply -f overrides/flink-deployment.yaml
----

.. Below is how you can deploy a Flink job [Optional]
+ 
[NOTE]
====
When creating the YAML for job deployment, set `deploymentName` to match the name used in your Flink application deployment defined in `flink-deployment.yaml`.  
If unchanged, the default name is `demo-flink`.
====

... Generate values.yaml and deploy the flink job
+
[source,bash]
----
cat <<EOF > flink_job_session.yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: basic-session-job-example
spec:
  deploymentName: [***FLINK_DEPLOYMENT_NAME***]
  job:
    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.16.1/flink-examples-streaming_2.12-1.16.1-TopSpeedWindowing.jar
    parallelism: 4
    upgradeMode: stateless
EOF

kubectl apply -f flink_job_session.yaml
----

=== Deploy CSM Operator
. Create namespace for CSM Operator
+ 
[source,bash]
----
kubectl create namespace csm-operator-system
----

. Set environment variables for Cloudera username and password
+ 
[source,bash]
----
# Update the [***cloudera_username***] and [***cloudera_password***] with your cloudera license paywall credentials.
export cloudera_username="[***cloudera_username***]"
export cloudera_password="[***cloudera_password***]"
----

. Create Docker registry secret for CSM Operator
+ 
[source,bash]
----
kubectl create secret docker-registry docker-pull-secret \
  --namespace csm-operator-system \
  --docker-server container.repository.cloudera.com \
  --docker-username ${cloudera_username} \
  --docker-password ${cloudera_password}
----

. Log in to Cloudera registry using Helm
+ 
[source,bash]
----
echo "${cloudera_password}" | helm registry login container.repository.cloudera.com \
    --username "$cloudera_username" \
    --password-stdin
----

. Install Strimzi Kafka Operator using Helm
+ 
[source,bash]
----
helm install strimzi-cluster-operator \
  --namespace [***NAMESPACE***] \
  --set 'image.imagePullSecrets[0].name=[***SECRET NAME***]' \
  --set-file clouderaLicense.fileContent=[***PATH TO LICENSE FILE***] \
  --set watchAnyNamespace=true \
  oci://container.repository.cloudera.com/cloudera-helm/csm-operator/strimzi-kafka-operator \
  --version [***strimzi-kafka-operator-version***]
----

.. Example Usage:
+ 
[source,bash]
----
helm install strimzi-cluster-operator \
  --namespace csm-operator-system \
  --set 'image.imagePullSecrets[0].name=docker-pull-secret' \
  --set-file clouderaLicense.fileContent=./cloudera_license.txt \
  --set watchAnyNamespace=true \
  oci://container.repository.cloudera.com/cloudera-helm/csm-operator/strimzi-kafka-operator \
  --version 1.3.0-b52
----

. Verify CSM Operator installation
+ 
[source,bash]
----
# Make sure the deployment and pod is in ready state before moving to next step.
kubectl get deployments --namespace csm-operator-system
kubectl get pods --namespace csm-operator-system
----

. Deploy Kafka
.. Create namespace for Kafka
+ 
[source,bash]
----
kubectl create namespace cloudera-kafka-demo
----

.. Create Docker registry secret for Kafka
+ 
[source,bash]
----
kubectl create secret docker-registry docker-pull-secret \
  --namespace cloudera-kafka-demo \
  --docker-server container.repository.cloudera.com \
  --docker-username ${cloudera_username} \
  --docker-password ${cloudera_password}
----

.. Apply Kafka configurations
+ 
[source,bash]
----
kubectl apply --filename overrides/kafka.yaml,overrides/nodepool-broker.yaml,overrides/nodepool-controller.yaml -n cloudera-kafka-demo
----

.. Validating a Kafka cluster
+
[source,bash]
----
#Wait until all the pods in cloudera-kafka-demo namespace are in ready state before moving to the next step.
kubectl get pods -n cloudera-kafka-demo
----

.. Create topic using Kafka Admin
+ 
[source,bash]
----
IMAGE=$(kubectl get pod kafka-cluster-broker-0 --namespace cloudera-kafka-demo --output jsonpath='{.spec.containers[0].image}')
kubectl run kafka-admin -it \
  --namespace cloudera-kafka-demo \
  --image=$IMAGE \
  --rm=true \
  --restart=Never \
  --command -- /opt/kafka/bin/kafka-topics.sh \
    --bootstrap-server kafka-cluster-kafka-bootstrap:9092 \
    --create \
    --topic my-topic
----

.. Produce message to the Kafka topic using Kafka console producer
+ 
[source,bash]
----
kubectl run kafka-producer -it \
  --namespace cloudera-kafka-demo \
  --image=$IMAGE \
  --rm=true \
  --restart=Never \
  --command -- /opt/kafka/bin/kafka-console-producer.sh \
    --bootstrap-server kafka-cluster-kafka-bootstrap:9092 \
    --topic my-topic

# It'll open an interactive shell. Type the messages and then press <ctrl+c> to exit.
----

.. Consume messages from Kafka topic using Kafka console consumer
+ 
[source,bash]
----
kubectl run kafka-consumer -it \
  --namespace cloudera-kafka-demo \
  --image=$IMAGE \
  --rm=true \
  --restart=Never \
  --command -- /opt/kafka/bin/kafka-console-consumer.sh \
    --bootstrap-server kafka-cluster-kafka-bootstrap:9092 \
    --topic my-topic \
    --from-beginning

# It'll show the produced messages. Press <ctrl+c> to exit.
----

== Access Application UI

=== Nifi
. Expose the NiFi UI when running on localhost
+
[source,bash]
----
minikube service demonifi-web --url -n demo-nifi
----

.. You will see an output like:
+
[source,bash]
----
üòø  service demo-nifi/demonifi-web has no node port
‚ùó  Services [demo-nifi/demonifi-web] have type "ClusterIP" not meant to be exposed, however for local development minikube allows you to access this !
http://127.0.0.1:53759
----

.. Now, keep this terminal open, and open your browser to access:
+
[source,bash]
----
https://127.0.0.1:53759/nifi/
----

. Expose the NiFi UI using NodePort and access via SSH tunneling(e.g When running minikube in an ec2 instance)

.. Change NiFi service type to NodePort
+
[source, bash]
----
kubectl -n demo-nifi patch svc demonifi-web \
  -p '{"spec": {"type": "NodePort"}}'
----

.. Get the NodePort value
+
[source, bash]
----
kubectl get svc demonifi-web -n demo-nifi -o jsonpath='{.spec.ports[*].nodePort}'
----

.. SSH tunneling example
+
[source, bash]
----
ssh -L 8443:192.168.49.2:32156 ubuntu@13.215.183.137
----

.. SSH tunneling with PEM key
+
[source, bash]
----
ssh -f -N -i <PEM_FILE_LOCATION> \
  -L 8443:192.168.49.2:<NodePort> ubuntu@<ec2PublicIp>
----

=== NiFi Registry
. Expose the NiFi Registry UI when running on localhost
+
[source,bash]
----
minikube service demonifiregistry-web --url -n demo-nifi-registry
----

.. You will see an output like:
+
[source,bash]
----
üòø  service demo-nifi-registry/demonifiregistry-web has no node port
‚ùó  Services [demo-nifi-registry/demonifiregistry-web] have type "ClusterIP" not meant to be exposed, however for local development minikube allows you to access this !
http://127.0.0.1:52866
----

.. Now, keep this terminal open, and open your browser to access:
+
[source,bash]
----
https://127.0.0.1:52866/nifi-registry/
----

. Expose the NiFi Registry UI using NodePort and access via SSH tunneling (e.g. when running minikube on an EC2 instance/remote host)

.. Change NiFi Registry service type to NodePort
+
[source,bash]
----
kubectl -n demo-nifi-registry patch svc demonifiregistry-web \
  -p '{"spec": {"type": "NodePort"}}'
----

.. Get the NodePort value
+
[source,bash]
----
kubectl get svc demonifiregistry-web -n demo-nifi-registry -o jsonpath='{.spec.ports[*].nodePort}'
----

.. SSH tunneling with PEM key
+
[source,bash]
----
ssh -f -N -i <PEM_FILE_LOCATION> \
  -L 18443:192.168.49.2:<NodePort> ubuntu@<ec2PublicIp>
----

=== Flink
. Access Flink UI when running on local laptop
+ 
[source,bash]
----
kubectl -n csa-operator-system port-forward service/demo-flink-rest <localport>:8081
----

.. Example Usage:
+ 
[source,bash]
----
kubectl -n csa-operator-system port-forward service/ssb-sse 8081:8081
----

. Expose the Flink UI using NodePort and access via SSH tunneling (e.g. when running minikube on an EC2 instance)
.. Patch cluster IP service to NodePort
+ 
[source,bash]
----
kubectl -n csa-operator-system patch svc demo-flink-rest -p '{"spec": {"type": "NodePort"}}'
----

.. Get the NodePort value
+ 
[source,bash]
----
kubectl get svc <service-name> -n <namespace> -o jsonpath='{.spec.ports[*].nodePort}'
----

.. SSH tunneling for local port forward
+ 
[source,bash]
----
ssh -f -N -i <PEM_FILE_LOCATION> -L 8081:192.168.49.2:<NodePort> ubuntu@<ec2PublicIp>
----

=== SSB
. Access SSB UI when running on local laptop
+ 
[source,bash]
----
kubectl -n csa-operator-system port-forward service/ssb-sse <localport>:8081
----

.. Example Usage:
+ 
[source,bash]
----
kubectl -n csa-operator-system port-forward service/ssb-sse 8081:8081
----

. Expose the SSB UI using NodePort and access via SSH tunneling(e.g When running minikube in an ec2 instance)
.. Change SSB service type to NodePort
+ 
[source,bash]
----
kubectl -n csa-operator-system patch svc ssb-sse -p '{"spec": {"type": "NodePort"}}'
----

.. Get the NodePort value for SSB UI
+ 
[source,bash]
----
kubectl get svc <service-name> -n <namespace> -o jsonpath='{.spec.ports[*].nodePort}'
----

.. SSH tunneling for local port forward
+ 
[source,bash]
----
ssh -f -N -i <PEM_FILE_LOCATION> -L 18121:192.168.49.2:<NodePort> ubuntu@<ec2PublicIp>
----
